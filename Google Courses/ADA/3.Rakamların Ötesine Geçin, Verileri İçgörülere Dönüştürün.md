# Vaka analizi: Deloitte

Keşifsel veri analizinin (EDA) altı uygulamasının her biri için tanımları öğrendiniz - keşfetme, yapılandırma, temizleme, birleştirme, doğrulama ve sunma. Daha sonra, bu uygulamaların veri kariyeri alanına nasıl uygulandığını keşfedeceksiniz. Aşağıdaki vaka çalışmasında, Deloitte'un veri ekibinin üyeleri, müşterilerinin metrik ve gösterge paneli ihtiyaçlarını karşılamak için EDA'nın altı uygulamasını kullanmıştır. Deloitte ekibinin deneyimleri hakkında daha fazla bilgi edindikten sonra, bu uygulamaların ne kadar yararlı ve güçlendirici olabileceğine dair somut bir örneğe sahip olacaksınız. 

![image](images/3001.png)

## Deloitte'u tanıyın

[Deloitte](https://www2.deloitte.com/us/en.html) bir denetim, danışmanlık, vergi ve danışmanlık hizmetidir. Şirketin dünya çapında 100.000'den fazla çalışanı ve iş ortağı var ve dünyanın en büyük şirketlerinin birçoğuyla çalışıyorlar. Hizmetleri vergi ve muhasebe çözümlerinden yapay zeka ve siber güvenlik risk programlarına kadar uzanmaktadır. 

![image](images/3002.png)

Çok uluslu şirketin işletmeden işletmeye (B2B) modeli, müşterileriyle onların bireysel ihtiyaçlarına göre bir dizi stratejik ve finansal cephede çalışmaya odaklanmalarını sağlar. [Deloitte, müşteri projeleri için denetim, danışmanlık, mali müşavirlik, risk danışmanlığı, vergi, veri analizi ve düzenleme gibi](https://youtu.be/8Gg5BsFexOU?list=PLl4by_vVwv0zjE2FRD7BMwI2lpaMCAgHh) bir dizi hizmet sunmaktadır. Deloitte, 2008 yılından bu yana profesyonel iş hizmetlerinde mükemmellik standardı olmayı vizyon haline getirmiştir. Bu odaklanma sayesinde Deloitte, müşterileri için gerçek zamanlı ölçümlerin kullanılması da dahil olmak üzere teknolojik gelişmelerin ön saflarında yer almıştır. 

Deloitte, birçok başarı öyküsünden yalnızca birinde, kısa süre önce milyar dolarlık bulut tabanlı analiz ve yazılım şirketinin pazarlama potansiyellerinin takibini ve organizasyonunu ve performans verilerinin stratejik analizini kolaylaştırmasına yardımcı oldu. Bu okumada, Deloitte'un bugün hala kullandıkları kapsamlı bir çözümü önermek ve uygulamak amacıyla müşterinin verilerini analiz etmek için EDA'yı nasıl kullandığını göreceksiniz.    

## Zorluklar 

![image](images/3003.png)

Deloitte'un en önemli bulut tabanlı yazılım çözümü müşterilerinden biri, pazarlama ve performans verileriyle ilgili çeşitli zorluklarla karşılaşıyordu. Hepsi için Deloitte'tan yardım istediler. İşte müşterinin Deloitte'a sunduğu özel zorluklar:  

-   Pazarlama potansiyel müşterilerinin takibinde zorluk 
    
-   Pazarlama kampanyalarının performansını anında takip etmekte zorlananlar
    
-   Stratejik şirket ölçümlerini gösteren kişiselleştirilmiş veri panolarının eksikliği
    

### **Pazarlama potansiyellerinin takibi**

Deloitte'un müşterisi, dünyanın dört bir yanındaki işletmelere bir dizi bulut tabanlı analiz hizmeti ve yazılım ürünü sunuyor ve bu da aylık olarak aldıkları binlerce soru ve potansiyel satış potansiyelinin her birini takip etmeyi muazzam bir görev haline getiriyor. Sonuç olarak, Deloitte'un müşterisi bu potansiyel müşterileri daha iyi takip etmelerine ve harekete geçmelerine yardımcı olacak bir çözüm arıyordu.   

### **Pazarlama kampanyası performansının izlenmesi**

Çok uluslu 1 milyar dolarlık bir şirketin pazarlama kampanyası performansının analizi için sadece dört veri analistine güvenmek, hızla sürdürülemez hale geliyordu. Deloitte yardım için devreye girmeden önce müşterinin performans takip sistemi, manuel olarak güncellenen ve bir elektronik tablo sekmesinde tutulan bir dizi veri tablosundan oluşuyordu. Her bölge kendi performans verilerini tutuyor, bu da tutarlılık sorunları yaratıyor ve şirket çapında bir performans resmi elde etmek için verileri derlemek ve birleştirmek zorunda kalan analistler için saatlerce boşa enerji harcanmasına neden oluyordu.  

### **Veri panolarını kişiselleştirme**

Her yönetim seviyesi için farklı ihtiyaçlar ve dünya genelinde farklı pazarlama bölgeleri nedeniyle, müşterinin şirket genelinde çalışanlarının ihtiyaçlarına özel verileri kolayca ayrıştırabilmesi ve gruplandırabilmesi gerekiyordu. Birkaç farklı departman ve bölgede birden fazla elektronik tablo üzerinden çalıştıkları için müşteri, verileri ihtiyaçlarına göre kolayca filtreleyemediğini ve bu nedenle performansı artırmak için stratejiyi yeterince değiştiremediğini fark etti. 

## Yaklaşım

Deloitte yeni müşterisiyle çalışmaya başladığında, müşterisinin önerdiği her bir ihtiyacı karşılayacak bir yaklaşım tasarladı. Bu yaklaşımı ayrıntılı olarak incelemek için aşağıdaki PACE girişlerine bakın.

![image](images/3004.png)

## Plan

Başlangıç olarak Deloitte'un müşteri paydaşlarıyla bir araya gelmesi ve yukarıda belirtilen üç alana göre geleceğe yönelik vizyonlarını geliştirmelerine yardımcı olması gerekiyordu: **Pazarlama potansiyelleri, pazarlama kampanyası performansının ölçülmesi** ve **veri panolarının kişiselleştirilmesi**. 

Süreçlerini, hedeflerini ve temel sonuçlarını (OKR'ler) ve müşteri ve potansiyel müşteri verilerine nasıl yaklaştığını ve bunları nasıl kullandığını öğrenerek müşterinin işini anlamaları gerekiyordu. 

Projenin kilit kilometre taşları erkenden belirlenmiş ve edinilen yeni bilgilere göre ayarlanmıştır. Bu kilometre taşları şunlardı: 

-   Veri mimarisi ve gösterge tablolarının gelecek vizyonu konusunda müşteri ile uyum
    
-   Yeni veri mimarisinin oluşturulması
    
-   Tanımlayıcı ve teşhis araçlarının piyasaya sürülmesi
    
-   Tahmine dayalı modellerin başlatılması
    
-   Küresel satış ekibinin yeni takımlar konusunda eğitilmesi
    

Deloitte, proje planının, müşterinin şirket genelindeki performans verileri üzerinde EDA'nın keşfedilmesi, yapılandırılması ve temizlenmesi uygulamalarıyla başlayacağını belirledi.  

Deloitte, kişiselleştirilmiş gösterge tabloları tamamlandığında müşterinin hangi OKR'lere kolayca erişmek istediğini hızla öğrendi ve bu sonuca ulaşmak için planlar hazırladı. Bu OKR'ler aşağıdaki sorular şeklinde geldi: 

-   Belirli bir pazarlama kampanyasından ne kadar gelir elde edildi? 
    
-   Yeni müşteri hangi endüstriye, bölgeye, sektöre ve şirket büyüklüğüne dahil oldu? 
    
-   İlk temas noktasından sonra satışın sonuçlandırılması ne kadar sürdü? 
    
-   Farklı kampanya türlerinin başarı oranı neydi (örneğin, yüz yüze etkinliklere karşı çevrimiçi web seminerleri)? 
    
-   Kazanılan müşteri başına ortalama pazarlama maliyeti neydi?
    

## Analiz

Müşterinin verilerinin ilk analizinin çerçevesinin bir parçası olarak, müşterinin kullanımı kolay Tableau gösterge paneli araçları gibi varlıklar da dahil olmak üzere Deloitte'un ürün hizmetleri tekliflerine aşina olmadığı anlaşıldı. Bu nedenle, müşterinin mümkün olana ilişkin varsayımları ve talepleri Deloitte'un gerçek kapasitesinin çok altında kalmıştır. Bu, müşterinin projeden beklentilerini sadece karşılamakla kalmayıp, aslında aşmak için bir fırsattı. 

Analizin ilk zorlukları, verilerin nereden geldiğini ve her bir veri değişkeninin ne anlama geldiğini anlamaktı. Müşteri, verilerinde standart olmayan adlandırma kuralları ve tanımlar kullanıyordu, bu nedenle Deloitte'un müşterinin verilerini tam olarak anlamak ve bunları kullanmalarına nasıl yardımcı olacağını planlamak için dillerini öğrenmek üzere keşif uygulamalarını kullanması gerekiyordu. 

Deloitte, müşterinin o zamanki performans izleme yaklaşımının en iyi ihtimalle hantal ve aşırı geniş olduğunu tespit etti. Topladıkları performans verileri, şirket çapındaki stratejik eylemlerin değerini belirlemek için genel ortalamaların kullanılmasından oluşuyordu. Bölge başına belirli performans verileri olmadan, şirketin bölgesel performansı izlemenin basit bir yolu yoktu.  

-   Müşterinin o dönemdeki aşırı kapsamlı veri takibine bir örnek olarak, pazarlama kampanyası başına elde ettikleri ortalama geliri elde etmek için kullandıkları formül buydu: 
    
$$
\frac{\textit{average customer contract value}}{\textit{average cost of marketing campaigns}} = \textit{average revenue gained per campaign}
$$

Deloitte, müşterinin dahili pazarlama ve satış ekiplerinin coğrafi bölgelere, sektörlere ve müşteri hesabı büyüklüğüne göre organize edildiğini öğrendi. Bölgesel liderler pazarlama araştırmalarını ve kampanyalarını kendi başlarına yürütme yetkisine ve özerkliğine sahipti, ancak küresel ekipten rehberlik ve zaman zaman talimat alıyorlardı. Deloitte analistleri, müşterinin verilerinin EDA'sının bir parçası olarak verileri şirket genelinde tutarlı bir şekilde yapılandırmaya başladı.

Müşteri yaşam döngüsü açısından Deloitte, müşterinin pazarlama potansiyel müşterilerinin pazarlama kampanyaları oluşturacağını ve ardından bölgelerindeki satış ortaklarını bilgilendireceğini öğrendi. Saha satış temsilcileri, kampanyalardan elde edilen tüm potansiyel müşterileri takip edecek ve ek müşteriler bulmak için mevcut kampanyaların dışında da ağ kuracaklardı. EDA'nın bu keşif kısmı, Deloitte ekibini müşterinin ihtiyaçlarını karşılayabilecek daha spesifik ve makul çözümler konusunda bilgilendirdi. 

Deloitte ekibi, müşterinin Salesforce ve pazarlama kampanyalarındaki mevcut tüm verileri gözden geçirmek için elektronik tablolarda EDA'nın keşif ve yapılandırma uygulamalarını kullandı. Müşterilerinin satış sisteminde var olan her veri değişkenini belirlemeye ve tanımlamaya başladılar. Veri değişkenlerinin envanteri yapılandırıldıktan, temizlendikten ve doğrulandıktan sonra Deloitte ekibi bu bilgilerden ne tür içgörüler elde edebilecekleri konusunda beyin fırtınası yaptı. 

İncelemenin ardından Deloitte, topladığı bilgileri müşteriyle paylaşmak için EDA'nın sunum uygulamasından yararlandı. Müşterinin kendi işini nasıl gördüğünü ve verilere göre onlar için hangi faktörlerin önemli olduğunu daha iyi anlamak için dinlediler ve sorular sordular.  

Yapılandırma Deloitte, müşterinin sistemlerine ilişkin analizlerinden ve müşterinin performans verileri üzerinde yaptıkları EDA'nın keşif, yapılandırma ve temizleme uygulamalarından, müşterinin analitik süreçlerinin

gerektiğini öğrendi; çok daha fazla ayrıntı sağlayan ve darboğazları, kârsız kampanyaları ve verimsiz müşteri segmentlerini belirlemelerine ve ele almalarına olanak tanıyan bir süreç

Müşterinin onayıyla Deloitte, analizi daha akıcı hale getirmek için müşterinin veri iç altyapısını tamamen yeniden inşa etti.  

Altyapı revizyonunun yanı sıra, müşteri için en pratik çözüm, belirli paydaşlar için özelleştirilmiş ve dinamik gösterge tabloları oluşturmaktı. 

-   Deloitte ekibi, ihtiyaçlarını ve OKR'lerini anlamak için her bir müşteri paydaş grubuyla birlikte çalıştı. 
    
-   Daha sonra, her bir değişkenin neyi temsil ettiğini ve nasıl kullanılıp kullanılmaması gerektiğini tanımlamak için bir veri sözlüğü gibi destekleyici teminat kaynakları oluşturarak müşteriye özel çözümler geliştirilmesine yardımcı oldular. 
    

Deloitte, müşterilerinin verilerinden topladıkları bilgileri kullanarak OKR'leri üzerinde tahmine dayalı analiz yapan otomasyonlar ve araçlar geliştirdi. Bu da müşterinin gelecek için uygulanabilir ancak agresif hedefler belirlemesini sağladı.

## Execute

Deloitte'un müşterisinin veri altyapısını elden geçirmesi, küresel veri toplamaya yönelik modern ve katmanlı bir yaklaşımın ortaya çıkmasını sağladı. Daha önce performans ölçümlerini formüle etmek için bölgesel olarak toplanmış ve tutarsız bir şekilde yapılandırılmış elektronik tablolara güvenmek zorunda kalan müşteri, artık küresel, şirket genelindeki performanstan bölgesel bir satış ekibi üyesinin belirli ürünler ve müşteri alt kümeleri üzerindeki performansına kadar her şeyi kolayca ölçebilmektedir.

Kişiselleştirilmiş gösterge tablolarının uygulanmasından sonra, Kıdemli Başkan Yardımcıları OKR'lerin durumunu ve performansını sorumlu oldukları tüm departmanlara ve bölgelere göre filtreleyebilmektedir. Ayrıca, bölgesel potansiyel müşteriler, pazarlama potansiyel müşterileri ve hatta yerel satış potansiyel müşterileri, ilgili alanlarına göre filtrelenmiş verileri görüntüleyebildi. 

Uygulamanın bir parçası olarak Deloitte ekip üyeleri, kişiselleştirilmiş gösterge tablolarının nasıl kullanılacağından yeni veri altyapısının bakımının nasıl yapılacağına kadar her konuda müşterilerinin ihtiyaçlarına göre eğitim oturumları oluşturdu.  

Deloitte tarafından tasarlanan gösterge tabloları ve veri altyapısı, müşterinin küresel ekibindeki çok sayıda paydaş için anında değerli varlıklar haline geldi. 

**Not**: Deloitte, müşteriyle yaptığı çalışmalarda yukarıda listelenen PACE iş akışını takip etmemiştir; bunun yerine, çalışmalarının ayrıntılarını bu vaka çalışmasında belgelendirirken, Deloitte'un müşteri için yaptığı çalışmalar, uygulamanın çok yönlülüğünü göstermek için PACE iş akışına göre düzenlenmiştir. 

## Sonuçların özetlenmesi

Deloitte yeni müşterisiyle çalışmaya başladığında, pazarlama ve satış ekipleri arasındaki standardizasyon eksikliği nedeniyle şirketin mevcut verilerinin doğruluğuna güvenmekte tereddüt ediyordu. Bu tutarlılık eksikliği, belirli bir satışı belirli bir kampanyayla ilişkilendirmeyi zorlaştırdı. Müşterinin satış ve pazarlama süreçlerini ve verilerinin EDA'sını birkaç hafta inceledikten sonra Deloitte, veri altyapısını tüm satışları geçerli pazarlama kampanyalarına doğrudan bağlayacak şekilde yeniden oluşturdu. Bu satışları, kolayca sıralanabilen bir dizi veri tablosunda hizalanmış birincil anahtarlar kullanarak pazarlama kampanyalarına bağladılar. Bu, Deloitte analistlerine verilerle nelerin mümkün olabileceği konusunda bir fikir verdi ve müşterinin başlangıçta mümkün olduğunu düşündüğü şeyleri aşan nihai ürünler oluşturmalarına yol açtı.

Altyapı elden geçirildikten sonra, Deloitte'un EDA performansı, müşterinin mevcut müşteri ilişkilerine zarar verecek şekilde yeni müşteriler kazanmaya çok fazla odaklandığını ortaya çıkardı. OKR'lere odaklanılması nedeniyle yeni müşterilere yapılan satışlar artarken, mevcut müşterilere yapılan satışlar düştü. Bu sonuç daha önce müşteri tarafından bilinmiyordu. Sonuç olarak müşteri, stratejisini hem satış bölgesi hem de ürün bazında değiştirebildi. Deloitte'un oluşturduğu dinamik gösterge tabloları, müşterinin performansı ölçme ve izlemenin yanı sıra strateji belirlemede de en etkili aracı haline geldi.  

## Sonuç

Deloitte, hem müşterinin iş uygulamalarını değerlendirirken hem de verilerinin EDA'sını yaparken müşterinin iş sorununa odaklandığı için, daha önce yapılandırılmamış verilerde saklı olan gerçekleri ortaya çıkarabilmiştir. Deloitte, müşterinin gelecekte bu gerçekleri kendi başına keşfedebilmesi için şirketteki herhangi bir satış bölgesine veya ürüne kadar filtreleyecek şekilde tasarlanmış etkileşimli, dinamik veri panoları oluşturdu. Bu çözümlerin en büyük faydası, müşterinin liderlik ve pazarlama liderlerinin çok daha çevik bir ekip haline gelmesi oldu. Gelirlerini doğrudan artıran etkili iş kararlarını daha iyi alabildiler. Veri altyapısının elden geçirilmesi ve veri gösterge paneli çözümleri, analistlerinin iş-yaşam dengesinin iyileştirilmesine de yardımcı oldu. Tüm bu sonuçlar sadece müşteri için değil Deloitte için de kayda değerdi çünkü müşteri firma için kilit bir ortak ve müşteriydi. Verilere odaklanmaları sayesinde Deloitte, müşterinin beklentilerini aşan bir çözüm sunabildi ve bunun karşılığında dünyanın dört bir yanındaki müşterilerine daha iyi hizmet vermelerine yardımcı oldu.

# Referans kılavuzu: EDA süreci

## EDA'nın altı uygulaması yinelemeli ve sıralı değildir

Keşifsel veri analizi (EDA) bir kek tarifi gibi değildir. Bu _**değil**_ adım adım takip ettiğiniz bir süreç. Bunun yerine, EDA'nın altı uygulaması yinelemeli ve sıralı değildir. 

-   **Yinelemeli**: Bir sürecin tekrarlanması ile ilgili veya tekrarlanmasını içeren
    
-   **Sıralamasız**: Bir düzen veya sıraya göre düzenlenmemiş veya bu sırayı takip etmeyen. 
    

Veri kümelerinin değişken doğası nedeniyle, bu verileri keşfetme yaklaşımı her seferinde farklı olacaktır. Bu, altı uygulamadan hangilerini kullanacağınızı, bunları kaç kez uygulayacağınızı ve süreçte ne zaman uygulamanız gerektiğini belirlemek için EDA süreci boyunca mantığınızı ve deneyiminizi kullanmanız gerekeceği anlamına gelir. 

## **Görsel örnek**

Norveç'teki iğne yapraklı bir ormandaki ağaçlar hakkında sadece 200 satır ve beş sütundan oluşan bir veri kümesine atandığınızı düşünün. Tam analizinizi tamamlamak için 1.000'den fazla satıra ve en az iki sütuna daha ihtiyacınız olacağını biliyorsunuz. Bundan daha fazla ayrıntı olmasa bile, tüm EDA süreciniz buna benzer bir şeye benzeyebilir:  

![image](images/3005.png)

1.  **Keşfetme**: Veri kümesinin genel şeklini, boyutunu ve içeriğini kontrol edersiniz. Verilerin yetersiz olduğunu görüyorsunuz. 
    
2.  **Katılıyorum**: Daha fazla veri eklersiniz.
    
3.  **Onaylama**: Yeni verilerde hata veya yazım yanlışı olup olmadığını hızlıca kontrol edersiniz. 
    
4.  **Yapılandırma**: Eğilimleri anlamak için verileri farklı zaman dilimlerinde ve segmentlerde yapılandırırsınız. 
    
5.  **Onaylama:** Yapılandırmada yaptığınız yeni sütunların doğru tasarlandığından emin olmak için hızlı bir kontrol daha yaparsınız. 
    
6.  **Temizleme**: Aykırı değerleri, eksik verileri ve dönüştürme veya dönüştürme ihtiyaçlarını kontrol edersiniz. 
    
7.  **Onaylama**: Temizledikten sonra, yaptığınız değişikliklerin doğru ve hatasız olduğunu iki kez kontrol edersiniz. 
    
8.  **Sunma**: Veri setinizi bir meslektaşınızla paylaşırsınız.

Verilerde yaptığınız değişikliklerin farkında olmadan hatalara yol açmadığından emin olmak için "doğrulama" uygulamasını yinelemeli olarak veya birden çok kez gerçekleştirdiğinize dikkat edin. Ayrıca, daha fazla veriye olan ihtiyacı önceden fark ettiğiniz için, "keşfetme" uygulamasının hemen ardından "birleştirme" uygulaması gerçekleştirildi. 

Temizlenmiş veri setinizi bir meslektaşınıza sunduktan sonra, daha fazla araştırma ve/veya temizlik için notlar veya fikirler alma şansınız yüksektir. Bu nedenle, daha da fazla yineleme göreceksiniz. 

**Profesyonel ipucu**: Veri bilimciler, bir veri kümesini "temiz" ve modelleme ya da makine öğrenimi algoritmaları için hazır ilan etmeden önce bu veri kümesi üzerinde birçok kez EDA uygulamalarını gerçekleştirmeyi beklemektedir. 

## Etik makine öğreniminde EDA'nın önemi

Algoritmalar ve makine ağları bireyler, şirketler ve hatta hükümetler adına giderek daha fazla karar almaya başladıkça, etik ve düzenleme tartışmaları da giderek daha önemli hale gelmektedir. [Institute for Ethical AI & Machine Learning](https://ethical.institute/principles.html)'e göre, makine öğrenimi sistemlerini sorumlu bir şekilde geliştirmek için sekiz ilke vardır. 

**EDA sürecinin temel ilkeleri**

Aşağıdaki iki ilke EDA sürecinin doğal bir parçasıdır:

-   **İnsan büyütme**: Bu ilke, gözetim için yapay zeka veya makine öğrenimi algoritma sistemlerine insanların eklenmesini sağlar. Veri bilimcileri tarafından gerçekleştirilen kapsamlı EDA, bir algoritmaya beslenen önyargı, dengesizlik ve yanlışlıkları sınırlamanın belki de en iyi yollarından biridir. 
    
-   **Yanlılık değerlendirmesi**: İnsan müdahalesi olmadan, makine öğrenimi modellerine önyargı çok kolay bir şekilde enjekte edilebilir ve yeniden üretilebilir. Metodik EDA süreçlerinin gerçekleştirilmesi, veri bilimcilerin verilerdeki önyargıların ve dengesizliklerin farkında olmalarını ve bunlara göre hareket etmelerini sağlayacaktır. 

**Profesyonel ipucu**: Veri kariyeri alanında etik standartlara uyulmasını sağlamanın önemi yadsınamaz. Veri uzmanlarının, EDA çalışmalarına sürekli olarak etik bir zihniyet uygulayarak önyargı ve ayrımcılığı fark etme kapasitelerini sürekli olarak geliştirmeleri gerekir.  

Makine öğreniminin ötesinde, EDA neredeyse tüm önemli veri tabanlı kararlara uygulanabilir. İleride, EDA'nın birçok uygulaması ve yinelemeli ve sıralı olmayan bir yaklaşımın gerekliliği hakkında bilgi edineceksiniz.

# Referans kılavuzu: Python ile veri kümelerini içe aktarma

Bir veri uzmanı olarak kariyerinizde, farklı dosya türlerine sahip veya çeşitli veritabanlarında depolanan çeşitli veri kümeleriyle karşılaşacaksınız. Daha önce öğrendiğiniz gibi, bu veri türlerinin ne olduğunu ve Python kullanarak verileri nasıl içe aktaracağınızı bilmeniz çok önemlidir. Aşağıda hem veritabanlarını bağlantılar aracılığıyla hem de veri dosyalarını Python'a aktarma örneklerini bulacaksınız.   

Python kodlaması için Coursera platformunu kullanacak olsanız da, CSV dosyalarını Coursera dışında indirmek ve açmak istiyorsanız bu dosyalarla nasıl çalışacağınızı ve içe aktaracağınızı bilmeniz gerekecektir.

## CSV dosyasından bir veri kümesi nasıl içe aktarılır

Bu örnek için bilgisayarınızda bir CSV dosyası bulun. Eğer bir tane yoksa, bu kursun Kurs kaynaklarına bağlantı[](https://www.coursera.org/learn/go-beyond-the-numbers-translate-data-into-insight/resources/9mSWv "Kaynaklar") bölümünden unicorn (1 milyar USD değerine ulaşan) şirketlerin bir veri kümesini kullanabilirsiniz.

Bir CSV dosyasını Python'a aktarmanın birkaç farklı yolu vardır, ancak biz sadece daha yaygın yollardan bazılarını inceleyeceğiz. Bir with deyimi ve open() fonksiyonu kullanarak başlayın. CSV dosyasının **dosya adını (veya dosya yolunu)** open() fonksiyonuna, fonksiyonun mode parametresi için bir argümanla birlikte iletin.

open("dosya yolu/dosya adı", mode=) ile

sözdizimi şöyledir:

**Not**: Aşağıdaki kod bloğu etkileşimli değildir.

```python
with open('file_path/file_name', mode=)
```

Mod, Python kütüphanesine dosya ile ne yapması gerektiğini söyler. **Modu** tanımlarken aşağıdaki seçeneklerden birini kullanırsınız: 

-   'r' - oku
    
-   'w' - yaz
    
-   'a' - ekle
    
-   '+' - yeni dosya oluştur
    

Tipik olarak, Python'un CSV dosyasını açmasını ve okumasını istediğiniz için with open() argüman alanının içindeki modu 'r' olarak tanımlayacaksınız. 

Daha sonra, sonucu bir değişken adına atayan as file sonuna ekleyeceğiz. Bu durumda, buna data adını vereceğiz. 

**Not:** Aşağıdaki kod bloğu etkileşimli değildir.

```python
with open('example_filepath/file', mode='r') as file:

    data = file.read()
```

### **Pandas kullanarak bir CSV dosyasını içe aktarma**

Bir dosyayı okumak için Python'un standart kütüphanesini kullanmak yerine, CSV dosyasını bir veri çerçevesine aktarmak için pandas'ı kullanabilirsiniz. İlk olarak, elbette, pandas kütüphanesini Python not defterinize aktarmak isteyeceksiniz.

```python
import pandas as pd
```

Daha sonra, verileri bir veri çerçevesine yüklemek için read_csv() fonksiyonu kullanacaksınız. Dosya yolu daha sonra argüman alanına gider.

```python
df = pd.read_csv("filepath/filename")
```

```python
import pandas as pd

df = pd.read_csv('example_filepath/file')
```

_**Not:**_ _Aynı sözdizimini internette depolanan bir CSV dosyasını içe aktarmak için de kullanabilirsiniz. Dosya adı yerine, basitçe url'yi kopyalayıp yapıştırırsınız._ 

## Bir veritabanına bağlanarak veriler nasıl içe aktarılır

Python ile bağlanabileceğiniz BigQuery, MySQL, SQLite ve Oracle gibi bir dizi veritabanı çözümü vardır. Veritabanları, şirketler ve kuruluşlar için büyük miktarda veriyi depolamanın uygun bir yoludur. 

Veri kümesi yeterince küçükse, bilgisayarınıza indirilebilir ve yerel olarak değiştirilebilir. Ancak, veri tabanlarında tutulan veri kümeleri genellikle kişisel bir bilgisayarda tamamına erişilemeyecek kadar büyüktür. Bu durumda, çoğu ilgilenilen belirli tabloları elde etmek için veritabanını SQL ile sorgulamayı içeren bir dizi farklı seçeneğiniz vardır. Başka bir deyişle, tüm veri kümesinden seçilen parçaları (genellikle belirli satırlar ve/veya sütunlar) ayıklarsınız. Sorgulamanın yapılma şekli sistemlere, platformlara ve arayüzlere göre değişebilir. Bu değişkenlik nedeniyle, bu başvuru kılavuzu veritabanlarını sorgulamak için yalnızca birkaç farklı yol sunacaktır. Özellikle, analizi kolaylaştırmak için çok çeşitli araçlar ve hizmetler sağlayan Google'ın veri ambarı BigQuery'yi keşfedecek.

### **BigQuery'den veri indirme**

#### **Adım 1: BigQuery'ye erişin**

BigQuery, depolama için veri yüklemenize olanak tanır ve ayrıca keşfedebileceğiniz halka açık bir dizi veri kümesine sahiptir. Ücretsiz bir Google hesabı gerektiren [BigQuery Sandbox](https://cloud.google.com/bigquery/docs/sandbox) adresini kullanarak bu herkese açık veri kümelerine ücretsiz olarak erişebilirsiniz. Sandbox size her ay ücretsiz olarak 10 GB aktif depolama alanı ve 1 TB işlenmiş sorgu verisi sağlar.

#### **Adım 2: Bir sorgu gerçekleştirin**

Hesabınızın kimliğini doğruladıktan ve birinci adımda bağlantılı talimatlarda belirtildiği gibi yeni bir proje oluşturduktan sonra, bir veritabanını sorgulamaya hazırsınız demektir. İlk kez oturum açıyorsanız, hızlı başlangıç kılavuzuna bir bağlantıyla birlikte "BigQueryUI'de yeni misiniz?" diye soran bir pencereyle karşılaşabileceğinizi unutmayın.

![image](images/3006.png)

Hızlı başlangıç kılavuzu, burada size sunulanlarla aynı adımlar boyunca size rehberlik edecektir.

"SQL Çalışma Alanınıza Hoş Geldiniz!" sayfasından "Yeni bir sorgu oluştur" düğmesine tıklayın.

![image](images/3007.jpg)

Sayfanın sol tarafındaki Explorer'da bulunan arama çubuğuna tıklayın. Örneğin, "ağaçlar" için arama yapabilirsiniz. Başlangıçta, bu sıfır sonuç döndürecektir. Ancak, "Tüm projeleri ara" seçeneğine tıkladığınızda bigquery-public-data projesinden uygulanabilir veri kümeleri ve bu veri kümelerinden önceden hazırlanmış tablolar döndürülecektir. 

san_francisco veri kümesindeki street_trees tablosuna tıklayın. Bu tablonun meta verileri sağdaki bir panelde görünecektir. Ardından, meta veri panelinin üst kısmındaki menüden "Sorgu "ya tıklayın. Sorgulamayı yeni bir sekmede veya geçerli pencerenin bölünmüş bir bölmesinde yapmayı tercih edebilirsiniz.

![image](images/3008.png)

Şimdi, SQL kullanarak tabloyu sorgulayabilirsiniz. Örneğin, aşağıdaki ekran görüntüsündeki sorgu, tree_id, plant_type, species, plant_date ve dbh - "derinlik, yükseklik" olarak tanımlanan sütunlarıyla 5.000 satır seçer.

![image](images/3009.png)

Sorgunuzdan memnun kaldığınızda, SQL sorgu panelinin üst kısmındaki "Çalıştır" düğmesine tıklayın. Sonuçlar aşağıda görüntülenecektir ve ortaya çıkan tabloyu farklı konum ve biçimlerde kaydetmenize olanak tanıyan "Sonuçları kaydet" düğmesi bulunmaktadır. Buradan, verileri not defterinize okuyabilirsiniz.

### **BigQuery içinde not defterlerini kullanma**

BigQuery'deki verilere erişmenin bir başka yolu da BigQuery platformunun kendi içindeki araçları kullanmaktır. Bu iş akışı, veri uzmanlarının bulutta depolanan çok büyük veri kümeleriyle çalışırken kullanacakları iş akışına daha çok benzemektedir. Esasen, BigQuery üzerinde bir sanal makine kurarsınız. Sanal makine, tıpkı diğer bilgisayarlar gibi kendi CPU'su, belleği, yazılımı vb. olan bir bilgisayardır, ancak kendi özel donanımı yoktur; çoğunlukla bir sunucuda bir bölüm olarak bulunurlar. BigQuery platformundaki sanal makinede bir Jupyter not defterinde çalışabilir, buradan doğrudan verileri sorgulayabilir ve çekebilirsiniz. 

Bu işlem, bir ödeme yöntemi ayarlamanızı gerektirir. Bununla birlikte, yeni kullanıcılar 300 $ kredi alır ve ML örneğinin dakikası yalnızca birkaç senttir, bu nedenle herhangi bir ücret ödemeden önce yaklaşık 2.000 saat ücretsiz kullanım elde edersiniz. Bunu kurmak için pek çok harika eğitim var. Örneğin, "Google Cloud AI'da Jupyter not defteri nasıl kullanılır?" diye aratırsanız konuyla ilgili çok sayıda faydalı video ve blog bulabilirsiniz.

### **Not defterlerini BigQuery dışında kullanma**

BigQuery platformunda olmayan not defterlerinden BigQuery'deki verileri sorgulamak da mümkündür. Ancak bu sürecin ayrıntıları, dizüstü bilgisayarı barındıran platform, işletim ortamı ve erişilen verilerin belirli konumu da dahil olmak üzere bir dizi faktöre bağlıdır. Bu nedenle, bu yöntem üzerinde derinlemesine durmayacağız. Yine de bunu kendi başınıza keşfetmekten çekinmeyin. Sadece bir arama uzaklığında birçok yararlı çevrimiçi kaynak bulacaksınız.

## Temel çıkarımlar

Çok sayıda farklı veri türü vardır, bu da verileri içe aktarmanın çok sayıda yolu olduğu anlamına gelir. İster bir veri dosyasından ister bir veritabanından olsun, verileri içe aktarmak için çeşitli yöntemler öğrenmek, bir veri uzmanı olarak kariyeriniz için sağlam bir temel oluşturacaktır.

## Daha fazla bilgi için kaynaklar

Python'a veri aktarma hakkında daha fazla bilgi edinmek için aşağıdaki bağlantılara başvurabilirsiniz:

-   [Python'da veri içe aktarmaya genel bir bakış](https://towardsdatascience.com/an-overview-of-importing-data-in-python-ac6aa46e0889)
    
-   [Bir Colab'dan BigQuery'ye nasıl bağlanılır](https://colab.sandbox.google.com/notebooks/bigquery.ipynb#scrollTo=fkhbyGaXKs_6)

