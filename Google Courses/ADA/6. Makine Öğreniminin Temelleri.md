# Karar ağaçlarını keşfedin

Bildiğiniz gibi ağaç tabanlı öğrenme, günümüzde endüstride kullanılan en etkili makine öğrenimi tekniklerinden biridir. Birçok farklı algoritma, tahminlerini yapmak için ağaç tabanlı bir mimari kullanır. Karar ağacı, bu algoritmaların temel yapı taşı olmasının yanı sıra kendi içinde güçlü bir tahmin algoritmasıdır. Veri uzmanları, modelleme kararlarını geliştiren güçlü araçlar olarak karar ağaçlarına güvenmektedir. Bu okumada karar ağaçlarını, nasıl yapılandırıldıklarını, nasıl çalıştıklarını ve nasıl oluşturulduklarını derinlemesine inceleyeceksiniz.

### **Karar ağacı nedir?**

Karar ağaçları, olayların sonuçlarını, belirli sonuçların olasılığını tahmin etmek veya bir karara varmak için dallanma yollarını kullanan akış şeması benzeri bir yapıdır. Bir spor takımının maçı kazanıp kazanmayacağı gibi belirli bir sınıfın veya sonucun tahmin edildiği sınıflandırma problemleri için kullanılabilirler. Ayrıca, bir arabanın fiyatı gibi sürekli bir değişkenin tahmin edildiği regresyon problemleri için de kullanılabilirler. Bu okuma sınıflandırma ağaçlarına odaklanmaktadır, ancak her iki durumda da modeller aynı temel karar sürecine bağlıdır. 

Karar ağaçları, problemleri analiz etmek için onlarca yıldır kullanılmaktadır. Ancak teknolojik gelişmeler, karar ağaçlarının bilgisayarlar tarafından oluşturulabilmesini sağlayarak insanların tek başına başarabileceğinden çok daha derin ve doğru analizler yapılabilmesini mümkün kılmıştır.

**Not**: Bu okumadaki tüm örnekler sadece **iki** alt düğüme ayrılan düğümleri göstermektedir, çünkü scikit-learn de dahil olmak üzere modelleme kütüphanelerinde bu şekilde uygulanmaktadırlar. Her bir düğümü üç, dört veya daha fazla yeni gruba bölmek teorik olarak mümkün olsa da (daha önce "Futbol oynayayım mı?" örneğinde gösterildiği gibi), hesaplama karmaşıklığı nedeniyle bu pratik olmayan bir yaklaşımdır. İki seviyeli bir ikili bölme, işlevsel olarak tek seviyeli üç yönlü bir bölmeye eşdeğerdir, ancak hesaplama talebi açısından çok daha basittir.

### **Bir sınıflandırma ağacının yapısı**

Karar ağaçları ancak ters çevrildiklerinde gerçek ağaçlara benzerler, çünkü kökleri tepede başlar ve aşağı doğru büyüyerek "yaprakların" altta kalmasını sağlarlar. Karar ağaçları düğümlerden oluşur. Düğümler örnek gruplarıdır. Ağaçta nasıl işlev gördüklerine bağlı olarak farklı düğüm türleri vardır. Bir karar ağacındaki ilk düğüme **kök düğ**üm denir. İlk bölme her zaman kök düğümden çıkar ve örnekleri belirli bir özellik için içerdikleri değerlere göre iki yeni düğüme böler.

Bu iki yeni düğüm kökün **alt** düğümleri olarak adlandırılır. Alt düğüm, bir bölmeden kaynaklanan herhangi bir düğümdür. Çocuğun ayrıldığı düğüm **ana düğ**üm olarak bilinir. Bu iki yeni alt düğümün her biri sırayla verileri yeni bir kritere göre yeniden böler. Bu işlem düğümler ayrılmayı durdurana kadar devam eder. Bölünmeyen alt düzey düğümlere **yaprak düğ**ümler denir. Yaprak düğümlerin üzerindeki tüm düğümler **karar** düğümleri olarak adlandırılır, çünkü hepsi verileri sola veya sağa doğru sıralayan bir karar verir.

![image](images/6001.png)

Decision tree diagram. Root node at top. It splits into 2 decision nodes in the middle & those each split into 2 leaf nodes at bottom.

### **Kararlar ve bölünmeler**

Bir karar ağacında veriler bölünür ve bir yaprak düğüme ulaşana kadar karar düğümlerinden geçirilir. Bir karar düğümü, ortaya çıkan çocuklarındaki sınıfların **safsızlığını** en aza indiren kritere göre bölünür. Safsızlık, sınıfa göre karışım derecesini ifade eder. Düşük safsızlığa sahip düğümlerde bir sınıftan diğerlerinden çok daha fazla bulunur. Mükemmel bir bölme, ortaya çıkan alt düğümlerde hiçbir kirlilik içermeyecektir; her alt düğüm yalnızca tek bir sınıf içerecek şekilde verileri bölecektir. Mümkün olan en kötü bölünme, ortaya çıkan çocuk düğümlerde yüksek safsızlığa sahip olacaktır; çocuk düğümlerin her ikisi de her sınıftan eşit sayıda olacaktır.

![2 ağaç. “En İyi”, saf sınıfların alt düğümlerine ayrılır. “En kötü” her iki düğümde her sınıfın eşit sayılarına bölünür](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/gCjK4NbGSsemYO35NHumYQ_2152cf12c9064e0f8036345d2c6de8f1_JIZkXsfCBU-DTnciU8_2PnyIU0jF8jMsMMeusKbShGS9njv43QxQVzQoy8JYPGfuHmum9oz9y_6E-hkklr0CHOE0Zp7bA3p5cLHCuwCOA4WCZcxn7I6BJ64L8iP-Ht7mYevkd6WZOZw3ZMzzY5kQcZhCcLqLqw9jwJhzY4pbP2KWWBz5qFWkHdVCVPLy4HzsvrgOnAijOGzLaPIy9wVnuzEXN2pl6VzXAzMkrQ.png)

2 trees. “Best” splits into child nodes of pure classes. “Worst” splits to equal numbers of each class in both nodes.

Bir ağaç oluştururken ve yeni bir düğüm büyütürken, veri kümesindeki her tahmin edici değişken için bir dizi potansiyel ayırma noktası oluşturulur. Her bir değişkenin her bir bölünme noktasından kaynaklanacak çocuk düğümlerin "saflığını" hesaplamak için bir algoritma kullanılır. Verileri bölmek için en saf alt düğümleri oluşturan özellik ve bölme noktası seçilir.

Bir değişken için dikkate alınacak potansiyel bölme noktaları kümesini belirlemek için algoritma ilk olarak değişkenin kategorik veya sürekli gibi ne tür bir değişken olduğunu ve bu değişken için var olan değer aralığını tanımlar. 

**Kategorik değişkenler**

Tahmin edici değişken kategorik ise, karar ağacı algoritması kategoriye göre bölmeyi dikkate alacaktır. İşte örnek olması için küçük bir meyve veri kümesi. Veriler, elma ya da üzüm olan meyve örneklerini içermektedir. Ayrıca meyvelerin rengine (sarı, kırmızı veya yeşil) ve santimetre cinsinden çapına sahiptir. Bu değişkenlerin her biri karar ağacı algoritmasını etkiler.

|  **Renk**   | **Çap (cm)** | **Meyve (hedef)** |
|---------|----------|---------------|
|  Sarı   |   3.5    |     Elma      |
|  Sarı   |    7     |     Elma      |
| Kırmızı |    2     |     Üzüm      |
| Kırmızı |   2.5    |     Üzüm      |
|  Yeşil  |    4     |     Üzüm      |
|  Yeşil  |    3     |     Elma      |
| Kırmızı |    6     |     Elma      |

İlk olarak, algoritma kategorik değişken olan renge göre bölmeyi değerlendirecektir. Üç kategori olduğu için üç seçenek değerlendirilmektedir. "Evet "in her zaman sola, "hayır "ın ise sağa gittiğini unutmayın.

![A'dan C'ye kadar etiketlenmiş üç ağaç, her biri meyveyi rengine göre sıralar: kırmızı, sarı veya yeşil.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/I3aWWvp7R2Wo6cIsRyWfzw_1f0c20bee1aa46c4a4c47dce358692f1_9DWZL3245sslsd7plZsGHBeYWwHlKlBMmocHxr3pH40TGUccEXXtwiVyI0KOhK0BhV4tVP7pPZ1biN1DnwWtbSSOXODEvxztQaZRqZJY-8trvuQFMoawILlmZZ-1BPUGS2GqndpgPNXLCOeLHxC399W5461QrfbIjjyJ2MWhlReZ_o6UA_UupU_9j7ultOt-cQK-oDPblGNCJpyX5ckV2Ig3EdB7C2fX4LvuJQ.png)

Three trees, labeled A through C, each sorting the fruit based on its color: red, yellow, or green.

**Sürekli değişkenler**

Tahmin edici değişken sürekliyse, verilerde var olan sayı aralığı boyunca herhangi bir yerde bölünmeler yapılabilir. Genellikle potansiyel ayırma noktaları, özelliğin değerlerini sıralayarak ve her bir ardışık değer çiftinin ortalamasını alarak belirlenir. Bununla birlikte, herhangi bir sayıda bölme noktası olabilir ve hesaplama kaynaklarından ve zamandan tasarruf etmek için daha az bölme noktası düşünülebilir. Özellikle çok geniş sayı aralıklarıyla uğraşırken, dağılımın yüzdelik dilimleri boyunca bölünmüş noktaları dikkate almak çok yaygındır.

Meyve örneğinde, "Çap" sürekli bir değişkendir. Bir karar ağacının bunu halledebilmesinin bir yolu şudur:

1.  **Değerleri sıralayın, ardışık değerlerin ortalamasını belirleyin:**
    

![Çaplar artan sıraya göre sıralanmıştır. Ardışık değerlerin ortalamaları 2.25, 2.75, 3.25, 3.75, 5 ve 6.5 olarak tanımlanır.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/2Xh3JFscQA-waYkHe8HuyA_f93b5d256eef4606aa85dc3975112ff1_ADA_R-028_Diameters-sorted-in-ascending-order.png)

1.  **Belirlenen bu araçlara göre bölünmeyi inceleyin:**
    

![D'den I'ye kadar etiketlenmiş altı ağaç, her biri meyveyi çapa göre sıralar: 2.25, 2.75, 3.25, 3.75, 5 ve 6.5 santimetre.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/wlD_D523R-i8bSrlnlydwg_220501faf3644ed98a75e64f02e2b4f1_yLl4vHnT9rlbEECQeJUtNlvxicIxTCvk0xaM9Q2uKjGJX7MM929X97qleCw4wGGIQxeL-uocs5rJjPT48UVeONoAKmuI6Fhafb4LTqk3ITvBVo6xf4smyDIstJL1k-0n5LK6sMKme6gV-Nv5bNPmferJ00c8DI4pkrcM2JpEV7UyAiGjsed0JKv0C0kxWFDPZt8KOmMbYVM2a6Adiv8Aq_M4fWRdHAok6ccF-A.png)

Six trees, labeled D through I, each sorting the fruit based on diameter: 2.25, 2.75, 3.25, 3.75, 5, and 6.5 centimeters.

Bunlar, Çap özelliği için algoritma tarafından belirlenen altı potansiyel ayırma noktasıdır. Her seçenek o bölünmenin çocuklarını içerir, ancak bu noktada hiçbirinin henüz değerlendirilmediğini unutmayın. Bir sonraki adım bu. 

### **Bölmeleri seçiyorum: Gini safsızlığı**

Artık _potansiyel_ bölünme noktalarını nasıl belirleyeceğinizi biliyorsunuz. Meyve örneğinde, aralarından seçim yapabileceğiniz dokuz seçenek vardır: A–I. Peki hangi bölmeyi kullanacağınıza nasıl karar veriyorsunuz? Çocuk düğümlerin "saflığı" bu noktada önem kazanır. Genel olarak, yukarıdaki E örneğinde olduğu gibi, ortaya çıkan her bir alt düğüm bir sınıfa ait diğerlerinden çok daha fazla örnek içerdiğinde bölmeler daha iyidir, çünkü bu, bölmenin sınıfları etkili bir şekilde ayırdığı anlamına gelir - karar ağacının birincil görevi! Bu gibi durumlarda, çocuk düğümlerin düşük safsızlığa (veya yüksek saflığa) sahip olduğu söylenir. Karar ağacı algoritması, bir hesaplama yaparak çocuk düğümler arasında en düşük safsızlıkla sonuçlanacak bölünmeyi belirler.

Bir düğümün saflığını belirlemek ve nasıl bölüneceğine karar vermek için **Gini safsızlığı, entropi, bilgi kazancı** ve **günlük kaybı** dahil olmak üzere kullanılabilecek birkaç olası ölçüm vardır. En basit olanı Gini safsızlığıdır ve aynı zamanda scikit-learn'deki karar ağacı sınıflandırıcısı için varsayılandır, bu nedenle bu okuma bu yönteme odaklanacaktır. Bir düğümün Gini safsızlığı şu şekilde tanımlanır:

Gini impurity\=1−∑i\=1NP(i)2Gini \\space impurity = 1 - \\sum\_{i=1}^{N} P{(i)}^2 Gini impurity\=1−i\=1∑NP(i)2G, i, n, i, space, i, m, p, u, r, i, t, y, equals, 1, minus, sum, start subscript, i, equals, 1, end subscript, start superscript, N, end superscript, P, left parenthesis, i, right parenthesis, squared

burada iiii

P(i)P(i)P(i)P, left parenthesis, i, right parenthesis = belirli bir düğümde _i_ sınıfına ait örneklerin olasılığıdır

:

Giniimpurity\=1−P(apple)2−P(grape)2Gini\\hspace{1mm}impurity =1- P(apple)^2 - P(grape)^2Giniimpurity\=1−P(apple)2−P(grape)2G, i, n, i, i, m, p, u, r, i, t, y, equals, 1, minus, P, left parenthesis, a, p, p, l, e, right parenthesis, squared, minus, P, left parenthesis, g, r, a, p, e, right parenthesis, squared

Gini safsızlığı, her bir potansiyel bölünme noktasının her bir alt düğümü için hesaplanır. Örneğin, meyve örneğinde (A-I) dokuz ayrık nokta seçeneği vardır. İlk potansiyel ayırma noktası Renk=kırmızı'dır:

![Seçenek A Meyveler renge göre bölünür: kırmızı. 1 elma, 2 üzüm sola “evet” olarak bölünür. 3 elma, 1 üzüm sağdan “hayır” e bölünür.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/124wQy9mSCCmYspwPpiMVw_ff4483d517fe45299a985ad1578260f1_ADA_R-028_Option-A.-Fruits-are-split-based-on-color.png)

**Her bir çocuk düğümün Gini safsızlığını hesaplayın:**

Gini impurity\=1−P(apple)2−P(grape)2Gini\\space impurity =1-P(apple)^2-P(grape)^2 Gini impurity\=1−P(apple)2−P(grape)2G, i, n, i, space, i, m, p, u, r, i, t, y, equals, 1, minus, P, left parenthesis, a, p, p, l, e, right parenthesis, squared, minus, P, left parenthesis, g, r, a, p, e, right parenthesis, squared

\=1−(number of apples in nodetotal number of samples in node)2−(number of grapes in nodetotal number of samples in node)2\= 1 - (\\frac {number\\space of\\space apples\\space in\\space node} {total\\space number\\space of\\space samples\\space in\\space node})^2 - (\\frac {number\\space of\\space grapes\\space in\\space node} {total\\space number\\space of\\space samples\\space in\\space node})^2\=1−(total number of samples in nodenumber of apples in node)2−(total number of samples in nodenumber of grapes in node)2equals, 1, minus, left parenthesis, start fraction, n, u, m, b, e, r, space, o, f, space, a, p, p, l, e, s, space, i, n, space, n, o, d, e, divided by, t, o, t, a, l, space, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, n, o, d, e, end fraction, right parenthesis, squared, minus, left parenthesis, start fraction, n, u, m, b, e, r, space, o, f, space, g, r, a, p, e, s, space, i, n, space, n, o, d, e, divided by, t, o, t, a, l, space, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, n, o, d, e, end fraction, right parenthesis, squared

"**red=yes**" alt düğümü için:

Gini safsızlığı = 1 - (1/3)<sup><span>2 </span></sup> \- (2/3)<sup><span>2</span></sup>

\= 1 - 0.111 - 0.444   

\= 0,445

Ve "**red=no**" alt düğümü için

Gini safsızlığı = 1 - (3/4)<sup><span>2 </span></sup> \- (1/4)<sup><span>2</span></sup>

\= 1 - 0.5625 - 0.0625   

\= 0,375

Şimdi A seçeneği (meyvenin kırmızı olup olmaması) için iki Gini safsızlık puanı vardır - her bir alt düğüm için bir tane. Son adım, ağırlıklı ortalamalarını alarak bu puanları birleştirmektir. 

**Gini safsızlıklarının ağırlıklı ortalamasını hesaplayın**

Ağırlıklı ortalama, her bir Gini safsızlık skorunda temsil edilen farklı örnek sayısını hesaba katar. İlk alt düğüm üç örnek ve ikinci alt düğüm dört örnek içerdiğinden, bunları basitçe toplayıp ikiye bölemezsiniz. Gini safsızlıklarının_(Gi)_ ağırlıklı ortalaması şu şekilde hesaplanır:

_Toplam Gi:_

\=(number of samples in LEFT childnumber of samples in BOTH child nodes)∗Gileft child+(number of samples in RIGHT childnumber of samples in BOTH child nodes)∗Giright child\= (\\frac {number\\space of\\space samples\\space in\\space LEFT\\space child} {number\\space of\\space samples\\space in\\space BOTH\\space child\\space nodes}) \* Gi \_{left \\space child} + (\\frac {number\\space of\\space samples\\space in\\space RIGHT\\space child} {number\\space of\\space samples\\space in\\space BOTH\\space child\\space nodes}) \* Gi \_{right \\space child}\=(number of samples in BOTH child nodesnumber of samples in LEFT child)∗Gileft child+(number of samples in BOTH child nodesnumber of samples in RIGHT child)∗Giright childequals, left parenthesis, start fraction, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, L, E, F, T, space, c, h, i, l, d, divided by, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, B, O, T, H, space, c, h, i, l, d, space, n, o, d, e, s, end fraction, right parenthesis, times, G, i, start subscript, l, e, f, t, space, c, h, i, l, d, end subscript, plus, left parenthesis, start fraction, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, R, I, G, H, T, space, c, h, i, l, d, divided by, n, u, m, b, e, r, space, o, f, space, s, a, m, p, l, e, s, space, i, n, space, B, O, T, H, space, c, h, i, l, d, space, n, o, d, e, s, end fraction, right parenthesis, times, G, i, start subscript, r, i, g, h, t, space, c, h, i, l, d, end subscript

\= (3/7 \* 0.445) + (4/7 \* 0.375)

<sub><span>Gitotal</span></sub> = 0.405

**Bu işlemi her bölme seçeneği için tekrarlayın**

Aynı işlem her bölme seçeneği için tekrarlanır. Meyve örneğinde dokuz seçenek vardır (A-I):

![Her biri için Gini kirliliği ile A—I seçeneklerini bölün. Puanlar, bölünmüş E için 0.229 ile bölünmüş C için 0.486 arasında değişmektedir.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/FubgiCT3TxytfVDm5keiYg_bdda1267f0214aa0a9b7f3e2ffde40f1_ADA_R-028_fruit-example-has-nine-options.png)

Şimdi 0,229 ile 0,486 arasında değişen dokuz Gini safsızlık puanı vardır. Bu bir safsızlık ölçüsü olduğundan, en iyi skorlar sıfıra en yakın olanlardır. Bu durumda, bu E seçeneğidir. Dokuz seçenekten en kötüsü C seçeneğidir, çünkü sınıfları iyi ayırmaz. Mümkün olan en kötü Gini safsızlık puanı 0,5'tir ve her bir alt düğüm her bir sınıftan eşit sayıda içerdiğinde ortaya çıkar. 

Algoritma potansiyel bölünme noktalarını belirlediğine ve bunlardan kaynaklanan çocuk düğümlerin Gini safsızlığını hesapladığına göre, en düşük Gini safsızlığına sahip bölünme noktasını seçerek ağacı büyütecektir.  

**Ağacı büyütün**

Verilen örnekte, kök düğüm verileri bölmek için bölme seçeneği E'yi kullanacaktır. Soldaki çocuk yaprak düğüm haline gelir, çünkü sadece bir sınıf içerir. Bununla birlikte, sağ çocuk hala sınıf saflığına sahip değildir, bu nedenle yeni bir karar düğümü haline gelir (dayatılan bazı durdurma koşullarının yokluğunda). Yukarıda özetlenen adımlar, en iyi sonucu verecek özelliği ve ayırma değerini belirlemek için bu düğümdeki örnekler üzerinde tekrarlanacaktır. 

![Bölünme seçeneği E 2 çocuğa ayrılır. “Evet” çocuğu bir yaprak düğümüdür. “Hayır” çocuğu saf değildir, bu yüzden bölünmeye devam eder.](Karar%20a%C4%9Fa%C3%A7lar%C4%B1n%C4%B1%20ke%C5%9Ffedin%20%20Coursera/Ffe5O6C8RQiuTy2zy-cuxA_1a3c27a99d6840dfbdb55e77f97de4f1_Hzy3yL5ziTU6mWHrg8OBQUZMZa18I84R1B2g8YrTqScnpjSGH7sAgazDsaNgz_6s_WK1gNcW3YHxGJkMnJ16DtmLSzLJxc3cu7M3adalQyiQKHGFfjelVFhKZ6owQVICSAa7ThWVlz67aO6t3PJfzhfjuw--PZsIjhlkD9kwcvMajI_9ZTHyanNeeb2mNF_teotLv-jaK8wi4DxYFrMjarLk6vwbqLA_nB8Wfg.png)

Bölme işlemi tüm yapraklar saf olana ya da dayatılan bir koşul bölme işlemini durdurana kadar devam edecektir. 

Bu işlemin _çok fazla_ hesaplama gerektirdiğini fark etmiş olabilirsiniz ve bu yalnızca iki özellik ve yedi gözlem içeren bir veri kümesi içindi. Birçok makine öğrenimi algoritmasında olduğu gibi, karar ağaçlarının arkasındaki teori ve metodoloji oldukça basittir ve uzun yıllardır kullanılmaktadır, ancak bu çözümlerin uygulamaya konması güçlü bilgi işlem yeteneklerinin ortaya çıkmasına kadar mümkün olmamıştır.

### **Sınıflandırma ağaçlarının avantajları ve dezavantajları**

**Avantajlar:**

-   Nispeten az sayıda ön işleme adımı gerektirir
    
-   Her tür değişkenle (sürekli, kategorik, ayrık) kolayca çalışabilir
    
-   Normalleştirme veya ölçeklendirme gerektirmez
    
-   Kararlar şeffaftır
    
-   Aşırı tek değişkenli değerlerden etkilenmez
    

**Dezavantajlar:**

-   Diğer algoritmalara göre hesaplama açısından pahalı olabilir
    
-   Verilerdeki küçük değişiklikler tahminlerde önemli değişikliklere yol açabilir
    

### **Önemli çıkarımlar**

Karar ağaçları, verilerdeki diğer algoritmaların yapamayacağı örüntüleri belirleyebilen güçlü tahmin araçlarıdır. Kullanıcı dostudurlar çünkü diğer modellere kıyasla nispeten daha az ön işleme adımı gerektirirler. Bir kök düğümden başlayıp yaprak düğümlerde biten bir dizi karar düğümünden oluşurlar. Eşik olarak tanımlanan belirli özellik değerlerinde verileri bölerek çalışırlar. Bu bölme eşikleri, bunlardan kaynaklanan çocuk düğümlerin safsızlığı hesaplanarak ve en az safsızlığı veren bölme seçilerek belirlenir.