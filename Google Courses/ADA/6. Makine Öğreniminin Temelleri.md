# Karar ağaçlarını keşfedin

Bildiğiniz gibi ağaç tabanlı öğrenme, günümüzde endüstride kullanılan en etkili makine öğrenimi tekniklerinden biridir. Birçok farklı algoritma, tahminlerini yapmak için ağaç tabanlı bir mimari kullanır. Karar ağacı, bu algoritmaların temel yapı taşı olmasının yanı sıra kendi içinde güçlü bir tahmin algoritmasıdır. Veri uzmanları, modelleme kararlarını geliştiren güçlü araçlar olarak karar ağaçlarına güvenmektedir. Bu okumada karar ağaçlarını, nasıl yapılandırıldıklarını, nasıl çalıştıklarını ve nasıl oluşturulduklarını derinlemesine inceleyeceksiniz.

### **Karar ağacı nedir?**

Karar ağaçları, olayların sonuçlarını, belirli sonuçların olasılığını tahmin etmek veya bir karara varmak için dallanma yollarını kullanan akış şeması benzeri bir yapıdır. Bir spor takımının maçı kazanıp kazanmayacağı gibi belirli bir sınıfın veya sonucun tahmin edildiği sınıflandırma problemleri için kullanılabilirler. Ayrıca, bir arabanın fiyatı gibi sürekli bir değişkenin tahmin edildiği regresyon problemleri için de kullanılabilirler. Bu okuma sınıflandırma ağaçlarına odaklanmaktadır, ancak her iki durumda da modeller aynı temel karar sürecine bağlıdır. 

Karar ağaçları, problemleri analiz etmek için onlarca yıldır kullanılmaktadır. Ancak teknolojik gelişmeler, karar ağaçlarının bilgisayarlar tarafından oluşturulabilmesini sağlayarak insanların tek başına başarabileceğinden çok daha derin ve doğru analizler yapılabilmesini mümkün kılmıştır.

**Not**: Bu okumadaki tüm örnekler sadece **iki** alt düğüme ayrılan düğümleri göstermektedir, çünkü scikit-learn de dahil olmak üzere modelleme kütüphanelerinde bu şekilde uygulanmaktadırlar. Her bir düğümü üç, dört veya daha fazla yeni gruba bölmek teorik olarak mümkün olsa da (daha önce "Futbol oynayayım mı?" örneğinde gösterildiği gibi), hesaplama karmaşıklığı nedeniyle bu pratik olmayan bir yaklaşımdır. İki seviyeli bir ikili bölme, işlevsel olarak tek seviyeli üç yönlü bir bölmeye eşdeğerdir, ancak hesaplama talebi açısından çok daha basittir.

### **Bir sınıflandırma ağacının yapısı**

Karar ağaçları ancak ters çevrildiklerinde gerçek ağaçlara benzerler, çünkü kökleri tepede başlar ve aşağı doğru büyüyerek "yaprakların" altta kalmasını sağlarlar. Karar ağaçları düğümlerden oluşur. Düğümler örnek gruplarıdır. Ağaçta nasıl işlev gördüklerine bağlı olarak farklı düğüm türleri vardır. Bir karar ağacındaki ilk düğüme **kök düğ**üm denir. İlk bölme her zaman kök düğümden çıkar ve örnekleri belirli bir özellik için içerdikleri değerlere göre iki yeni düğüme böler.

Bu iki yeni düğüm kökün **alt** düğümleri olarak adlandırılır. Alt düğüm, bir bölmeden kaynaklanan herhangi bir düğümdür. Çocuğun ayrıldığı düğüm **ana düğ**üm olarak bilinir. Bu iki yeni alt düğümün her biri sırayla verileri yeni bir kritere göre yeniden böler. Bu işlem düğümler ayrılmayı durdurana kadar devam eder. Bölünmeyen alt düzey düğümlere **yaprak düğ**ümler denir. Yaprak düğümlerin üzerindeki tüm düğümler **karar** düğümleri olarak adlandırılır, çünkü hepsi verileri sola veya sağa doğru sıralayan bir karar verir.

![image](images/6001.png)

### **Kararlar ve bölünmeler**

Bir karar ağacında veriler bölünür ve bir yaprak düğüme ulaşana kadar karar düğümlerinden geçirilir. Bir karar düğümü, ortaya çıkan çocuklarındaki sınıfların **safsızlığını** en aza indiren kritere göre bölünür. Safsızlık, sınıfa göre karışım derecesini ifade eder. Düşük safsızlığa sahip düğümlerde bir sınıftan diğerlerinden çok daha fazla bulunur. Mükemmel bir bölme, ortaya çıkan alt düğümlerde hiçbir kirlilik içermeyecektir; her alt düğüm yalnızca tek bir sınıf içerecek şekilde verileri bölecektir. Mümkün olan en kötü bölünme, ortaya çıkan çocuk düğümlerde yüksek safsızlığa sahip olacaktır; çocuk düğümlerin her ikisi de her sınıftan eşit sayıda olacaktır.

![image](images/6002.png)

Bir ağaç oluştururken ve yeni bir düğüm büyütürken, veri kümesindeki her tahmin edici değişken için bir dizi potansiyel ayırma noktası oluşturulur. Her bir değişkenin her bir bölünme noktasından kaynaklanacak çocuk düğümlerin "saflığını" hesaplamak için bir algoritma kullanılır. Verileri bölmek için en saf alt düğümleri oluşturan özellik ve bölme noktası seçilir.

Bir değişken için dikkate alınacak potansiyel bölme noktaları kümesini belirlemek için algoritma ilk olarak değişkenin kategorik veya sürekli gibi ne tür bir değişken olduğunu ve bu değişken için var olan değer aralığını tanımlar. 

**Kategorik değişkenler**

Tahmin edici değişken kategorik ise, karar ağacı algoritması kategoriye göre bölmeyi dikkate alacaktır. İşte örnek olması için küçük bir meyve veri kümesi. Veriler, elma ya da üzüm olan meyve örneklerini içermektedir. Ayrıca meyvelerin rengine (sarı, kırmızı veya yeşil) ve santimetre cinsinden çapına sahiptir. Bu değişkenlerin her biri karar ağacı algoritmasını etkiler.

|  **Renk**   | **Çap (cm)** | **Meyve (hedef)** |
|---------|----------|---------------|
|  Sarı   |   3.5    |     Elma      |
|  Sarı   |    7     |     Elma      |
| Kırmızı |    2     |     Üzüm      |
| Kırmızı |   2.5    |     Üzüm      |
|  Yeşil  |    4     |     Üzüm      |
|  Yeşil  |    3     |     Elma      |
| Kırmızı |    6     |     Elma      |

İlk olarak, algoritma kategorik değişken olan renge göre bölmeyi değerlendirecektir. Üç kategori olduğu için üç seçenek değerlendirilmektedir. "Evet "in her zaman sola, "hayır "ın ise sağa gittiğini unutmayın.

![image](images/6003.png)

**Sürekli değişkenler**

Tahmin edici değişken sürekliyse, verilerde var olan sayı aralığı boyunca herhangi bir yerde bölünmeler yapılabilir. Genellikle potansiyel ayırma noktaları, özelliğin değerlerini sıralayarak ve her bir ardışık değer çiftinin ortalamasını alarak belirlenir. Bununla birlikte, herhangi bir sayıda bölme noktası olabilir ve hesaplama kaynaklarından ve zamandan tasarruf etmek için daha az bölme noktası düşünülebilir. Özellikle çok geniş sayı aralıklarıyla uğraşırken, dağılımın yüzdelik dilimleri boyunca bölünmüş noktaları dikkate almak çok yaygındır.

Meyve örneğinde, "Çap" sürekli bir değişkendir. Bir karar ağacının bunu halledebilmesinin bir yolu şudur:

- **Değerleri sıralayın, ardışık değerlerin ortalamasını belirleyin:**
    
![image](images/6004.png)

- **Belirlenen bu araçlara göre bölünmeyi inceleyin:**
    

![image](images/6005.png)

Bunlar, Çap özelliği için algoritma tarafından belirlenen altı potansiyel ayırma noktasıdır. Her seçenek o bölünmenin çocuklarını içerir, ancak bu noktada hiçbirinin henüz değerlendirilmediğini unutmayın. Bir sonraki adım bu. 

### **Bölmeleri seçiyorum: Gini safsızlığı**

Artık _potansiyel_ bölünme noktalarını nasıl belirleyeceğinizi biliyorsunuz. Meyve örneğinde, aralarından seçim yapabileceğiniz dokuz seçenek vardır: A–I. Peki hangi bölmeyi kullanacağınıza nasıl karar veriyorsunuz? Çocuk düğümlerin "saflığı" bu noktada önem kazanır. Genel olarak, yukarıdaki E örneğinde olduğu gibi, ortaya çıkan her bir alt düğüm bir sınıfa ait diğerlerinden çok daha fazla örnek içerdiğinde bölmeler daha iyidir, çünkü bu, bölmenin sınıfları etkili bir şekilde ayırdığı anlamına gelir - karar ağacının birincil görevi! Bu gibi durumlarda, çocuk düğümlerin düşük safsızlığa (veya yüksek saflığa) sahip olduğu söylenir. Karar ağacı algoritması, bir hesaplama yaparak çocuk düğümler arasında en düşük safsızlıkla sonuçlanacak bölünmeyi belirler.

Bir düğümün saflığını belirlemek ve nasıl bölüneceğine karar vermek için **Gini safsızlığı, entropi, bilgi kazancı** ve **günlük kaybı** dahil olmak üzere kullanılabilecek birkaç olası ölçüm vardır. En basit olanı Gini safsızlığıdır ve aynı zamanda scikit-learn'deki karar ağacı sınıflandırıcısı için varsayılandır, bu nedenle bu okuma bu yönteme odaklanacaktır. Bir düğümün Gini safsızlığı şu şekilde tanımlanır:

$$
\text{Gini impurity} = 1 - \sum_{i=1}^{N} P(i)^2
$$

Burada $i$:

$$
P(i) = \text{belirli bir düğümde } i \text{ sınıfına ait örneklerin olasılığıdır}
$$

Örneğin:

$$
\text{Gini impurity} = 1 - P(\text{apple})^2 - P(\text{grape})^2
$$

Gini safsızlığı, her bir potansiyel bölünme noktasının her bir alt düğümü için hesaplanır. Örneğin, meyve örneğinde (A-I) dokuz ayrık nokta seçeneği vardır. İlk potansiyel ayırma noktası Renk=kırmızı'dır:

![image](images/6006.png)

**Her bir çocuk düğümün Gini safsızlığını hesaplayın:**

$$
\text{Gini impurity} = 1 - P(\text{apple})^2 - P(\text{grape})^2
$$

$$
= 1 - \left( \frac{\text{number of apples in node}}{\text{total number of samples in node}} \right)^2 
- \left( \frac{\text{number of grapes in node}}{\text{total number of samples in node}} \right)^2
$$

**"red=yes"** alt düğümü için:

$$

\text{Gini safsızlığı} = 1 - (1/3)² - (2/3)² = 1 - 0.111 - 0.444 = 0.445

$$

**"red=no"** alt düğümü için:

$$

\text{Gini safsızlığı}  = 1 - (3/4)² - (1/4)² = 1 - 0.5625 - 0.0625 = 0.375

$$


Şimdi A seçeneği (meyvenin kırmızı olup olmaması) için iki Gini safsızlık puanı vardır - her bir alt düğüm için bir tane. Son adım, ağırlıklı ortalamalarını alarak bu puanları birleştirmektir. 

**Gini safsızlıklarının ağırlıklı ortalamasını hesaplayın**

Ağırlıklı ortalama, her bir Gini safsızlık skorunda temsil edilen farklı örnek sayısını hesaba katar. İlk alt düğüm üç örnek ve ikinci alt düğüm dört örnek içerdiğinden, bunları basitçe toplayıp ikiye bölemezsiniz. Gini safsızlıklarının_(Gi)_ ağırlıklı ortalaması şu şekilde hesaplanır:
Aşağıda görseldeki içeriğin Markdown + LaTeX biçimine çevrilmiş hali bulunmaktadır:


**Toplam Gi:**

$$
= \left( \frac{\text{SOL alt düğümdeki örnek sayısı}}{\text{HER İKİ alt düğümdeki örnek sayısı}} \right) \cdot Gi_{\text{SOL}} 
+ \left( \frac{\text{SAĞ alt düğümdeki örnek sayısı}}{\text{HER İKİ alt düğümdeki örnek sayısı}} \right) \cdot Gi_{\text{SAĞ}}
$$

$$
= (3/7 \cdot 0.445) + (4/7 \cdot 0.375)
$$

$$
\text{Gini toplam (Gi total)} = 0.405
$$

**Bu işlemi her bölme seçeneği için tekrarlayın**

Aynı işlem her bölme seçeneği için tekrarlanır. Meyve örneğinde dokuz seçenek vardır (A-I):

![image](images/6007.png)

Şimdi 0,229 ile 0,486 arasında değişen dokuz Gini safsızlık puanı vardır. Bu bir safsızlık ölçüsü olduğundan, en iyi skorlar sıfıra en yakın olanlardır. Bu durumda, bu E seçeneğidir. Dokuz seçenekten en kötüsü C seçeneğidir, çünkü sınıfları iyi ayırmaz. Mümkün olan en kötü Gini safsızlık puanı 0,5'tir ve her bir alt düğüm her bir sınıftan eşit sayıda içerdiğinde ortaya çıkar. 

Algoritma potansiyel bölünme noktalarını belirlediğine ve bunlardan kaynaklanan çocuk düğümlerin Gini safsızlığını hesapladığına göre, en düşük Gini safsızlığına sahip bölünme noktasını seçerek ağacı büyütecektir.  

**Ağacı büyütün**

Verilen örnekte, kök düğüm verileri bölmek için bölme seçeneği E'yi kullanacaktır. Soldaki çocuk yaprak düğüm haline gelir, çünkü sadece bir sınıf içerir. Bununla birlikte, sağ çocuk hala sınıf saflığına sahip değildir, bu nedenle yeni bir karar düğümü haline gelir (dayatılan bazı durdurma koşullarının yokluğunda). Yukarıda özetlenen adımlar, en iyi sonucu verecek özelliği ve ayırma değerini belirlemek için bu düğümdeki örnekler üzerinde tekrarlanacaktır. 

![image](images/6008.png)

Bölme işlemi tüm yapraklar saf olana ya da dayatılan bir koşul bölme işlemini durdurana kadar devam edecektir. 

Bu işlemin _çok fazla_ hesaplama gerektirdiğini fark etmiş olabilirsiniz ve bu yalnızca iki özellik ve yedi gözlem içeren bir veri kümesi içindi. Birçok makine öğrenimi algoritmasında olduğu gibi, karar ağaçlarının arkasındaki teori ve metodoloji oldukça basittir ve uzun yıllardır kullanılmaktadır, ancak bu çözümlerin uygulamaya konması güçlü bilgi işlem yeteneklerinin ortaya çıkmasına kadar mümkün olmamıştır.

### **Sınıflandırma ağaçlarının avantajları ve dezavantajları**

**Avantajlar:**

-   Nispeten az sayıda ön işleme adımı gerektirir
    
-   Her tür değişkenle (sürekli, kategorik, ayrık) kolayca çalışabilir
    
-   Normalleştirme veya ölçeklendirme gerektirmez
    
-   Kararlar şeffaftır
    
-   Aşırı tek değişkenli değerlerden etkilenmez
    

**Dezavantajlar:**

-   Diğer algoritmalara göre hesaplama açısından pahalı olabilir
    
-   Verilerdeki küçük değişiklikler tahminlerde önemli değişikliklere yol açabilir
    

### **Önemli çıkarımlar**

Karar ağaçları, verilerdeki diğer algoritmaların yapamayacağı örüntüleri belirleyebilen güçlü tahmin araçlarıdır. Kullanıcı dostudurlar çünkü diğer modellere kıyasla nispeten daha az ön işleme adımı gerektirirler. Bir kök düğümden başlayıp yaprak düğümlerde biten bir dizi karar düğümünden oluşurlar. Eşik olarak tanımlanan belirli özellik değerlerinde verileri bölerek çalışırlar. Bu bölme eşikleri, bunlardan kaynaklanan çocuk düğümlerin safsızlığı hesaplanarak ve en az safsızlığı veren bölme seçilerek belirlenir.

