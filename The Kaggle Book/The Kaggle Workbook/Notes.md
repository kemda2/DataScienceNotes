# BÃ¶lÃ¼m 1: En ÃœnlÃ¼ Tablosal YarÄ±ÅŸma â€“ Porto Seguro'nun GÃ¼venli SÃ¼rÃ¼cÃ¼ Tahmini *(Chapter 1: The Most Renowned Tabular Competition â€“ Porto Seguroâ€™s Safe Driver Prediction)*

Herhangi bir Kaggle yarÄ±ÅŸmasÄ±nda liderlik tablosunun zirvesine nasÄ±l ulaÅŸÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenmek sabÄ±r, **azim** ve en iyi sonuÃ§larÄ± elde etmek iÃ§in rekabet etmenin en iyi yolunu Ã¶ÄŸrenmek iÃ§in **birÃ§ok deneme** gerektirir. Bu nedenle, geÃ§miÅŸteki bazÄ± Kaggle yarÄ±ÅŸmalarÄ±nÄ± deneyerek ve tartÄ±ÅŸmalarÄ± okuyarak, not defterlerini (notebooks) yeniden kullanarak, **Ã¶zellik mÃ¼hendisliÄŸi** (feature engineering) yaparak ve Ã§eÅŸitli modelleri eÄŸiterek liderlik tablosunun zirvesine nasÄ±l ulaÅŸÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenerek bu becerileri daha hÄ±zlÄ± geliÅŸtirmenize yardÄ±mcÄ± olabilecek bir Ã§alÄ±ÅŸma kitabÄ± dÃ¼ÅŸÃ¼ndÃ¼k.

En Ã¼nlÃ¼ tablo (tabular) yarÄ±ÅŸmalarÄ±ndan biri olan **Porto Seguroâ€™s Safe Driver Prediction** ile baÅŸlÄ±yoruz. Bu yarÄ±ÅŸmada, sigortacÄ±lÄ±kta yaygÄ±n bir sorunu Ã§Ã¶zmeniz ve Ã¶nÃ¼mÃ¼zdeki yÄ±l kimin araÃ§ sigortasÄ± talebinde bulunacaÄŸÄ±nÄ± bulmanÄ±z isteniyor. BÃ¶yle bir bilgi, talepte bulunma olasÄ±lÄ±ÄŸÄ± daha yÃ¼ksek olan sÃ¼rÃ¼cÃ¼ler iÃ§in sigorta Ã¼cretini artÄ±rmak ve olasÄ±lÄ±ÄŸÄ± daha dÃ¼ÅŸÃ¼k olanlar iÃ§in dÃ¼ÅŸÃ¼rmek aÃ§Ä±sÄ±ndan faydalÄ±dÄ±r.

Bu yarÄ±ÅŸmayÄ± Ã§Ã¶zmek iÃ§in gerekli olan temel iÃ§gÃ¶rÃ¼leri ve teknik detaylarÄ± aÃ§Ä±klarken, size gerekli kodu gÃ¶sterecek ve **The Kaggle Book**â€™ta bulunan konularÄ± incelemenizi ve sorularÄ± yanÄ±tlamanÄ±zÄ± isteyeceÄŸiz. Ã–yleyse, daha fazla gecikmeden, bu yeni Ã¶ÄŸrenme yolunuza baÅŸlayalÄ±m.

Bu bÃ¶lÃ¼mde ÅŸunlarÄ± Ã¶ÄŸreneceksiniz:

  * Bir **LightGBM** modelini nasÄ±l ayarlayacaÄŸÄ±nÄ±z (tune) ve eÄŸiteceÄŸiniz
  * Bir **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±yÄ± (denoising autoencoder)** nasÄ±l oluÅŸturacaÄŸÄ±nÄ±z ve bunu bir sinir aÄŸÄ±nÄ± beslemek iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ±z
  * Birbirinden oldukÃ§a farklÄ± olan modelleri **etkili bir ÅŸekilde nasÄ±l harmanlayacaÄŸÄ±nÄ±z (blend)**

> Bu bÃ¶lÃ¼mdeki tÃ¼m kod dosyalarÄ± **[https://packt.link/kwbchp1](https://www.google.com/search?q=https://packt.link/kwbchp1)** adresinde bulunabilir.

## YarÄ±ÅŸmayÄ± ve veriyi anlama *(Understanding the competition and the data)*

Porto Seguro, Brezilya'nÄ±n (Brezilya ve Uruguay'da faaliyet gÃ¶steren) Ã¼Ã§Ã¼ncÃ¼ bÃ¼yÃ¼k sigorta ÅŸirketidir; otomobil sigortasÄ± teminatÄ±nÄ±n yanÄ± sÄ±ra pek Ã§ok baÅŸka sigorta Ã¼rÃ¼nÃ¼ de sunmaktadÄ±r ve son 20 yÄ±ldÄ±r fiyatlarÄ±nÄ± belirlemek ve otomatik sigorta teminatÄ±nÄ± daha fazla sÃ¼rÃ¼cÃ¼ iÃ§in daha eriÅŸilebilir kÄ±lmak amacÄ±yla analitik yÃ¶ntemler ve makine Ã¶ÄŸrenimi kullanmaktadÄ±r. GÃ¶revlerini baÅŸarmanÄ±n yeni yollarÄ±nÄ± keÅŸfetmek amacÄ±yla, Kaggle kullanÄ±cÄ±larÄ±nÄ±n (Kagglers) bazÄ± temel analitik problemlerine yeni ve daha iyi Ã§Ã¶zÃ¼mler bulmalarÄ±nÄ± bekleyerek bir yarÄ±ÅŸmaya ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction)) sponsor olmuÅŸlardÄ±r.

YarÄ±ÅŸma, Kaggle kullanÄ±cÄ±larÄ±nÄ±n, bir sÃ¼rÃ¼cÃ¼nÃ¼n Ã¶nÃ¼mÃ¼zdeki yÄ±l bir otomobil sigortasÄ± talebi baÅŸlatma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eden bir model oluÅŸturmasÄ±nÄ± amaÃ§lamaktadÄ±r ki bu oldukÃ§a yaygÄ±n bir gÃ¶rev tÃ¼rÃ¼dÃ¼r (sponsor bunu "sigortacÄ±lÄ±k iÃ§in klasik bir meydan okuma" olarak bahsetmektedir). Talepte bulunma olasÄ±lÄ±ÄŸÄ± hakkÄ±ndaki bu tÃ¼r bilgiler, bir sigorta ÅŸirketi iÃ§in oldukÃ§a deÄŸerli olabilir. BÃ¶yle bir model olmadan, sigorta ÅŸirketleri mÃ¼ÅŸterilerine risklerine bakÄ±lmaksÄ±zÄ±n yalnÄ±zca sabit bir prim uygulayabilir veya kÃ¶tÃ¼ performans gÃ¶steren bir modele sahiplerse, onlara **uygunsuz bir prim** uygulayabilirler. MÃ¼ÅŸterilerin riskini profilendirmedeki yanlÄ±ÅŸlÄ±klar, iyi sÃ¼rÃ¼cÃ¼lere daha yÃ¼ksek sigorta maliyeti yÃ¼klenmesine ve kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼ler iÃ§in fiyatÄ±n dÃ¼ÅŸÃ¼rÃ¼lmesine yol aÃ§abilir. Bunun ÅŸirket Ã¼zerindeki etkisi iki yÃ¶nlÃ¼ olacaktÄ±r: iyi sÃ¼rÃ¼cÃ¼ler sigortalarÄ±nÄ± baÅŸka yerlerde arayacak ve ÅŸirketin portfÃ¶yÃ¼ kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼lerle aÅŸÄ±rÄ± yÃ¼klenecektir (teknik olarak, ÅŸirketin **kÃ¶tÃ¼ bir hasar oranÄ±** olacaktÄ±r: [https://www.investopedia.com/terms/l/loss-ratio.asp](https://www.investopedia.com/terms/l/loss-ratio.asp)). Bunun yerine, ÅŸirket talep olasÄ±lÄ±ÄŸÄ±nÄ± doÄŸru bir ÅŸekilde tahmin edebilirse, mÃ¼ÅŸterilerinden **adil bir fiyat** isteyebilir, bÃ¶ylece pazar paylarÄ±nÄ± artÄ±rabilir, daha memnun mÃ¼ÅŸterilere ve daha dengeli bir mÃ¼ÅŸteri portfÃ¶yÃ¼ne (daha iyi hasar oranÄ±) sahip olabilir ve rezervlerini (ÅŸirketin talepleri Ã¶demek iÃ§in ayÄ±rdÄ±ÄŸÄ± para) daha iyi yÃ¶netebilir.

Bunu yapmak iÃ§in, sponsor eÄŸitim ve test veri kÃ¼meleri saÄŸlamÄ±ÅŸtÄ±r ve veri kÃ¼mesi Ã§ok bÃ¼yÃ¼k olmadÄ±ÄŸÄ± ve Ã§ok iyi hazÄ±rlanmÄ±ÅŸ gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ iÃ§in yarÄ±ÅŸma herkes iÃ§in idealdi.

Verilerin sunulmasÄ±na ayrÄ±lmÄ±ÅŸ olan yarÄ±ÅŸma sayfasÄ±nda ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data)) belirtildiÄŸi gibi:

> Benzer gruplara ait Ã¶zellikler, Ã¶zellik adlarÄ±nda bu ÅŸekilde etiketlenmiÅŸtir (Ã¶rneÄŸin, ind, reg, car, calc).
>
> Ek olarak, Ã¶zellik adlarÄ± ikili (binary) Ã¶zellikleri belirtmek iÃ§in **bin** ve kategorik Ã¶zellikleri belirtmek iÃ§in **cat** sonekini iÃ§erir. Bu tanÄ±mlamalara sahip olmayan Ã¶zellikler ya sÃ¼rekli (continuous) ya da sÄ±ralÄ±dÄ±r (ordinal). **-1** deÄŸerleri, Ã¶zelliÄŸin gÃ¶zlemde **eksik** olduÄŸunu gÃ¶sterir. **Hedef (target)** sÃ¼tunu, poliÃ§e sahibi iÃ§in bir talep aÃ§Ä±lÄ±p aÃ§Ä±lmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.

YarÄ±ÅŸma iÃ§in veri hazÄ±rlÄ±ÄŸÄ±, herhangi bir bilgi sÄ±zÄ±ntÄ±sÄ±nÄ± (leak) Ã¶nlemek iÃ§in dikkatlice yapÄ±lmÄ±ÅŸtÄ±r ve Ã¶zelliklerin anlamÄ± hakkÄ±nda gizlilik korunmuÅŸ olsa da, kullanÄ±lan farklÄ± etiketlerin motorlu taÅŸÄ±t sigortasÄ± modellemesinde yaygÄ±n olarak kullanÄ±lan belirli Ã¶zellik tÃ¼rlerine atÄ±fta bulunduÄŸu oldukÃ§a aÃ§Ä±ktÄ±r:

  * **ind**: "Bireysel Ã¶zellikler" (individual characteristics)
  * **car**: "Araba Ã¶zellikleri" (car characteristics)
  * **calc**: "HesaplanmÄ±ÅŸ Ã¶zellikler" (calculated features)
  * **reg**: "BÃ¶lgesel/coÄŸrafi Ã¶zellikler" (regional/geographic features)

Bireysel Ã¶zelliklere gelince, yarÄ±ÅŸma sÄ±rasÄ±nda anlamlarÄ± hakkÄ±nda Ã§ok fazla spekÃ¼lasyon yapÄ±lmÄ±ÅŸtÄ±r. Ã–rneÄŸin ÅŸuraya bakÄ±nÄ±z:

  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489), burada Raddar, **ps\_car\_13** Ã¶zelliÄŸinin iki yÄ±lda bir zorunlu araÃ§ muayeneleri arasÄ±nda kat edilen mesafeyi temsil edebileceÄŸini Ã¶ne sÃ¼rmektedir.
  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488), burada Raddar, **ps\_car\_12** Ã¶zelliÄŸinin bunun yerine motor silindir hacmini temsil edebileceÄŸini Ã¶ne sÃ¼rmektedir.
  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057), burada bazÄ± Ã¶zelliklerin Porto Seguro'nun Ã§evrimiÃ§i teklif formundan tÃ¼retildiÄŸi yÃ¶nÃ¼ndeki Ã¶neriyi okuyabilirsiniz.

TÃ¼m bu ve daha fazla Ã§abaya raÄŸmen, sonunda Ã¶zelliklerin Ã§oÄŸunun anlamÄ± ÅŸimdiye kadar bir **gizem olarak kalmÄ±ÅŸtÄ±r**.

Bu yarÄ±ÅŸmanÄ±n ilginÃ§ gerÃ§ekleri ÅŸunlardÄ±r:

1.  Veriler, Ã¶zellikler anonim olsa da, **gerÃ§ek dÃ¼nyadan** alÄ±nmÄ±ÅŸtÄ±r.
2.  Veriler, herhangi bir tÃ¼r sÄ±zÄ±ntÄ± olmaksÄ±zÄ±n **Ã§ok iyi hazÄ±rlanmÄ±ÅŸtÄ±r** (burada sihirli Ã¶zellikler yoktur â€“ sihirli Ã¶zellik, becerikli bir iÅŸlemle Kaggle yarÄ±ÅŸmasÄ±nda modellerinize yÃ¼ksek tahmin gÃ¼cÃ¼ saÄŸlayabilen bir Ã¶zelliktir).
3.  Test veri seti sadece eÄŸitim veri setiyle aynÄ± kategorik seviyeleri tutmakla kalmaz; aynÄ± zamanda aynÄ± daÄŸÄ±lÄ±mdan geliyormuÅŸ gibi gÃ¶rÃ¼nmektedir, ancak Yuya Yamamoto, verilerin t-SNE ile Ã¶n iÅŸlenmesinin dÃ¼ÅŸmanca doÄŸrulama (adversarial validation) testinin baÅŸarÄ±sÄ±z olmasÄ±na yol aÃ§tÄ±ÄŸÄ±nÄ± iddia etmektedir ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784)).

> ğŸ“ AlÄ±ÅŸtÄ±rma 1
> 
> 
> 
> Ä°lk alÄ±ÅŸtÄ±rma olarak, **The Kaggle Book**'taki **dÃ¼ÅŸmanca doÄŸrulama (adversarial validation)** ile ilgili iÃ§eriklere ve koda (sayfa 179'dan baÅŸlayarak) atÄ±fta bulunarak, eÄŸitim ve test verilerinin **bÃ¼yÃ¼k olasÄ±lÄ±kla aynÄ± veri daÄŸÄ±lÄ±mÄ±ndan** kaynaklandÄ±ÄŸÄ±nÄ± kanÄ±tlayÄ±nÄ±z.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Tilii (Mensur Dlakic, Montana Eyalet Ãœniversitesi'nde DoÃ§ent: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197)) tarafÄ±ndan yapÄ±lan ilginÃ§ bir paylaÅŸÄ±m, t-SNE kullanarak ÅŸunu gÃ¶stermektedir: "**Sigorta parametreleri aÃ§Ä±sÄ±ndan Ã§ok benzer olan birÃ§ok insan vardÄ±r, ancak bunlardan bazÄ±larÄ± talepte bulunacak, bazÄ±larÄ± ise bulunmayacaktÄ±r**." Tilii'nin bahsettiÄŸi ÅŸey, sigortacÄ±lÄ±kta olanlara oldukÃ§a tipiktir; belirli Ã¶ncÃ¼ller (sigorta parametreleri) iÃ§in bir ÅŸeyin olma olasÄ±lÄ±ÄŸÄ± aynÄ±dÄ±r, ancak o olay, olaylar dizisini ne kadar sÃ¼re gÃ¶zlemlediÄŸimize baÄŸlÄ± olarak gerÃ§ekleÅŸir veya gerÃ§ekleÅŸmez.

Ã–rneÄŸin, sigortacÄ±lÄ±kta IoT ve telematik verilerini ele alalÄ±m. Bir sÃ¼rÃ¼cÃ¼nÃ¼n gelecekte talepte bulunup bulunmayacaÄŸÄ±nÄ± tahmin etmek iÃ§in sÃ¼rÃ¼ÅŸ davranÄ±ÅŸÄ±nÄ± analiz etmek oldukÃ§a yaygÄ±ndÄ±r. GÃ¶zlem sÃ¼reniz **Ã§ok kÄ±saysa** (Ã¶rneÄŸin, bu yarÄ±ÅŸmada olduÄŸu gibi bir yÄ±l), Ã§ok kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼lerin bile bir talepte bulunmamasÄ± sÃ¶z konusu olabilir, Ã§Ã¼nkÃ¼ kÃ¶tÃ¼ bir sÃ¼rÃ¼cÃ¼ iÃ§in bile bÃ¶yle bir olayÄ±n kÄ±sa bir zaman diliminde meydana gelme olasÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼ktÃ¼r.

Benzer fikirler, Andy Harless ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735)) tarafÄ±ndan tartÄ±ÅŸÄ±lmaktadÄ±r; Harless, bunun yerine yarÄ±ÅŸmanÄ±n gerÃ§ek gÃ¶revinin "kazaya daha yatkÄ±n olan sÃ¼rÃ¼cÃ¼leri belirleyen **gizli bir sÃ¼rekli deÄŸiÅŸkenin deÄŸerini tahmin etmek**" olduÄŸunu savunur, Ã§Ã¼nkÃ¼ aslÄ±nda "**talepte bulunmak bir sÃ¼rÃ¼cÃ¼nÃ¼n Ã¶zelliÄŸi deÄŸil; ÅŸansÄ±n bir sonucudur**."

## DeÄŸerlendirme metriÄŸini anlama
*(Understanding the evaluation metric)*

Elbette, metninizi TÃ¼rkÃ§eye Ã§evirdim:

-----

YarÄ±ÅŸmada kullanÄ±lan metrik, **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±dÄ±r** (ekonomide kullanÄ±lan benzer Gini katsayÄ±sÄ±/endeksinden almÄ±ÅŸtÄ±r) ve daha Ã¶nce baÅŸka bir yarÄ±ÅŸmada, Allstate Claim Prediction Challenge'da ([https://www.kaggle.com/competitions/ClaimPredictionChallenge](https://www.google.com/search?q=https://www.kaggle.com/competitions/ClaimPredictionChallenge)) kullanÄ±lmÄ±ÅŸtÄ±r. Bu yarÄ±ÅŸmadan, metriÄŸin ne hakkÄ±nda olduÄŸuna dair Ã§ok net bir aÃ§Ä±klama alabiliriz:

> Bir giriÅŸ gÃ¶nderdiÄŸinizde, gÃ¶zlemler "en bÃ¼yÃ¼k tahminden" "en kÃ¼Ã§Ã¼k tahmine" doÄŸru sÄ±ralanÄ±r. Tahminlerinizin devreye girdiÄŸi tek adÄ±m budur, bu nedenle yalnÄ±zca tahminlerinizin belirlediÄŸi sÄ±ra Ã¶nemlidir. GÃ¶zlemleri soldan saÄŸa, en bÃ¼yÃ¼k tahminler solda olacak ÅŸekilde dÃ¼zenlenmiÅŸ olarak gÃ¶rselleÅŸtirin. ArdÄ±ndan soldan saÄŸa hareket ederek ÅŸunu sorarÄ±z: "**Verinin en soldaki %x'lik kÄ±smÄ±nda, gerÃ§ekten gÃ¶zlemlenen kaybÄ±n ne kadarÄ±nÄ± biriktirdiniz?**" Bir model olmadan, tahminlerin %10'unda kaybÄ±n %10'unu biriktirmeyi beklersiniz, bu nedenle model olmamasÄ± (veya bir "sÄ±fÄ±r" modeli) dÃ¼z bir Ã§izgiye ulaÅŸÄ±r. **Sizin eÄŸriniz ile bu dÃ¼z Ã§izgi arasÄ±ndaki alana Gini katsayÄ±sÄ± diyoruz.**
>
> "MÃ¼kemmel" bir model iÃ§in ulaÅŸÄ±labilecek maksimum bir alan vardÄ±r. Biz, modelinizin Gini katsayÄ±sÄ±nÄ± mÃ¼kemmel modelin Gini katsayÄ±sÄ±na bÃ¶lerek **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ±** kullanacaÄŸÄ±z.

Daha iyi bir aÃ§Ä±klama da Kilian Batzner'Ä±n not defterinde (notebook) saÄŸlanmÄ±ÅŸtÄ±r: [https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation](https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation). Kilian, net Ã§izimler ve bazÄ± basit Ã¶rnekler kullanarak, sigorta ÅŸirketlerinin aktÃ¼erya departmanlarÄ± tarafÄ±ndan rutin olarak kullanÄ±lan, ancak Ã§ok yaygÄ±n olmayan bu metriÄŸi anlamlandÄ±rmaya Ã§alÄ±ÅŸmaktadÄ±r.

Bu metrik, yaklaÅŸÄ±k olarak $2 \cdot \text{ROC-AUC} - 1$ formÃ¼lÃ¼ne karÅŸÄ±lÄ±k geldiÄŸi iÃ§in **ROC-AUC skoru** veya **Mannâ€“Whitney U non-parametrik istatistiksel testi** (U istatistiÄŸi, alÄ±cÄ± iÅŸletim karakteristiÄŸi eÄŸrisi altÄ±ndaki alana â€“ AUC'ye eÅŸdeÄŸer olduÄŸundan) ile yaklaÅŸtÄ±rÄ±labilir. DolayÄ±sÄ±yla, **ROC-AUC'yi maksimize etmek, normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ± maksimize etmekle aynÄ±dÄ±r** (bir referans iÃ§in Wikipedia giriÅŸindeki *DiÄŸer istatistiksel Ã¶lÃ§Ã¼mlerle iliÅŸki* bÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z: [https://en.wikipedia.org/wiki/Gini\_coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)).

Metrik, ayrÄ±ca Ã¶lÃ§eklenmiÅŸ tahmin sÄ±rasÄ± (rank) ile Ã¶lÃ§eklenmiÅŸ hedef deÄŸerinin kovaryansÄ± olarak da yaklaÅŸÄ±k olarak ifade edilebilir, bu da daha anlaÅŸÄ±lÄ±r bir sÄ±ra iliÅŸkisi Ã¶lÃ§Ã¼tÃ¼ saÄŸlar (bkz. Dmitriy Guller: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576)).

**AmaÃ§ fonksiyonu** (objective function) aÃ§Ä±sÄ±ndan bakÄ±ldÄ±ÄŸÄ±nda, bir sÄ±nÄ±flandÄ±rma probleminde yapacaÄŸÄ±nÄ±z gibi **ikili log-kaybÄ±** (binary log-loss) iÃ§in optimizasyon yapabilirsiniz. Ne ROC-AUC ne de normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ± tÃ¼revlenebilir deÄŸildir ve bunlar yalnÄ±zca doÄŸrulama kÃ¼mesi Ã¼zerindeki metrik deÄŸerlendirmesi iÃ§in kullanÄ±labilir (Ã¶rneÄŸin, erken durdurma veya bir sinir aÄŸÄ±nda Ã¶ÄŸrenme hÄ±zÄ±nÄ± azaltma iÃ§in). Ancak, log-kaybÄ± iÃ§in optimizasyon yapmak her zaman ROC-AUC'yi ve normalleÅŸtirilmiÅŸ Gini katsayÄ±larÄ±nÄ± iyileÅŸtirmez ve ikisi de doÄŸrudan tÃ¼revlenebilir deÄŸildir.

> AslÄ±nda tÃ¼revlenebilir bir ROC-AUC yaklaÅŸtÄ±rmasÄ± mevcuttur. Bunun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Toon Calders ve Szymon Jaroszewicz'in *Efficient AUC Optimization for Classification* adlÄ± Ã§alÄ±ÅŸmasÄ±nda okuyabilirsiniz. European Conference on Principles of Data Mining and Knowledge Discovery. Springer, Berlin, Heidelberg, 2007: [https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9\_8.pdf](https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf).

Ancak, yarÄ±ÅŸmada bir amaÃ§ fonksiyonu olarak log-kaybÄ±ndan farklÄ± bir ÅŸey kullanmaya ve deÄŸerlendirme metriÄŸi olarak ROC-AUC veya normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±ndan baÅŸka bir ÅŸey kullanmaya gerek olmadÄ±ÄŸÄ± anlaÅŸÄ±lmaktadÄ±r.

Kaggle Not Defterleri arasÄ±nda normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ± hesaplamak iÃ§in aslÄ±nda birkaÃ§ Python uygulamasÄ± bulunmaktadÄ±r. Burada, CPMP'nin ([https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook](https://www.google.com/search?q=https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook)) **Numba** kullanarak hesaplamalarÄ± hÄ±zlandÄ±ran Ã§alÄ±ÅŸmasÄ±nÄ± kullandÄ±k ve Ã¶neriyoruz: bu hem kesin hem de hÄ±zlÄ±dÄ±r.

> ğŸ“ AlÄ±ÅŸtÄ±rma 2
> 
> 
> 
> **The Kaggle Book**'un 5. bÃ¶lÃ¼mÃ¼nde (sayfa 95 ve sonrasÄ±), Ã¶zellikle yeni ve genellikle bilinmeyen yarÄ±ÅŸma metrikleriyle nasÄ±l baÅŸa Ã§Ä±kÄ±lacaÄŸÄ±nÄ± aÃ§Ä±klamÄ±ÅŸtÄ±k.
> 
> 
> 
> Bir alÄ±ÅŸtÄ±rma olarak, Kaggle'da **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ±** bir deÄŸerlendirme metriÄŸi olarak kullanan **kaÃ§ yarÄ±ÅŸma** olduÄŸunu bulabilir misiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

## Michael Jahrer'Ä±n en iyi Ã§Ã¶zÃ¼m fikirlerini inceleme *(Examining the top solution ideas from Michael Jahrer)*

Michael Jahrer ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer), yarÄ±ÅŸma BÃ¼yÃ¼k UstasÄ± ve Netflix Ã–dÃ¼lÃ¼'nÃ¼n "BellKor's Pragmatic Chaos" ekibindeki kazananlarÄ±ndan biri), yarÄ±ÅŸma boyunca halka aÃ§Ä±k liderlik tablosunu uzun sÃ¼re ve belirgin bir farkla Ã¶nde gÃ¶tÃ¼rdÃ¼ ve Ã¶zel Ã§Ã¶zÃ¼mler nihayet aÃ§Ä±klandÄ±ÄŸÄ±nda kazanan ilan edildi.

KÄ±sa sÃ¼re sonra, tartÄ±ÅŸma forumunda, **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±larÄ± (denoising autoencoders)** ve sinir aÄŸlarÄ±nÄ± akÄ±llÄ±ca kullanmasÄ± nedeniyle birÃ§ok Kaggler iÃ§in bir referans haline gelen Ã§Ã¶zÃ¼mÃ¼nÃ¼n kÄ±sa bir Ã¶zetini yayÄ±nladÄ± ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)). Michael, Ã§Ã¶zÃ¼mÃ¼ne iliÅŸkin herhangi bir Python kodu eklememiÅŸ olsa da (kodlama Ã§alÄ±ÅŸmasÄ±nÄ± doÄŸrudan C++/CUDA ile Python kullanmadan yazÄ±lmÄ±ÅŸ "eski usul" ve "dÃ¼ÅŸÃ¼k seviyeli" olarak tanÄ±mlamÄ±ÅŸtÄ±r), yazÄ±larÄ± kullandÄ±ÄŸÄ± modellere, hiperparametrelerine ve mimarilerine dair referanslar aÃ§Ä±sÄ±ndan oldukÃ§a zengindir.

Ã–ncelikle Michael, Ã§Ã¶zÃ¼mÃ¼nÃ¼n altÄ± modelin bir karÄ±ÅŸÄ±mÄ±ndan (bir LightGBM modeli ve beÅŸ sinir aÄŸÄ±) oluÅŸtuÄŸunu aÃ§Ä±klamaktadÄ±r. AyrÄ±ca, muhtemelen **aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting)** nedeniyle, karÄ±ÅŸÄ±ma her bir modelin katkÄ±sÄ±nÄ± aÄŸÄ±rlÄ±klandÄ±rmanÄ±n (ayrÄ±ca doÄŸrusal ve doÄŸrusal olmayan yÄ±ÄŸÄ±nlama (stacking) yapmanÄ±n) bir avantaj saÄŸlamadÄ±ÄŸÄ± iÃ§in, yalnÄ±zca farklÄ± tohumlardan (seed) oluÅŸturulmuÅŸ, **eÅŸit aÄŸÄ±rlÄ±ÄŸa sahip** modellerin bir karÄ±ÅŸÄ±mÄ±na baÅŸvurduÄŸunu belirtmektedir.

Bu iÃ§gÃ¶rÃ¼, Michael'Ä±n yaklaÅŸÄ±mÄ±nÄ± Ã§oÄŸaltma gÃ¶revimizi Ã§ok daha kolay hale getiriyor, Ã§Ã¼nkÃ¼ kendisi ayrÄ±ca, **yalnÄ±zca LightGBM sonuÃ§larÄ±nÄ± oluÅŸturduÄŸu sinir aÄŸlarÄ±ndan biriyle karÄ±ÅŸtÄ±rmanÄ±n (blending)** bile yarÄ±ÅŸmada birinciliÄŸi garantilemek iÃ§in yeterli olacaÄŸÄ±nÄ± da belirtmiÅŸtir.

Bu iÃ§gÃ¶rÃ¼, alÄ±ÅŸtÄ±rma Ã§alÄ±ÅŸmamÄ±zÄ± bir sÃ¼rÃ¼ model yerine, iki iyi tekil modelle sÄ±nÄ±rlayacaktÄ±r. Buna ek olarak, Michael, bazÄ± sÃ¼tunlarÄ± Ã§Ä±karmak ve kategorik Ã¶zellikleri **tek-sÄ±cak kodlama (one-hot encoding)** dÄ±ÅŸÄ±nda Ã§ok az veri iÅŸleme yaptÄ±ÄŸÄ±nÄ± da belirtmiÅŸtir.

## Bir LightGBM gÃ¶nderimi oluÅŸturma *(Building a LightGBM submission)*

AlÄ±ÅŸtÄ±rmamÄ±z LightGBM tabanlÄ± bir Ã§Ã¶zÃ¼m Ã¼zerinde Ã§alÄ±ÅŸmakla baÅŸlÄ±yor. Kodu, Kaggle Notebooks'u kullanarak hemen yÃ¼rÃ¼tÃ¼lmeye hazÄ±r ÅŸekilde ÅŸu adreste bulabilirsiniz: [https://www.kaggle.com/code/lucamassaron/workbook-lgb](https://www.kaggle.com/code/lucamassaron/workbook-lgb). Kodu kolayca eriÅŸilebilir hale getirmiÅŸ olsak da, bunun yerine kodu doÄŸrudan kitaptan yazmanÄ±zÄ± veya kopyalamanÄ±zÄ± ve hÃ¼cre hÃ¼cre Ã§alÄ±ÅŸtÄ±rmanÄ±zÄ± Ã¶neriyoruz; her bir kod satÄ±rÄ±nÄ±n ne yaptÄ±ÄŸÄ±nÄ± anlamak ve Ã§Ã¶zÃ¼mÃ¼ kiÅŸiselleÅŸtirmek, performansÄ±nÄ± daha da artÄ±rabilir.

> LightGBM kullanÄ±rken, **GPU veya TPU hÄ±zlandÄ±rÄ±cÄ±larÄ±ndan hiÃ§birini aÃ§manÄ±za gerek yoktur ve aÃ§mamalÄ±sÄ±nÄ±z**. GPU hÄ±zlandÄ±rmasÄ± yalnÄ±zca LightGBM'nin GPU sÃ¼rÃ¼mÃ¼nÃ¼ yÃ¼klediyseniz yardÄ±mcÄ± olabilir. Bu tÃ¼r GPU hÄ±zlandÄ±rmalÄ± bir sÃ¼rÃ¼mÃ¼ Kaggle Notebooks'a nasÄ±l kuracaÄŸÄ±nÄ±za dair Ã§alÄ±ÅŸma ipuÃ§larÄ±nÄ± ÅŸu Ã¶rnekte bulabilirsiniz: [https://www.kaggle.com/code/lucamassaron/gpu-accelerated-lightgbm](https://www.kaggle.com/code/lucamassaron/gpu-accelerated-lightgbm).

Anahtar paketleri (hiperparametre optimizasyonu iÃ§in NumPy, pandas ve Optuna, LightGBM ve bazÄ± yardÄ±mcÄ± fonksiyonlar) iÃ§e aktararak baÅŸlÄ±yoruz. AyrÄ±ca bir konfigÃ¼rasyon sÄ±nÄ±fÄ± tanÄ±mlÄ±yor ve onu Ã¶rneklendiriyoruz. KonfigÃ¼rasyon sÄ±nÄ±fÄ±nda tanÄ±mlanan parametreleri, kodun ilerleyiÅŸi sÄ±rasÄ±nda keÅŸfederken tartÄ±ÅŸacaÄŸÄ±z. Burada Ã¶nemli olan nokta, tÃ¼m parametrelerinizi iÃ§eren bir sÄ±nÄ±f kullanarak, onlarÄ± kod boyunca tutarlÄ± bir ÅŸekilde deÄŸiÅŸtirmenizin daha kolay olacaÄŸÄ±dÄ±r. YarÄ±ÅŸmanÄ±n hararetinde, kodun birden fazla yerinde atÄ±fta bulunulan bir parametreyi gÃ¼ncellemeyi unutmak kolaydÄ±r ve parametreler hÃ¼crelere ve fonksiyonlara daÄŸÄ±lmÄ±ÅŸsa bunlarÄ± ayarlamak her zaman zordur. Bir konfigÃ¼rasyon sÄ±nÄ±fÄ±, size Ã§ok Ã§aba kazandÄ±rabilir ve yol boyunca hatalardan koruyabilir:

```python
import numpy as np
import pandas as pd
import optuna
import lightgbm as lgb
from path import Path
from sklearn.model_selection import StratifiedKFold

class Config:
    input_path = Path('../input/porto-seguro-safe-driver-prediction')
    optuna_lgb = False
    n_estimators = 1500
    early_stopping_round = 150
    cv_folds = 5
    random_state = 0
    params = {'objective': 'binary',
              'boosting_type': 'gbdt',
              'learning_rate': 0.01,
              'max_bin': 25,
              'num_leaves': 31,
              'min_child_samples': 1500,
              'colsample_bytree': 0.7,
              'subsample_freq': 1,
              'subsample': 0.7,
              'reg_alpha': 1.0,
              'reg_lambda': 1.0,
              'verbosity': 0,
              'random_state': 0}

config = Config()
```

Bir sonraki adÄ±m, eÄŸitim, test ve Ã¶rnek gÃ¶nderim veri setlerini iÃ§e aktarmayÄ± gerektirir. Bunu pandas'Ä±n `read_csv` fonksiyonunu kullanarak yapÄ±yoruz. AyrÄ±ca, yÃ¼klenen `DataFrame`'lerin indeksini her bir veri Ã¶rneÄŸinin tanÄ±mlayÄ±cÄ±sÄ±na (`id` sÃ¼tununa) ayarlÄ±yoruz.

Benzer gruplara ait Ã¶zellikler etiketlendiÄŸi (`ind`, `reg`, `car` ve `calc` etiketlerini kullanarak) ve ayrÄ±ca ikili (binary) ve kategorik Ã¶zellikler kolayca bulunabildiÄŸi (etiketlerinde sÄ±rasÄ±yla `bin` ve `cat` etiketlerini kullanÄ±rlar), bunlarÄ± numaralandÄ±rabilir ve listelere kaydedebiliriz:

```python
train = pd.read_csv(config.input_path / 'train.csv', index_col='id')
test = pd.read_csv(config.input_path / 'test.csv', index_col='id')
submission = pd.read_csv(config.input_path / 'sample_submission.csv', 
index_col='id')
calc_features = [feat for feat in train.columns if "_calc" in feat]
cat_features = [feat for feat in train.columns if "_cat" in feat]
```

Daha sonra, sadece hedefi (0'lar ve 1'lerden oluÅŸan ikili bir hedef) Ã§Ä±karÄ±r ve eÄŸitim veri setinden kaldÄ±rÄ±rÄ±z:

```python
target = train["target"]
train = train.drop("target", axis="columns")
```

Bu noktada, Michael Jahrer'in de iÅŸaret ettiÄŸi gibi, `calc` Ã¶zelliklerini silebiliriz. Bu fikir, Ã¶zellikle not defterlerinde yarÄ±ÅŸma sÄ±rasÄ±nda Ã§okÃ§a yinelenmiÅŸtir ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970)), Ã§Ã¼nkÃ¼ bunlarÄ± silmenin hem yerel Ã§apraz doÄŸrulama skorunu hem de halka aÃ§Ä±k liderlik tablosu skorunu iyileÅŸtirdiÄŸi deneysel olarak doÄŸrulanabilmiÅŸtir (genel bir kural olarak, Ã¶zellik seÃ§imi sÄ±rasÄ±nda her ikisini de takip etmek Ã¶nemlidir). AyrÄ±ca, bu Ã¶zellikler gradyan artÄ±rma (gradient boosting) modellerinde de kÃ¶tÃ¼ performans gÃ¶stermiÅŸtir (Ã¶nemleri her zaman ortalamanÄ±n altÄ±ndadÄ±r).

TartÄ±ÅŸabiliriz ki, bu Ã¶zellikler mÃ¼hendislik Ã¼rÃ¼nÃ¼ Ã¶zellikler olduklarÄ± iÃ§in, orijinal Ã¶zelliklerine gÃ¶re yeni bir bilgi iÃ§ermezler, ancak onlarÄ± iÃ§eren eÄŸitilmiÅŸ herhangi bir modele sadece gÃ¼rÃ¼ltÃ¼ katarlar:

```python
train = train.drop(calc_features, axis="columns")
test = test.drop(calc_features, axis="columns")
```

> ğŸ“ AlÄ±ÅŸtÄ±rma 3
> 
> 
> 
> **The Kaggle Book**'un 220. sayfasÄ±nda (Ã‡alÄ±ÅŸmanÄ±zÄ± deÄŸerlendirmek iÃ§in Ã¶zellik Ã¶nemini kullanma baÅŸlÄ±ÄŸÄ± altÄ±nda) saÄŸlanan Ã¶nerilere dayanarak, bir alÄ±ÅŸtÄ±rma olarak:
> 
> 
> 
> 1.  Bu yarÄ±ÅŸma iÃ§in **kendi Ã¶zellik seÃ§imi** not defterinizi (notebook) kodlayÄ±n.
> 
> 2.  **Hangi Ã¶zelliklerin tutulmasÄ±** ve **hangilerinin atÄ±lmasÄ±** gerektiÄŸini kontrol edin.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Kategorik Ã¶zellikler bunun yerine **tek-sÄ±cak kodlama (one-hot encoded)** ile iÅŸlenir. EÄŸitim ve test veri setlerinde aynÄ± etiketler bulunduÄŸundan (Porto Seguro ekibi tarafÄ±ndan dÃ¼zenlenen dikkatli bir eÄŸitim/test ayrÄ±mÄ±nÄ±n sonucu), olaÄŸan scikit-learn `OneHotEncoder` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) yerine, pandas'Ä±n `get_dummies` fonksiyonunu ([https://pandas.pydata.org/docs/reference/api/pandas.get\_dummies.html](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)) kullanacaÄŸÄ±z. Pandas fonksiyonu, Ã¶zellikler ve seviyeleri eÄŸitim ve test setleri arasÄ±nda farklÄ±lÄ±k gÃ¶sterirse farklÄ± kodlamalar Ã¼retebileceÄŸi iÃ§in, tek-sÄ±cak kodlamanÄ±n her ikisi iÃ§in de aynÄ± olmasÄ±nÄ± saÄŸlayan bir kontrol (`assert`) ekliyoruz:

```python
train = pd.get_dummies(train, columns=cat_features)
test = pd.get_dummies(test, columns=cat_features)
assert((train.columns==test.columns).all())
```

Kategorik Ã¶zelliklerin tek-sÄ±cak kodlanmasÄ±, veri iÅŸleme aÅŸamasÄ±nÄ± tamamlar. Daha Ã¶nce tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z gibi, deÄŸerlendirme metriÄŸimiz olan **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ±** tanÄ±mlamaya geÃ§iyoruz. Daha Ã¶nce bahsedilen CPMP tarafÄ±ndan Ã¶nerilen son derece hÄ±zlÄ± Gini hesaplama kodunu kullanacaÄŸÄ±z.

Bir LightGBM modeli kullanacaÄŸÄ±mÄ±z iÃ§in, **GBM algoritmasÄ±na** eÄŸitim ve doÄŸrulama veri setlerinin deÄŸerlendirmesini onunla Ã§alÄ±ÅŸabilecek bir biÃ§imde dÃ¶ndÃ¼rmek Ã¼zere uygun bir sarmalayÄ±cÄ± (`gini_lgb`) eklememiz gerekiyor (bkz. [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher\_better\#lightgbm.Booster.eval](https://www.google.com/search?q=https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html%3Fhighlight%3Dhigher_better%23lightgbm.Booster.eval) â€“ Her deÄŸerlendirme fonksiyonu iki parametre kabul etmeli: `preds`, `eval_data` ve `(eval_name, eval_result, is_higher_better)` veya bu tÃ¼r demetlerin bir listesini dÃ¶ndÃ¼rmelidir):

```python
from numba import jit
@jit
def eval_gini(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_true = y_true[np.argsort(y_pred)]
    ntrue = 0
    gini = 0
    delta = 0
    n = len(y_true)
    for i in range(n-1, -1, -1):
        y_i = y_true[i]
        ntrue += y_i
        gini += y_i * delta
        delta += 1 - y_i
    gini = 1 - 2 * gini / (ntrue * (n - ntrue))
    return gini

def gini_lgb(y_true, y_pred):
    eval_name = 'normalized_gini_coef'
    eval_result = eval_gini(y_true, y_pred)
    is_higher_better = True
    return eval_name, eval_result, is_higher_better
```

EÄŸitim parametrelerine gelince, Michael Jahrer'in paylaÅŸÄ±mÄ±nda ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)) Ã¶nerdiÄŸi parametrelerin mÃ¼kemmel Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k.

EÄŸer `Config` sÄ±nÄ±fÄ±ndaki `optuna_lgb` bayraÄŸÄ±nÄ± `True` olarak ayarlarsanÄ±z, Optuna ([https://optuna.org/](https://optuna.org/)) ile bir arama yaparak aynÄ± parametreleri veya benzer performans gÃ¶steren parametreleri bulmaya Ã§alÄ±ÅŸabilirsiniz. Burada optimizasyon, eÄŸitim verileri Ã¼zerinde beÅŸ katlÄ± Ã§apraz doÄŸrulama testine dayanarak Ã¶ÄŸrenme oranÄ± (learning rate) ve dÃ¼zenlileÅŸtirme (regularization) parametreleri gibi temel parametreler iÃ§in en iyi deÄŸerleri bulmaya Ã§alÄ±ÅŸÄ±r. Ä°ÅŸleri hÄ±zlandÄ±rmak iÃ§in, doÄŸrulamanÄ±n kendisinde **erken durdurma (early stopping)** dikkate alÄ±nÄ±r (ki bunun, doÄŸrulama katmanÄ±nÄ± daha iyi aÅŸÄ±rÄ± Ã¶ÄŸrenebilecek bazÄ± parametrelerin seÃ§ilmesini destekleyebileceÄŸinin farkÄ±ndayÄ±z â€“ iyi bir alternatif, erken durdurma geri Ã§aÄŸrÄ±sÄ±nÄ± kaldÄ±rmak ve eÄŸitim iÃ§in sabit bir tur sayÄ±sÄ± tutmaktÄ±r):

```python
if config.optuna_lgb:
        
    def objective(trial):
        params = {
    'learning_rate': trial.suggest_float("learning_rate", 0.01, 1.0),
    'num_leaves': trial.suggest_int("num_leaves", 3, 255),
    'min_child_samples': trial.suggest_int("min_child_samples", 
                                           3, 3000),
    'colsample_bytree': trial.suggest_float("colsample_bytree", 
                                            0.1, 1.0),
    'subsample_freq': trial.suggest_int("subsample_freq", 0, 10),
    'subsample': trial.suggest_float("subsample", 0.1, 1.0),
    'reg_alpha': trial.suggest_loguniform("reg_alpha", 1e-9, 10.0),
    'reg_lambda': trial.suggest_loguniform("reg_lambda", 1e-9, 10.0),
        }

        score = list()
        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, 
                              random_state=config.random_state)
        
        for train_idx, valid_idx in skf.split(train, target):
            X_train = train.iloc[train_idx]
            y_train = target.iloc[train_idx]
            X_valid = train.iloc[valid_idx] 
            y_valid = target.iloc[valid_idx]
            
            model = lgb.LGBMClassifier(**params,
                                    n_estimators=1500,
                                    early_stopping_round=150,
                                    force_row_wise=True)
            callbacks=[lgb.early_stopping(stopping_rounds=150, 
                                          verbose=False)]
            
            model.fit(X_train, y_train, 
                      eval_set=[(X_valid, y_valid)],  
                      eval_metric=gini_lgb, callbacks=callbacks)
            
            score.append(
                model.best_score_['valid_0']['normalized_gini_coef'])
                
        return np.mean(score)
    
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=300)
    
    print("Best Gini Normalized Score", study.best_value)
    print("Best parameters", study.best_params)
    
    params = {'objective': 'binary',
              'boosting_type': 'gbdt',
              'verbosity': 0,
              'random_state': 0}
              
    params.update(study.best_params)
    
else:
    params = config.params
```

YarÄ±ÅŸma sÄ±rasÄ±nda Tilii, **Boruta** ([https://github.com/scikit-learn-contrib/boruta\_py](https://github.com/scikit-learn-contrib/boruta_py)) kullanarak Ã¶zellik elemesini test etti. Ã‡ekirdeÄŸini (kernel) burada bulabilirsiniz: [https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.google.com/search?q=https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook). Kontrol edebileceÄŸiniz gibi, Boruta tarafÄ±ndan onaylanmÄ±ÅŸ bir Ã¶zellik olarak kabul edilen **hiÃ§bir `calc_` Ã¶zelliÄŸi yoktur**.

> ğŸ“ AlÄ±ÅŸtÄ±rma 4
> 
> 
> 
> **The Kaggle Book**'ta, **hiperparametre optimizasyonunu** (sayfa 241 ve sonrasÄ±) aÃ§Ä±klÄ±yor ve LightGBM modeli iÃ§in bazÄ± temel hiperparametreler saÄŸlÄ±yoruz.
> 
> 
> 
> Bir alÄ±ÅŸtÄ±rma olarak:
> 
> 
> 
> GerektiÄŸini dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼nÃ¼z yerlerde araÅŸtÄ±rÄ±lan parametreleri azaltarak veya artÄ±rarak **Optuna** ile hiperparametre aramasÄ±nÄ± iyileÅŸtirmeye Ã§alÄ±ÅŸÄ±n ve ayrÄ±ca scikit-learn'den **rastgele arama (random search)** veya **yarÄ±lamalÄ± arama (halving search)** gibi alternatif optimizasyon yÃ¶ntemlerini deneyin (sayfa 245â€“246).
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

En iyi parametrelerimizi elde ettiÄŸimizde (ya da sadece Jahrer'in parametrelerini denediÄŸimizde), **eÄŸitim yapmaya ve tahmin etmeye** hazÄ±rÄ±z. En iyi Ã§Ã¶zÃ¼mÃ¼n Ã¶nerdiÄŸi gibi, stratejimiz **her Ã§apraz doÄŸrulama (cross-validation) katmanÄ±nda** bir model eÄŸitmek ve bu katmanÄ±, test tahminlerinin ortalamasÄ±na katkÄ±da bulunmak iÃ§in kullanmaktÄ±r. AÅŸaÄŸÄ±daki kod parÃ§asÄ±, hem test tahminlerini hem de sonuÃ§larÄ± bir araya getirmek iÃ§in faydalÄ± olacak eÄŸitim veri setindeki **katman dÄ±ÅŸÄ± (out-of-fold)** tahminleri Ã¼retecektir:

```python
preds = np.zeros(len(test))
oof = np.zeros(len(train))
metric_evaluations = list()
skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_
                      state=config.random_state)
                      
for idx, (train_idx, valid_idx) in enumerate(skf.split(train, 
                                                       target)):
    print(f"CV fold {idx}")
    X_train, y_train = train.iloc[train_idx], target.iloc[train_idx]
    X_valid, y_valid = train.iloc[valid_idx], target.iloc[valid_idx]
    
    model = lgb.LGBMClassifier(**params,
                               n_estimators=config.n_estimators,
                    early_stopping_round=config.early_stopping_round,
                               force_row_wise=True)
    
    callbacks=[lgb.early_stopping(stopping_rounds=150), 
               lgb.log_evaluation(period=100, show_stdv=False)]
                                                                                           
    model.fit(X_train, y_train, 
              eval_set=[(X_valid, y_valid)], 
              eval_metric=gini_lgb, callbacks=callbacks)
              
    metric_evaluations.append(
                model.best_score_['valid_0']['normalized_gini_coef'])
    
    # Test tahminlerini toplama
    preds += (model.predict_proba(test,  
              num_iteration=model.best_iteration_)[:,1] 
              / skf.n_splits)
              
    # Katman dÄ±ÅŸÄ± tahminleri kaydetme
    oof[valid_idx] = model.predict_proba(X_valid, 
                    num_iteration=model.best_iteration_)[:,1]
```

Model eÄŸitimi Ã§ok uzun sÃ¼rmemelidir. Sonunda, Ã§apraz doÄŸrulama prosedÃ¼rÃ¼ sÄ±rasÄ±nda elde edilen **NormalleÅŸtirilmiÅŸ Gini KatsayÄ±sÄ±nÄ±** alabilirsiniz:

```python
print(f"LightGBM CV normalized Gini coefficient: 
{np.mean(metric_evaluations):0.3f}
        ({np.std(metric_evaluations):0.3f})")
```

SonuÃ§lar oldukÃ§a cesaret vericidir Ã§Ã¼nkÃ¼ ortalama skor **0.289**'dur ve deÄŸerlerin standart sapmasÄ± oldukÃ§a kÃ¼Ã§Ã¼ktÃ¼r:

```
LightGBM CV Gini Normalized Score: 0.289 (0.015)
```

Geriye kalan tek ÅŸey, katman dÄ±ÅŸÄ± (out-of-fold) ve test tahminlerini bir gÃ¶nderi (submission) olarak kaydetmek ve sonuÃ§larÄ± herkese aÃ§Ä±k ve Ã¶zel liderlik tablolarÄ±nda doÄŸrulamaktÄ±r:

```python
submission['target'] = preds
submission.to_csv('lgb_submission.csv')
oofs = pd.DataFrame({'id':train_index, 'target':oof})
oofs.to_csv('lgb_oof.csv', index=False)
```

Elde edilen herkese aÃ§Ä±k skor **yaklaÅŸÄ±k 0.28442** olmalÄ±dÄ±r. Ä°liÅŸkili Ã¶zel skor ise **yaklaÅŸÄ±k 0.29121** olup, sizi final liderlik tablosunda **29. sÄ±raya** yerleÅŸtirir. OldukÃ§a iyi bir sonuÃ§, ancak bunu hala farklÄ± bir modelle, yani bir **sinir aÄŸÄ±yla harmanlamamÄ±z (blend)** gerekiyor.

EÄŸitim setini **bagging** yapmak (yani, eÄŸitim verilerinin birden fazla Ã¶nyÃ¼klemesini almak ve Ã¶nyÃ¼klemelere dayalÄ± birden fazla model eÄŸitmek), Michael Jahrer'in kendi gÃ¶nderisinde belirttiÄŸi gibi, performansÄ± artÄ±rmalÄ±dÄ±r, ancak bu artÄ±ÅŸ Ã§ok fazla deÄŸildir.

## Bir gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ± (denoising autoencoder) ve bir DNN kurma *(Setting up a denoising autoencoder and a DNN)*

Bir sonraki adÄ±m, ondan Ã¶ÄŸrenip tahmin edebilecek bir **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ± (Denoising Autoencoder - DAE)** ve bir **sinir aÄŸÄ± (Neural Network)** kurmaktÄ±r. Ã‡alÄ±ÅŸan kodu ÅŸu not defterinde bulabilirsiniz: [https://www.kaggle.com/code/lucamassaron/workbook-dae](https://www.kaggle.com/code/lucamassaron/workbook-dae). Not defteri GPU modunda Ã§alÄ±ÅŸtÄ±rÄ±labilir (Kaggle Notebook'ta hÄ±zlandÄ±rÄ±cÄ±larÄ± aÃ§arsanÄ±z daha hÄ±zlÄ± olacaktÄ±r), ancak bazÄ± kÃ¼Ã§Ã¼k deÄŸiÅŸikliklerle CPU modunda da Ã§alÄ±ÅŸabilir.

GÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±larÄ±n Kaggle yarÄ±ÅŸmalarÄ±nda kullanÄ±lmasÄ± hakkÄ±nda daha fazlasÄ±nÄ± **The Kaggle Book**'un 230. sayfasÄ±ndan itibaren okuyabilirsiniz.

Porto Seguro's Safe Driver Prediction YarÄ±ÅŸmasÄ± 18

AslÄ±nda, yarÄ±ÅŸmada Michael Jahrer'in yaklaÅŸÄ±mÄ±nÄ± DAE'ler kullanarak yeniden Ã¼reten hiÃ§bir Ã¶rnek yoktur, bu yÃ¼zden OsciiArt tarafÄ±ndan kodlanmÄ±ÅŸ ([https://www.kaggle.com/code/osciiart/denoising-autoencoder](https://www.kaggle.com/code/osciiart/denoising-autoencoder)) baÅŸka bir yarÄ±ÅŸmadaki bir TensorFlow uygulamasÄ±ndan bir Ã¶rnek aldÄ±k.

Burada, gerekli tÃ¼m paketleri, Ã¶zellikle **TensorFlow** ve **Keras'Ä±** iÃ§e aktararak baÅŸlÄ±yoruz. Birden fazla sinir aÄŸÄ± oluÅŸturacaÄŸÄ±mÄ±z iÃ§in, **deneysel `set_memory_growth`** komutunu kullanarak TensorFlow'a mevcut tÃ¼m GPU belleÄŸini kullanmamasÄ±nÄ± sÃ¶ylÃ¼yoruz. Bu, yol boyunca bellek taÅŸmasÄ± sorunlarÄ± yaÅŸamamÄ±zÄ± Ã¶nlemeye yardÄ±mcÄ± olacaktÄ±r. AyrÄ±ca **Leaky ReLU** aktivasyonunu Ã¶zel bir aktivasyon olarak kaydediyoruz, bÃ¶ylece Keras katmanlarÄ±nda bir dize ile aktivasyon olarak bahsedebiliriz:

```python
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from path import Path
import gc
import optuna
from sklearn.model_selection import StratifiedKFold
from scipy.special import erfinv
import tensorflow as tf

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
    
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import AUC
from tensorflow.keras.utils import get_custom_objects

from tensorflow.keras.layers import Activation, LeakyReLU
get_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})
```

Birden fazla sinir aÄŸÄ± oluÅŸturma ve belleÄŸimizin tÃ¼kenmemesi niyetimizle ilgili olarak, ayrÄ±ca GPU'daki belleÄŸi temizlemek ve artÄ±k ihtiyaÃ§ duyulmayan modelleri kaldÄ±rmak iÃ§in basit bir fonksiyon tanÄ±mlÄ±yoruz:

```python
def gpu_cleanup(objects):
    if objects:
        del(objects)
    K.clear_session()
    gc.collect()
```

AyrÄ±ca, gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ± ve sinir aÄŸÄ±yla ilgili birden fazla parametreyi dikkate almak iÃ§in **Config sÄ±nÄ±fÄ±nÄ± yeniden yapÄ±landÄ±rÄ±yoruz**. LightGBM hakkÄ±nda daha Ã¶nce belirtildiÄŸi gibi, tÃ¼m parametreleri tek bir yerde tutmak, bunlarÄ± tutarlÄ± bir ÅŸekilde deÄŸiÅŸtirmeniz gerektiÄŸinde sÃ¼reci basitleÅŸtirir:

```python
class Config:
    input_path = Path('../input/porto-seguro-safe-driver-prediction')
    dae_batch_size = 128
    dae_num_epoch = 50
    dae_architecture = [1500, 1500, 1500]
    reuse_autoencoder = False
    batch_size = 128
    num_epoch = 150
    units = [64, 32]
    input_dropout=0.06
    dropout=0.08
    regL2=0.09
    activation='selu'
    
    cv_folds = 5
    nas = False
    random_state = 0
    
config = Config()
```

Daha Ã¶nce gÃ¶sterildiÄŸi gibi, veri setlerini yÃ¼klÃ¼yor ve `calc` Ã¶zelliklerini kaldÄ±rarak ve kategorik olanlarÄ± tek-sÄ±cak kodlayarak Ã¶zellikleri iÅŸlemeye devam ediyoruz. Michael Jahrer'in Ã§Ã¶zÃ¼mÃ¼nde belirttiÄŸi gibi, eksik durumlarÄ± **-1** deÄŸeriyle bÄ±rakÄ±yoruz:

```python
train = pd.read_csv(config.input_path / 'train.csv', index_col='id')
test = pd.read_csv(config.input_path / 'test.csv', index_col='id')
submission = pd.read_csv(config.input_path / 'sample_submission.csv', 
index_col='id')
calc_features = [feat for feat in train.columns if "_calc" in feat]
cat_features = [feat for feat in train.columns if "_cat" in feat]
target = train["target"]
train = train.drop("target", axis="columns")
train = train.drop(calc_features, axis="columns")
test = test.drop(calc_features, axis="columns")
train = pd.get_dummies(train, columns=cat_features)
test = pd.get_dummies(test, columns=cat_features)
assert((train.columns==test.columns).all())
```

Ancak, sinir aÄŸlarÄ±yla uÄŸraÅŸtÄ±ÄŸÄ±mÄ±z iÃ§in, ikili veya tek-sÄ±cak kodlanmÄ±ÅŸ kategorik olmayan **tÃ¼m Ã¶zellikleri normalleÅŸtirmemiz** gerekiyor. NormalleÅŸtirme, **yeniden Ã¶lÃ§eklendirme** (sÄ±nÄ±rlÄ± bir deÄŸer aralÄ±ÄŸÄ± ayarlama) ve **merkezleme** (daÄŸÄ±lÄ±mÄ±nÄ±zÄ±n belirli bir deÄŸere, genellikle sÄ±fÄ±ra, merkezlenmesi) anlamÄ±na gelir.

NormalleÅŸtirme, hem otomatik kodlayÄ±cÄ±nÄ±n hem de sinir aÄŸÄ±nÄ±n optimizasyon algoritmasÄ±nÄ±n daha hÄ±zlÄ± iyi bir Ã§Ã¶zÃ¼me yakÄ±nsamasÄ±nÄ± saÄŸlayacaktÄ±r Ã§Ã¼nkÃ¼ optimizasyon sÄ±rasÄ±nda kayÄ±p fonksiyonunun **salÄ±nÄ±m tehlikesini azaltÄ±r**. Ek olarak, normalleÅŸtirme, giriÅŸin aktivasyon fonksiyonlarÄ± aracÄ±lÄ±ÄŸÄ±yla yayÄ±lmasÄ±nÄ± kolaylaÅŸtÄ±rÄ±r.

Ä°statistiksel normalleÅŸtirme (deÄŸerlerinizin daÄŸÄ±lÄ±mÄ±nÄ± sÄ±fÄ±r ortalama ve birim standart sapmaya getirme) yerine, **GaussRank** (Gauss SÄ±ralamasÄ±), deÄŸiÅŸkenlerin daÄŸÄ±lÄ±mÄ±nÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ bir Gauss daÄŸÄ±lÄ±mÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmeye de olanak tanÄ±yan bir prosedÃ¼rdÃ¼r. *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift* ([https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)) gibi bazÄ± makalelerde de belirtildiÄŸi gibi, sinir aÄŸlarÄ± kendilerine bir Gauss giriÅŸi saÄŸlarsanÄ±z daha da iyi performans gÃ¶sterirler. Bu NVIDIA blog gÃ¶nderisine ([https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/](https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/)) gÃ¶re, GaussRank, Ã¶zellikler zaten normal daÄŸÄ±lmÄ±ÅŸsa veya aÅŸÄ±rÄ± asimetrikse (bu durumlarda dÃ¶nÃ¼ÅŸÃ¼mÃ¼n uygulanmasÄ± performansÄ±n kÃ¶tÃ¼leÅŸmesine neden olabilir) hariÃ§, Ã§oÄŸu zaman iÅŸe yarar:

```python
print("Applying GaussRank to columns: ", end='')
to_normalize = list()
for k, col in enumerate(train.columns):
    if '_bin' not in col and '_cat' not in col and '_missing' not in col:
        to_normalize.append(col)
print(to_normalize)

def to_gauss(x): return np.sqrt(2) * erfinv(x) 
def normalize(data, norm_cols):
    n = data.shape[0]
    for col in norm_cols:
        sorted_idx = data[col].sort_values().index.tolist()
        uniform = np.linspace(start=-0.99, stop=0.99, num=n)
        normal = to_gauss(uniform)
        normalized_col = pd.Series(index=sorted_idx, data=normal)
        data[col] = normalized_col
    return data

train = normalize(train, to_normalize)
test = normalize(test, to_normalize)
```

GaussRank dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼, veri setimizdeki tÃ¼m sayÄ±sal Ã¶zellikler Ã¼zerinde, eÄŸitim ve test Ã¶zellikleri Ã¼zerinde ayrÄ± ayrÄ± uygulayabiliriz:

```
Applying GaussRank to columns: ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15']
```

Ã–zellikleri normalleÅŸtirirken, verilerimizi basitÃ§e **NumPy array'ine** ve GPU iÃ§in ideal giriÅŸ olan `float32` deÄŸerlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz:

```python
features = train.columns
train_index = train.index
test_index = test.index
train = train.values.astype(np.float32)
test = test.values.astype(np.float32)
```

Daha sonra, deÄŸerlendirme fonksiyonu, normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ± (daha Ã¶nce aÃ§Ä±klanan koda dayanarak) ve bir Keras modelinin eÄŸitim ve doÄŸrulama setleri Ã¼zerindeki uygunluk geÃ§miÅŸini faydalÄ± bir ÅŸekilde temsil eden bir Ã§izim fonksiyonu gibi bazÄ± kullanÄ±ÅŸlÄ± fonksiyonlarÄ± hazÄ±rlÄ±yoruz:

```python
def plot_keras_history(history, measures):
    rows = len(measures) // 2 + len(measures) % 2
    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))
    plt.subplots_adjust(top = 0.99, bottom=0.01, 
                        hspace=0.4, wspace=0.2)
    try:
        panels = [item for sublist in panels for item in sublist]
    except:
        pass
    for k, measure in enumerate(measures):
        panel = panels[k]
        panel.set_title(measure + ' history')
        panel.plot(history.epoch, history.history[measure],  
                   label="Train "+measure)
        try:
            panel.plot(history.epoch,  
                       history.history["val_"+measure], 
                       label="Validation "+measure)
        except:
            pass
        panel.set(xlabel='epochs', ylabel=measure)
        panel.legend()
    plt.show(fig)
    
from numba import jit
@jit
def eval_gini(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_true = y_true[np.argsort(y_pred)]
    ntrue = 0
    gini = 0
    delta = 0
    
    n = len(y_true)
    for i in range(n-1, -1, -1):
        y_i = y_true[i]
        ntrue += y_i
        gini += y_i * delta
        delta += 1 - y_i
    gini = 1 - 2 * gini / (ntrue * (n - ntrue))
    return gini
```

Sonraki fonksiyonlar aslÄ±nda biraz daha karmaÅŸÄ±k ve hem gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±nÄ±n hem de denetimli sinir aÄŸÄ±nÄ±n iÅŸleyiÅŸiyle daha alakalÄ±dÄ±r. `batch_generator`, veri yÄ±ÄŸÄ±nlarÄ±nÄ±n karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ halde, yÄ±ÄŸÄ±n boyutuna (batch size) gÃ¶re saÄŸlayan bir jeneratÃ¶r oluÅŸturacak bir fonksiyondur. AslÄ±nda baÄŸÄ±msÄ±z bir jeneratÃ¶r olarak deÄŸil, yakÄ±nda aÃ§Ä±klayacaÄŸÄ±mÄ±z daha karmaÅŸÄ±k bir yÄ±ÄŸÄ±n jeneratÃ¶rÃ¼nÃ¼n, **`mixup_generator`'Ä±n**, bir parÃ§asÄ± olarak kullanÄ±lÄ±r:

```python
def batch_generator(x, batch_size, shuffle=True, random_state=None):
    batch_index = 0
    n = x.shape[0]
    while True:
        if batch_index == 0:
            index_array = np.arange(n)
            if shuffle:
                np.random.seed(seed=random_state)
                index_array = np.random.permutation(n)
        current_index = (batch_index * batch_size) % n
        if n >= current_index + batch_size:
            current_batch_size = batch_size
            batch_index += 1
        else:
            current_batch_size = n - current_index
            batch_index = 0
        batch = x[index_array[current_index: current_index + current_batch_size]]
        yield batch
```

`mixup_generator`, DAE'nin eÄŸitim veri setini **aÅŸÄ±rÄ± Ã¶ÄŸrenmesini (overfitting)** Ã¶nlemek iÃ§in **bir miktar gÃ¼rÃ¼ltÃ¼ oluÅŸturmak** ve veriyi zenginleÅŸtirmek amacÄ±yla deÄŸerlerinin kÄ±smen deÄŸiÅŸtirildiÄŸi veri yÄ±ÄŸÄ±nlarÄ±nÄ± dÃ¶ndÃ¼ren bir jeneratÃ¶rdÃ¼r. Bu jeneratÃ¶re, veri setine rastgele deÄŸerler enjekte etmenin ve eÄŸitim iÃ§in kullanÄ±lacak Ã§ok daha fazla Ã¶rnek oluÅŸturmanÄ±n bir yolu olarak bakabilirsiniz. Michael Jahrer tarafÄ±ndan Ã¶nerildiÄŸi gibi, Ã¶zelliklerin **%15'lik sabit bir takas oranÄ±na (swap rate)** gÃ¶re Ã§alÄ±ÅŸÄ±r, bu da her yÄ±ÄŸÄ±nda Ã¶rneÄŸin %15'inin rastgele deÄŸerlerden oluÅŸacaÄŸÄ± anlamÄ±na gelir. Rastgele seÃ§ilen deÄŸerlerin aynÄ± Ã¶zelliklerden rastgele olarak seÃ§ilmiÅŸ olmasÄ± da Ã¶nemlidir, Ã§Ã¼nkÃ¼ bu, yer deÄŸiÅŸtiren rastgele deÄŸerlerin tamamen rastgele olmadÄ±ÄŸÄ±, orijinal Ã¶zelliklerin daÄŸÄ±lÄ±mÄ±ndan geldiÄŸi anlamÄ±na gelir.

Fonksiyon, iki farklÄ± veri yÄ±ÄŸÄ±nÄ± Ã¼retir: biri modele verilecek ve diÄŸeri ise serbest bÄ±rakÄ±lacak yÄ±ÄŸÄ±ndaki deÄŸiÅŸtirilecek deÄŸer iÃ§in bir kaynak olarak kullanÄ±lacaktÄ±r. Temel olasÄ±lÄ±ÄŸÄ± takas oranÄ± olan rastgele bir seÃ§ime dayanarak, her yÄ±ÄŸÄ±nda belirli sayÄ±da Ã¶zellik iki yÄ±ÄŸÄ±n arasÄ±nda takas edilecektir.

Bu, DAE'nin her zaman aynÄ± Ã¶zelliklere gÃ¼venemeyeceÄŸi (Ã§Ã¼nkÃ¼ zaman zaman rastgele takas edilebilirler), bunun yerine aralarÄ±ndaki iliÅŸkileri bulmak ve sÃ¼recin sonunda veriyi doÄŸru bir ÅŸekilde yeniden yapÄ±landÄ±rmak iÃ§in **Ã¶zelliklerin tamamÄ±na odaklanmasÄ±** gerektiÄŸi anlamÄ±na gelir (bir anlamda dropout'a benzer bir ÅŸey):

```python
def mixup_generator(X, batch_size, swaprate=0.15, shuffle=True, random_state=None):
    if random_state is None:
        random_state = np.randint(0, 999)
    num_features = X.shape[1]
    num_swaps = int(num_features * swaprate)    
    generator_a = batch_generator(X, batch_size, shuffle, random_state)
    generator_b = batch_generator(X, batch_size, shuffle, random_state + 1)
    while True:
        batch = next(generator_a)
        mixed_batch = batch.copy()
        effective_batch_size = batch.shape[0]
        alternative_batch = next(generator_b)
        assert((batch != alternative_batch).any())
        for i in range(effective_batch_size):
            swap_idx = np.random.choice(num_features, num_swaps, replace=False)
            mixed_batch[i, swap_idx] = alternative_batch[i, swap_idx]
        yield (mixed_batch, batch)
```

`get_DAE`, gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±yÄ± oluÅŸturan fonksiyondur. Mimarinin tanÄ±mlanmasÄ± iÃ§in bir parametre kabul eder; bu durumda (Michael Jahrer'in Ã§Ã¶zÃ¼mÃ¼nÃ¼n Ã¶nerdiÄŸi gibi) her biri 1.500 dÃ¼ÄŸÃ¼me sahip Ã¼Ã§ katman olarak ayarlanmÄ±ÅŸtÄ±r. Ä°lk katman bir **kodlayÄ±cÄ± (encoder)**, ikincisi verideki bilgiyi ifade edebilen gizli Ã¶zellikleri ideal olarak iÃ§eren bir **darboÄŸaz katmanÄ± (bottleneck layer)** ve son katman ise baÅŸlangÄ±Ã§taki girdi verilerini yeniden yapÄ±landÄ±rabilen bir **kod Ã§Ã¶zme katmanÄ±dÄ±r (decoding layer)**. ÃœÃ§ katmanÄ±n bir `relu` aktivasyon fonksiyonu, yanlÄ±lÄ±k (bias) yok ve her biri bir **toplu normalleÅŸtirme (batch normalization)** katmanÄ± ile takip edilmektedir. Yeniden yapÄ±landÄ±rÄ±lmÄ±ÅŸ girdi verilerine sahip nihai Ã§Ä±ktÄ±, doÄŸrusal (linear) bir aktivasyona sahiptir. EÄŸitim, standart ayarlarla bir **adam iyileÅŸtirici** (optimizer) kullanÄ±larak optimize edilir (optimize edilen maliyet fonksiyonu **ortalama karesel hatadÄ±r â€“ mse**):

```python
def get_DAE(X, architecture=[1500, 1500, 1500]):
    features = X.shape[1]
    inputs = Input((features,))
    for i, nodes in enumerate(architecture):
        layer = Dense(nodes, activation='relu', 
                      use_bias=False, name=f"code_{i+1}")
        if i==0:
            x = layer(inputs)
        else:
            x = layer(x)
        x = BatchNormalization()(x)
    outputs = Dense(features, activation='linear')(x)
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mse', 
                  metrics=['mse', 'mae'])
    return model
```

`extract_dae_features` fonksiyonu burada sadece eÄŸitim amaÃ§lÄ± rapor edilmiÅŸtir. Fonksiyon, eÄŸitilmiÅŸ gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±nÄ±n belirli katmanlarÄ±nÄ±n deÄŸerlerinin Ã§Ä±karÄ±lmasÄ±na yardÄ±mcÄ± olur. Ã‡Ä±karÄ±m, DAE giriÅŸ katmanÄ±nÄ± ve istenen Ã§Ä±kÄ±ÅŸ katmanÄ±nÄ± birleÅŸtirerek **yeni bir model oluÅŸturarak** Ã§alÄ±ÅŸÄ±r. Basit bir tahmin (`predict`) daha sonra ihtiyacÄ±mÄ±z olan deÄŸerleri Ã§Ä±karacaktÄ±r (tahmin, ayrÄ±ca herhangi bir bellek gereksinimine uyacak ÅŸekilde tercih edilen yÄ±ÄŸÄ±n boyutunu ayarlamamÄ±za da olanak tanÄ±r).

YarÄ±ÅŸma durumunda, gÃ¶zlem sayÄ±sÄ± ve otomatik kodlayÄ±cÄ±dan Ã§Ä±karÄ±lacak Ã¶zellik sayÄ±sÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, bu fonksiyonu kullanÄ±rsak, ortaya Ã§Ä±kan yoÄŸun matrisin bir Kaggle Notebook'un belleÄŸi tarafÄ±ndan iÅŸlenmesi **Ã§ok bÃ¼yÃ¼k** olacaktÄ±r. Bu nedenle, stratejimiz orijinal veriyi otomatik kodlayÄ±cÄ± dÃ¼ÄŸÃ¼m deÄŸerlerine (darboÄŸaz katmanÄ±na) dÃ¶nÃ¼ÅŸtÃ¼rmek deÄŸil, bunun yerine otomatik kodlayÄ±cÄ±yÄ±, **dondurulmuÅŸ katmanlarÄ±yla darboÄŸaz katmanÄ±na kadar denetimli sinir aÄŸÄ±yla birleÅŸtirmek** olacaktÄ±r, ki bunu yakÄ±nda tartÄ±ÅŸacaÄŸÄ±z:

```python
def extract_dae_features(autoencoder, X, layers=[3], batch_size=128):
    data = []
    for layer in layers:
        if layer==0:
            data.append(X)
        else:
            get_layer_output = Model([autoencoder.layers[0].input], 
                                  [autoencoder.layers[layer].output])
            layer_output = get_layer_output.predict(X, 
                                              batch_size= batch_size)
            data.append(layer_output)
    data = np.hstack(data)
    return data
```

DAE ile Ã§alÄ±ÅŸmayÄ± tamamlamak iÃ§in, Ã¶nceki tÃ¼m fonksiyonlarÄ± denetimsiz bir eÄŸitim prosedÃ¼rÃ¼ne (en azÄ±ndan bir doÄŸrulama setinde ayarlanmÄ±ÅŸ bir erken durdurma monitÃ¶rÃ¼ olduÄŸu iÃ§in kÄ±smen denetimsiz) saran son bir fonksiyona sahibiz. Fonksiyon, `mix-up` jeneratÃ¶rÃ¼nÃ¼ ayarlar, gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ± mimarisini oluÅŸturur ve ardÄ±ndan, aÅŸÄ±rÄ± Ã¶ÄŸrenme belirtileri varsa erken durdurma iÃ§in bir doÄŸrulama setindeki uygunluÄŸunu izleyerek onu eÄŸitir. Son olarak, eÄŸitilmiÅŸ DAE'yi dÃ¶ndÃ¼rmeden Ã¶nce, eÄŸitim ve doÄŸrulama uygunluÄŸunun bir grafiÄŸini Ã§izer ve modeli diske kaydeder.

Bu model Ã¼zerinde bir tohum (seed) belirlemeye Ã§alÄ±ÅŸsak bile, LightGBM modelinin aksine, sonuÃ§lar **aÅŸÄ±rÄ± derecede deÄŸiÅŸkendir** ve nihai birleÅŸtirme (ensemble) sonuÃ§larÄ±nÄ± etkileyebilir. SonuÃ§ yÃ¼ksek puanlÄ± olsa da, herkese aÃ§Ä±k ve Ã¶zel liderlik tablolarÄ±nda (herkese aÃ§Ä±k sonuÃ§lar Ã¶zel liderlik tablosuyla Ã§ok koreledir) daha yÃ¼ksek veya daha dÃ¼ÅŸÃ¼k sÄ±ralamaya inebilir ve herkese aÃ§Ä±k sonuÃ§larÄ±na dayanarak her zaman en iyi nihai gÃ¶nderimi seÃ§meniz kolay olacaktÄ±r:

```python
def autoencoder_fitting(X_train, X_valid, filename='dae',  
                        random_state=None, suppress_output=False):
    if suppress_output:
        verbose = 0
    else:
        verbose = 2
    print("Fitting a denoising autoencoder")
    tf.random.set_seed(seed=random_state)
    generator = mixup_generator(X_train, 
                                batch_size=config.dae_batch_size, 
                                swaprate=0.15, 
                                random_state=config.random_state)
    dae = get_DAE(X_train, architecture=config.dae_architecture)
    steps_per_epoch = np.ceil(X_train.shape[0] / 
                              config.dae_batch_size)
    early_stopping = EarlyStopping(monitor='val_mse', 
                                mode='min', 
                                patience=5, 
                                restore_best_weights=True,
                                verbose=0)
    history = dae.fit(generator,
                    steps_per_epoch=steps_per_epoch,
                    epochs=config.dae_num_epoch,
                    validation_data=(X_valid, X_valid),
                    callbacks=[early_stopping],
                    verbose=verbose)
    if not suppress_output: plot_keras_history(history, 
                                           measures=['mse', 'mae'])
    dae.save(filename)
    return dae
```

DAE ile uÄŸraÅŸtÄ±ktan sonra, talep beklentilerimizi tahmin etmesi gereken **denetimli sinir modelini** de tanÄ±mlama ÅŸansÄ±nÄ± deÄŸerlendiriyoruz. Ä°lk adÄ±m olarak, Ã§alÄ±ÅŸmanÄ±n tek bir katmanÄ±nÄ± tanÄ±mlamak iÃ§in bir fonksiyon tanÄ±mlÄ±yoruz:

  * **Rastgele normal baÅŸlatma**, Ã§Ã¼nkÃ¼ ampirik olarak bu problemde daha iyi sonuÃ§lara yakÄ±nsadÄ±ÄŸÄ± bulunmuÅŸtur.
  * L2 dÃ¼zenlileÅŸtirmesi ve Ã¶zelleÅŸtirilebilir bir aktivasyon fonksiyonu olan **yoÄŸun bir katman (dense layer)**.
  * Mimariden kolayca dahil edilebilen veya hariÃ§ tutulabilen ayarlanabilir bir **dropout katmanÄ±**.

Ä°ÅŸte yoÄŸun bloklarÄ± oluÅŸturma kodu:

```python
def dense_blocks(x, units, activation, regL2, dropout):
    kernel_initializer = keras.initializers.RandomNormal(mean=0.0, 
                                stddev=0.1, seed=config.random_state)
    for k, layer_units in enumerate(units):
        if regL2 > 0:
            x = Dense(layer_units, activation=activation, 
                      kernel_initializer=kernel_initializer, 
                      kernel_regularizer=l2(regL2))(x)
        else:
            x = Dense(layer_units, 
                      kernel_initializer=kernel_initializer, 
                      activation=activation)(x)
        if dropout > 0:
            x = Dropout(dropout)(x)
    return x
```

Daha Ã¶nce fark etmiÅŸ olabileceÄŸiniz gibi, tek bir katmanÄ± tanÄ±mlayan fonksiyon oldukÃ§a Ã¶zelleÅŸtirilebilir. AynÄ±sÄ±, iÃ§indeki katman ve birim sayÄ±sÄ±, dropout olasÄ±lÄ±klarÄ±, dÃ¼zenlileÅŸtirme ve aktivasyon tÃ¼rÃ¼ iÃ§in girdileri alan sarmalayÄ±cÄ± mimari fonksiyonu iÃ§in de geÃ§erlidir. Fikir, bir **sinir mimarisi aramasÄ± (Neural Architecture Search - NAS)** Ã§alÄ±ÅŸtÄ±rabilmek ve problemimizde hangi konfigÃ¼rasyonun daha iyi performans gÃ¶stereceÄŸini bulmaktÄ±r.

Fonksiyonla ilgili son bir not olarak, girdiler arasÄ±nda eÄŸitilmiÅŸ DAE'yi saÄŸlamak gereklidir, Ã§Ã¼nkÃ¼ giriÅŸleri sinir aÄŸÄ± model giriÅŸleri olarak kullanÄ±lÄ±rken, ilk katmanlarÄ± DAE'nin **darboÄŸaz katmanÄ±na** (DAE mimarisindeki orta katman) baÄŸlanÄ±r. Bu ÅŸekilde, iki modeli fiilen tek bir modelde birleÅŸtirmiÅŸ oluruz (ancak DAE aÄŸÄ±rlÄ±klarÄ± zaten dondurulmuÅŸtur ve eÄŸitilemez).

Bu Ã§Ã¶zÃ¼m, tÃ¼m eÄŸitim verilerinizi dÃ¶nÃ¼ÅŸtÃ¼rmek zorunda kalmamak, bunun yerine yalnÄ±zca sinir aÄŸÄ±nÄ±n iÅŸlediÄŸi tek yÄ±ÄŸÄ±nlarÄ± dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in tasarlanmÄ±ÅŸtÄ±r, bÃ¶ylece sistem belleÄŸinden tasarruf edilir:

```python
def dnn_model(dae, units=[4500, 1000, 1000], 
            input_dropout=0.1, dropout=0.5,
            regL2=0.05,
            activation='relu'):
    inputs = dae.get_layer("code_2").output
    if input_dropout > 0:
        x = Dropout(input_dropout)(inputs)
    else:
        x = tf.keras.layers.Layer()(inputs)
    x = dense_blocks(x, units, activation, regL2, dropout)
    outputs = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=dae.input, outputs=outputs)
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                loss=keras.losses.binary_crossentropy,
                metrics=[AUC(name='auc')])
    return model
```

EÄŸitim sÃ¼recine yÃ¶nelik, tÃ¼m ardÄ±ÅŸÄ±k dÃ¼zeni bir Ã§apraz doÄŸrulama katmanÄ±nda eÄŸitmek iÃ§in tÃ¼m adÄ±mlarÄ± iÃ§eren bir sarmalayÄ±cÄ± ile bitiriyoruz:

```python
def model_fitting(X_train, y_train, X_valid, y_valid, autoencoder, 
                 filename, random_state=None, suppress_output=False):
    if suppress_output:
            verbose = 0
    else:
            verbose = 2
    print("Fitting model")
    
    early_stopping = EarlyStopping(monitor='val_auc', 
                                    mode='max', 
                                    patience=10, 
                                    restore_best_weights=True,
                                    verbose=0)
    
    rlrop = ReduceLROnPlateau(monitor='val_auc', 
                                mode='max',
                                patience=2,
                                factor=0.75,
                                verbose=0)
        
    tf.random.set_seed(seed=random_state)
    model = dnn_model(autoencoder,
                    units=config.units,
                    input_dropout=config.input_dropout,
                    dropout=config.dropout,
                    regL2=config.regL2,
                    activation=config.activation)
        
    history = model.fit(X_train, y_train, 
                            epochs=config.num_epoch, 
                            batch_size=config.batch_size, 
                            validation_data=(X_valid, y_valid),
                            callbacks=[early_stopping, rlrop],
                            shuffle=True,
                            verbose=verbose)
    model.save(filename)
        
    if not suppress_output:  
        plot_keras_history(history, measures=['loss', 'auc'])
    return model, history
```

DAE uygulamamÄ±z, arkasÄ±ndaki fikir aynÄ± olmasÄ±na raÄŸmen, Jahrer'inkinden kesinlikle farklÄ± olduÄŸu iÃ§in, denetimli sinir aÄŸÄ±nÄ±n mimarisi hakkÄ±ndaki gÃ¶zlemlerine tamamen gÃ¼venemeyiz ve LightGBM modelindeki en iyi hiperparametreleri aradÄ±ÄŸÄ±mÄ±z gibi ideal gÃ¶stergeleri aramalÄ±yÄ±z. Optuna'yÄ± kullanarak ve aÄŸÄ± yapÄ±landÄ±rmak iÃ§in ayarladÄ±ÄŸÄ±mÄ±z birden fazla parametreden yararlanarak, bu kod parÃ§asÄ±nÄ± birkaÃ§ saat Ã§alÄ±ÅŸtÄ±rabilir ve neyin daha iyi Ã§alÄ±ÅŸabileceÄŸi hakkÄ±nda bir fikir edinebiliriz.

Deneylerimizde ÅŸunlarÄ± bulduk:

  * SÄ±rasÄ±yla 64 ve 32 dÃ¼ÄŸÃ¼mlÃ¼, **daha az dÃ¼ÄŸÃ¼me sahip iki katmanlÄ± bir aÄŸ** kullanmalÄ±yÄ±z.
  * **GiriÅŸ dropout'u, katmanlar arasÄ±ndaki dropout ve biraz L2 dÃ¼zenlileÅŸtirmesi** yardÄ±mcÄ± olur.
  * **SELU aktivasyon fonksiyonunu** kullanmak daha iyidir.

Ä°ÅŸte tÃ¼m optimizasyon deneylerini Ã§alÄ±ÅŸtÄ±rmak iÃ§in kod parÃ§asÄ±:

```python
if config.nas is True:
    def evaluate():
        metric_evaluations = list()
        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, 
random_state=config.random_state)
        for k, (train_idx, valid_idx) in enumerate(skf.split(train, 
target)):
            
            X_train, y_train = train[train_idx, :], target[train_idx]
            X_valid, y_valid = train[valid_idx, :], target[valid_idx]
            if config.reuse_autoencoder:
                autoencoder = load_model(f"./dae_fold_{k}")
            else:
                autoencoder = autoencoder_fitting(X_train, X_valid,
                                                filename=f'./dae_fold_{k}', 
                                                random_state=config.random_state,
                                                suppress_output=True)
            
            model, _ = model_fitting(X_train, y_train, X_valid, y_valid,
                                        autoencoder=autoencoder,
                                        filename=f"dnn_model_fold_{k}", 
                                        random_state=config.random_state,
                                        suppress_output=True)
            
            val_preds = model.predict(X_valid, batch_size=128, verbose=0)
            best_score = eval_gini(y_true=y_valid, y_pred=np.ravel(val_preds))
            metric_evaluations.append(best_score)
            gpu_cleanup([autoencoder, model])
        return np.mean(metric_evaluations)
    
    def objective(trial):
        params = {
 'first_layer': trial.suggest_categorical("first_layer", 
[8, 16, 32, 64, 128, 256, 512]),
 'second_layer': trial.suggest_categorical("second_layer", 
[0, 8, 16, 32, 64, 128, 256]),
 'third_layer': trial.suggest_categorical("third_layer", 
[0, 8, 16, 32, 64, 128, 256]),
 'input_dropout': trial.suggest_float("input_dropout", 0.0, 
0.5),
 'dropout': trial.suggest_float("dropout", 0.0, 0.5),
 'regL2': trial.suggest_uniform("regL2", 0.0, 0.1),
 'activation': trial.suggest_categorical("activation", 
['relu', 'leaky-relu', 'selu'])
        }
        config.units = [nodes for nodes in [params['first_layer'], 
params['second_layer'], params['third_layer']] if nodes > 0]
        config.input_dropout = params['input_dropout']
        config.dropout = params['dropout']
        config.regL2 = params['regL2']
        config.activation = params['activation']
        return evaluate()
    
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=60)
    
    print("Best Gini Normalized Score", study.best_value)
    print("Best parameters", study.best_params)
    
    config.units = [nodes for nodes in [study.best_params['first_layer'], 
study.best_params['second_layer'], study.best_params['third_layer']] if 
nodes > 0]
    config.input_dropout = study.best_params['input_dropout']
    config.dropout = study.best_params['dropout']
    config.regL2 = study.best_params['regL2']
    config.activation = study.best_params['activation']
```

> ğŸ“ AlÄ±ÅŸtÄ±rma 5
> 
> 
> 
> EÄŸer **NAS (Sinir Mimarisi AramasÄ±)** hakkÄ±nda daha fazla bilgi arÄ±yorsanÄ±z, **The Kaggle Book**'un 276. sayfasÄ±ndan itibaren inceleyebilirsiniz. DAE ve denetimli sinir aÄŸÄ± sÃ¶z konusu olduÄŸunda, Michael Jahrer'in Ã§Ã¶zÃ¼mÃ¼nden kesinlikle farklÄ± bir ÅŸey uyguladÄ±ÄŸÄ±mÄ±z iÃ§in, **en iyi mimariyi aramak kritiktir**.
> 
> 
> 
> Bir alÄ±ÅŸtÄ±rma olarak, Keras'Ä±n yaratÄ±cÄ±sÄ± FranÃ§ois Chollet'in katkÄ±sÄ±nÄ± iÃ§eren, sinir aÄŸlarÄ±nÄ± optimize etmek iÃ§in hÄ±zlÄ± bir Ã§Ã¶zÃ¼m olan **KerasTuner**'Ä± (The Kaggle Book'un 285. sayfasÄ±ndan itibaren bulabilirsiniz) kullanarak hiperparametre aramasÄ±nÄ± iyileÅŸtirmeye Ã§alÄ±ÅŸÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Her ÅŸeyi nihayet hazÄ±r hale getirdikten sonra, eÄŸitime baÅŸlamaya hazÄ±rÄ±z. GPU'lu bir Kaggle Notebook'ta yaklaÅŸÄ±k bir saat iÃ§inde, eksiksiz test ve katman dÄ±ÅŸÄ± (out-of-fold) tahminleri elde edebilirsiniz:

```python
preds = np.zeros(len(test))
oof = np.zeros(len(train))
metric_evaluations = list()
skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_
                      state=config.random_state)
for k, (train_idx, valid_idx) in enumerate(skf.split(train, target)):
    print(f"CV fold {k}")
    X_train, y_train = train[train_idx, :], target[train_idx]
    X_valid, y_valid = train[valid_idx, :], target[valid_idx]
    
    if config.reuse_autoencoder:
        print("restoring previously trained dae")
        autoencoder = load_model(f"./dae_fold_{k}")
    else:
        autoencoder = autoencoder_fitting(X_train, X_valid,
                                        filename=f'./dae_fold_{k}', 
                                        random_state=config.random_state)
        
    model, history = model_fitting(X_train, y_train, X_valid, y_valid,
                                autoencoder=autoencoder,
                                filename=f"dnn_model_fold_{k}", 
                                random_state=config.random_state)
                                
    val_preds = model.predict(X_valid, batch_size=128)
    best_score = eval_gini(y_true=y_valid, 
                           y_pred=np.ravel(val_preds))
    best_epoch = np.argmax(history.history['val_auc']) + 1
    
    print(f"[best epoch is {best_epoch}]\tvalidation_0-gini_dnn: {best_score:0.5f}\n")
    
    metric_evaluations.append(best_score)
    
    # Test tahminlerini toplama
    preds += (model.predict(test, batch_size=128).ravel() / skf.n_splits)
    
    # Katman dÄ±ÅŸÄ± tahminleri kaydetme
    oof[valid_idx] = model.predict(X_valid, batch_size=128).ravel()
    
    # GPU belleÄŸini temizleme
    gpu_cleanup([autoencoder, model])
```

LightGBM modelinde yaptÄ±ÄŸÄ±mÄ±z gibi, ortalama katman normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±na bakarak sonuÃ§lar hakkÄ±nda bir fikir edinebiliriz:

```python
print(f"DNN CV normalized Gini coefficient: {np.mean(metric_evaluations):0.3f} ({np.std(metric_evaluations):0.3f})")
```

SonuÃ§lar, LightGBM kullanÄ±larak daha Ã¶nce elde edilenlerle tamamen uyumlu olmayacaktÄ±r:

```
DNN CV Gini Normalized Score: 0.276 (0.015)
```

GÃ¶nderi dosyasÄ±nÄ± oluÅŸturmak ve gÃ¶ndermek, yaklaÅŸÄ±k **0.27737'lik bir herkese aÃ§Ä±k skor** ve yaklaÅŸÄ±k **0.28471'lik bir Ã¶zel skor** ile sonuÃ§lanacaktÄ±r (daha Ã¶nce bahsettiÄŸimiz gibi sonuÃ§lar bÃ¼yÃ¼k Ã¶lÃ§Ã¼de deÄŸiÅŸebilir) â€“ **Ã§ok yÃ¼ksek bir skor deÄŸil**:

```python
submission['target'] = preds
submission.to_csv('dnn_submission.csv')
oofs = pd.DataFrame({'id':train_index, 'target':oof})
oofs.to_csv('dnn_oof.csv', index=False)
```

Sinir aÄŸÄ±ndan elde edilen zayÄ±f sonuÃ§lar, sinir aÄŸlarÄ±nÄ±n tablo problemlerinde yetersiz performans gÃ¶sterdiÄŸi fikrini doÄŸrular gibi gÃ¶rÃ¼nmektedir. Ancak, bir Kaggler olarak, liderlik tablosunda baÅŸarÄ±lÄ± bir yer edinmek iÃ§in **tÃ¼m modellerin faydalÄ± olduÄŸunu** biliyoruz; sadece onlarÄ± en iyi nasÄ±l kullanacaÄŸÄ±mÄ±zÄ± bulmamÄ±z gerekiyor. Kesinlikle, bir otomatik kodlayÄ±cÄ± ile beslenen bir sinir aÄŸÄ±, verilerdeki gÃ¼rÃ¼ltÃ¼den daha az etkilenen ve bilgiyi bir GBM'den farklÄ± bir ÅŸekilde iÅŸleyen bir Ã§Ã¶zÃ¼m Ã¼retmiÅŸtir.

## SonuÃ§larÄ± birleÅŸtirme *(Ensembling the results)*

ArtÄ±k iki modelimiz olduÄŸuna gÃ¶re, geriye kalan tek ÅŸey onlarÄ± **harmanlamak (mix)** ve sonuÃ§larÄ± iyileÅŸtirip iyileÅŸtiremeyeceÄŸimizi gÃ¶rmektir. Jahrer'in Ã¶nerdiÄŸi gibi doÄŸrudan bir **karÄ±ÅŸÄ±m (blend)** kullanÄ±yoruz, ancak kendimizi sadece ikisinin ortalamasÄ±nÄ± Ã¼retmekle sÄ±nÄ±rlandÄ±rmayacaÄŸÄ±z (Ã§Ã¼nkÃ¼ yaklaÅŸÄ±mÄ±mÄ±z sonuÃ§ta Jahrer'inkinden biraz farklÄ± oldu) ve karÄ±ÅŸÄ±m iÃ§in **en uygun aÄŸÄ±rlÄ±klarÄ±** bulmaya da Ã§alÄ±ÅŸacaÄŸÄ±z. Katman dÄ±ÅŸÄ± (out-of-fold) tahminleri iÃ§e aktararak ve deÄŸerlendirme fonksiyonumuzu hazÄ±rlayarak baÅŸlÄ±yoruz:

```python
import pandas as pd
import numpy as np

from numba import jit
@jit
def eval_gini(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_true = y_true[np.argsort(y_pred)]
    ntrue = 0
    gini = 0
    delta = 0
    n = len(y_true)
    for i in range(n-1, -1, -1):
        y_i = y_true[i]
        ntrue += y_i
        gini += y_i * delta
        delta += 1 - y_i
    gini = 1 - 2 * gini / (ntrue * (n - ntrue))
    return gini

lgb_oof = pd.read_csv("../input/workbook-lgb/lgb_oof.csv")
dnn_oof = pd.read_csv("../input/workbook-dae/dnn_oof.csv")
target = pd.read_csv("../input/porto-seguro-safe-driver-prediction/train.csv", usecols=['id','target'])
```

Bu yapÄ±ldÄ±ktan sonra, LightGBM'in katman dÄ±ÅŸÄ± tahminlerini ve sinir aÄŸÄ±nÄ±n tahminlerini **sÄ±ralara (rank)** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz. Bunu yapÄ±yoruz Ã§Ã¼nkÃ¼ normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ± sÄ±ralamaya dayanÄ±r (tÄ±pkÄ± bir ROC-AUC deÄŸerlendirmesi gibi) ve dolayÄ±sÄ±yla sÄ±ralamalarÄ± harmanlamak, tahmin edilen olasÄ±lÄ±klarÄ± harmanlamaktan daha iyi Ã§alÄ±ÅŸÄ±r:

```python
lgb_oof_ranks = (lgb_oof.target.rank() / len(lgb_oof))
dnn_oof_ranks = (dnn_oof.target.rank() / len(dnn_oof))
```

Åimdi, iki modeli farklÄ± aÄŸÄ±rlÄ±klar kullanarak birleÅŸtirerek katman dÄ±ÅŸÄ± veriler iÃ§in daha iyi bir deÄŸerlendirme elde edip edemeyeceÄŸimizi test ediyoruz:

```python
baseline = eval_gini(y_true=target.target, y_pred=lgb_oof_ranks)
print(f"starting from a oof lgb baseline {baseline:0.5f}\n")
best_alpha = 1.0
for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
    ensemble = alpha * lgb_oof_ranks + (1.0 - alpha) * dnn_oof_ranks
    score = eval_gini(y_true=target.target, y_pred=ensemble)
    print(f"lgd={alpha:0.1f} dnn={(1.0 - alpha):0.1f} -> {score:0.5f}")
    
    if score > baseline:
        baseline = score
        best_alpha = alpha
        
print(f"\nBest alpha is {best_alpha:0.1f}")
```

HazÄ±r olduÄŸumuzda, kod parÃ§asÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak ilginÃ§ sonuÃ§lar elde edebiliriz:

```
starting from a oof lgb baseline 0.28850
lgd=0.1 dnn=0.9 -> 0.27352
lgd=0.2 dnn=0.8 -> 0.27744
lgd=0.3 dnn=0.7 -> 0.28084
lgd=0.4 dnn=0.6 -> 0.28368
lgd=0.5 dnn=0.5 -> 0.28595
lgd=0.6 dnn=0.4 -> 0.28763
lgd=0.7 dnn=0.3 -> 0.28873
lgd=0.8 dnn=0.2 -> 0.28923
lgd=0.9 dnn=0.1 -> 0.28916

Best alpha is 0.8
```

GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re, **LightGBM modeline gÃ¼Ã§lÃ¼ bir aÄŸÄ±rlÄ±k (0.8)** ve sinir aÄŸÄ±na daha zayÄ±f bir aÄŸÄ±rlÄ±k (0.2) harmanlamak, daha da iyi performans gÃ¶steren bir model ortaya Ã§Ä±karacaktÄ±r. Bu hipotezi, modeller iÃ§in aynÄ± aÄŸÄ±rlÄ±klarÄ±n bir karÄ±ÅŸÄ±mÄ±nÄ± ve bulduÄŸumuz ideal aÄŸÄ±rlÄ±klarÄ± ayarlayarak hemen deniyoruz:

```python
lgb_submission = pd.read_csv("../input/workbook-lgb/lgb_submission.csv")
dnn_submission = pd.read_csv("../input/workbook-dae/dnn_submission.csv")
submission = pd.read_csv(
 "../input/porto-seguro-safe-driver-prediction/sample_submission.csv")
```

Ã–nce, Michael Jahrer tarafÄ±ndan kullanÄ±lan strateji olan **eÅŸit aÄŸÄ±rlÄ±klÄ± Ã§Ã¶zÃ¼mÃ¼** deniyoruz:

```python
lgb_ranks = (lgb_submission.target.rank() / len(lgb_submission))
dnn_ranks = (dnn_submission.target.rank() / len(dnn_submission))
submission.target = lgb_ranks * 0.5 + dnn_ranks * 0.5
submission.to_csv("equal_blend_rank.csv", index=False)
```

Bu, nihai liderlik tablosunda 50. pozisyon civarÄ±nda olan bir **0.28393'lÃ¼k herkese aÃ§Ä±k skor** ve **0.29093'lÃ¼k Ã¶zel skor** ile sonuÃ§lanÄ±r, ki bu beklentilerimizden biraz uzaktÄ±r. Åimdi, katman dÄ±ÅŸÄ± tahminlerin bulmamÄ±za yardÄ±mcÄ± olduÄŸu aÄŸÄ±rlÄ±klarÄ± kullanmayÄ± deneyelim:

```python
lgb_ranks = (lgb_submission.target.rank() / len(lgb_submission))
dnn_ranks = (dnn_submission.target.rank() / len(dnn_submission))
submission.target = lgb_ranks * best_alpha +  dnn_ranks * (1.0 - best_alpha)
submission.to_csv("blend_rank.csv", index=False)
```

Burada sonuÃ§lar, **0.28502'lik herkese aÃ§Ä±k skor** ve **0.29192'lik Ã¶zel skor** ile sonuÃ§lanÄ±r, bu da nihai liderlik tablosunda **yedinci pozisyon** civarÄ±na denk gelir. GerÃ§ekten de Ã§ok daha iyi bir sonuÃ§tur, Ã§Ã¼nkÃ¼ LightGBM iyi bir modeldir, ancak muhtemelen verilere **gÃ¼rÃ¼ltÃ¼sÃ¼ giderilmiÅŸ veriler** Ã¼zerinde eÄŸitilmiÅŸ sinir aÄŸÄ±ndan bazÄ± bilgiler eklenerek saÄŸlanabilecek bazÄ± nÃ¼anslarÄ± kaÃ§Ä±rmaktadÄ±r.

-----

> ğŸ“ AlÄ±ÅŸtÄ±rma 6
> 
> 
> 
> CPMP'nin Ã§Ã¶zÃ¼mÃ¼nde ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614)) iÅŸaret ettiÄŸi gibi, Ã§apraz doÄŸrulamanÄ±zÄ± nasÄ±l oluÅŸturduÄŸunuza baÄŸlÄ± olarak, **"katmanlar arasÄ±nda Gini skorlarÄ±nda bÃ¼yÃ¼k bir varyasyon"** yaÅŸayabilirsiniz. Bu nedenle, CPMP, birden fazla Ã§apraz doÄŸrulama iÃ§in **birÃ§ok farklÄ± tohum (seed)** kullanarak ve sonuÃ§larÄ± ortalayarak tahminlerin varyansÄ±nÄ± azaltmayÄ± Ã¶nermektedir.
> 
> 
> 
> Bir alÄ±ÅŸtÄ±rma olarak, Ã¶zellikle **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±** iÃ§in daha kararlÄ± tahminler oluÅŸturmak Ã¼zere kullandÄ±ÄŸÄ±mÄ±z kodu deÄŸiÅŸtirmeye Ã§alÄ±ÅŸÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):


## Ã–zet *(Summary)*

Bu ilk bÃ¶lÃ¼mde, klasik bir tablo (tabular) yarÄ±ÅŸmasÄ±yla ilgilendiniz. YarÄ±ÅŸmanÄ±n not defterlerini ve tartÄ±ÅŸmalarÄ±nÄ± okuyarak, kolayca harmanlanabilen sadece iki modeli iÃ§eren basit bir Ã§Ã¶zÃ¼m ortaya Ã§Ä±kardÄ±k. Ã–zellikle, tablo verileri iÃ§in sinir aÄŸlarÄ±yla Ã§alÄ±ÅŸÄ±rken Ã¶zellikle yararlÄ± olan **alternatif veri iÅŸleme** Ã¼retmek amacÄ±yla bir **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±nÄ±n (denoising autoencoder)** nasÄ±l kullanÄ±lacaÄŸÄ±na dair bir Ã¶rnek sunduk. GeÃ§miÅŸ yarÄ±ÅŸmalardaki Ã§Ã¶zÃ¼mleri anlayÄ±p Ã§oÄŸaltarak, Kaggle yarÄ±ÅŸmalarÄ±ndaki temel yetkinliklerinizi hÄ±zla geliÅŸtirebilir ve daha yeni yarÄ±ÅŸma ve meydan okumalarda sÃ¼rekli olarak daha yÃ¼ksek performans gÃ¶sterme yeteneÄŸi kazanabilirsiniz.

Bir sonraki bÃ¶lÃ¼mde, bu kez **zaman serileriyle** ilgili karmaÅŸÄ±k bir tahmin problemine odaklanan Kaggle'dan baÅŸka bir tablo yarÄ±ÅŸmasÄ±nÄ± inceleyeceÄŸiz.

---

# BÃ¶lÃ¼m 2: Makridakis YarÄ±ÅŸmalarÄ± â€“ DoÄŸruluk ve Belirsizlik Ä°Ã§in Kaggle'daki M5 *(Chapter 2: The Makridakis Competitions â€“ M5 on Kaggle for Accuracy and Uncertainty)*

Elbette, metninizi TÃ¼rkÃ§eye Ã§evirdim:

---

## ğŸ“… M YarÄ±ÅŸmalarÄ± ve M5 Kaggle YarÄ±ÅŸmasÄ±

1982'den beri, **Spyros Makridakis** ([https://mofc.unic.ac.cy/dr-spyros-makridakis/](https://mofc.unic.ac.cy/dr-spyros-makridakis/)) mevcut ve yeni tahmin yÃ¶ntemlerinin farklÄ± tahmin problemlerine karÅŸÄ± etkinliÄŸini karÅŸÄ±laÅŸtÄ±rmak amacÄ±yla **M YarÄ±ÅŸmalarÄ±** adÄ± verilen tahmin meydan okumalarÄ±na dÃ¼nyanÄ±n her yerinden araÅŸtÄ±rmacÄ± gruplarÄ±nÄ± dahil etmiÅŸtir. Bu nedenle, M YarÄ±ÅŸmalarÄ± her zaman hem akademisyenlere hem de uygulayÄ±cÄ±lara tamamen aÃ§Ä±k olmuÅŸtur.

Bu yarÄ±ÅŸmalar muhtemelen **tahmin topluluÄŸunda en Ã§ok alÄ±ntÄ± yapÄ±lan ve atÄ±fta bulunulan** etkinliklerdir ve tahmin yÃ¶ntemlerindeki sÃ¼rekli deÄŸiÅŸen teknoloji haritasÄ±nÄ± her zaman Ã¶ne Ã§Ä±karmÄ±ÅŸlardÄ±r. Ã–nceki her M YarÄ±ÅŸmasÄ±, hem araÅŸtÄ±rmacÄ±lara hem de uygulayÄ±cÄ±lara tahmin araÃ§larÄ±nÄ± eÄŸitmek ve test etmek iÃ§in faydalÄ± veriler saÄŸlamanÄ±n yanÄ± sÄ±ra, tahmin yapÄ±lÄ±ÅŸ biÃ§iminde devrim yaratan bir dizi keÅŸif ve yaklaÅŸÄ±m sunmuÅŸtur.

En sonuncusu olan **M5 YarÄ±ÅŸmasÄ±** (bu bÃ¶lÃ¼m yazÄ±lÄ±rken M6 devam etmektedir) Kaggle'da dÃ¼zenlenmiÅŸtir ve perakende Ã¼rÃ¼nlerinin bir dizi hacim tahminini Ã§Ã¶zmeye Ã§alÄ±ÅŸÄ±rken **gradyan artÄ±rma (gradient-boosting) yÃ¶ntemlerinin kullanÄ±ÅŸlÄ±lÄ±ÄŸÄ±nÄ±** vurgulamada Ã¶zellikle Ã¶nemli olduÄŸunu kanÄ±tlamÄ±ÅŸtÄ±r. Bu bÃ¶lÃ¼mde, doÄŸruluk parkuruna (accuracy track) odaklanarak, Kaggle yarÄ±ÅŸmalarÄ±ndan bir zaman serisi problemiyle ilgileniyoruz ve en Ã¼st sÄ±ralarda yer alan, ancak en basit ve en anlaÅŸÄ±lÄ±r Ã§Ã¶zÃ¼mlerden birini Ã§oÄŸaltarak, okuyucularÄ±mÄ±za Kaggle'da ortaya Ã§Ä±kabilecek gelecekteki herhangi bir tahmin yarÄ±ÅŸmasÄ±nÄ± baÅŸarÄ±yla ele almak iÃ§in kod ve fikirler sunmayÄ± amaÃ§lÄ±yoruz.

> YarÄ±ÅŸma sayfalarÄ± dÄ±ÅŸÄ±nda, yarÄ±ÅŸma ve dinamikleri hakkÄ±nda UluslararasÄ± Tahmin Dergisi'nden aÅŸaÄŸÄ±daki makalelerde Ã§ok sayÄ±da bilgi bulduk:
>
> * Makridakis, Spyros, Evangelos Spiliotis ve Vassilios Assimakopoulos. M5 yarÄ±ÅŸmasÄ±: Arka plan, organizasyon ve uygulama. International Journal of Forecasting (2021).
> * Makridakis, Spyros, Evangelos Spiliotis ve Vassilios Assimakopoulos. M5 doÄŸruluk yarÄ±ÅŸmasÄ±: SonuÃ§lar, bulgular ve Ã§Ä±karÄ±mlar. International Journal of Forecasting (2022).
> * Makridakis, Spyros, vd. M5 Belirsizlik yarÄ±ÅŸmasÄ±: SonuÃ§lar, bulgular ve Ã§Ä±karÄ±mlar. International Journal of Forecasting (2021).

Bu bÃ¶lÃ¼mde ÅŸunlarÄ± Ã¶ÄŸreneceksiniz:

* YarÄ±ÅŸmanÄ±n zaman serisi verileri ve deÄŸerlendirme metriÄŸi
* Belirli tarihler ve zaman ufuklarÄ± (time horizons) iÃ§in tahminlerin hesaplanmasÄ±
* FarklÄ± zaman pencerelerinden gelen tahminlerin birleÅŸtirilmesi (Assembling)

> Bu bÃ¶lÃ¼mdeki tÃ¼m kod dosyalarÄ±nÄ± [https://packt.link/kwbchp2](https://packt.link/kwbchp2) adresinde bulabilirsiniz.

## YarÄ±ÅŸmayÄ± ve veriyi anlama *(Understanding the competition and the data)*

YarÄ±ÅŸma ([https://www.kaggle.com/competitions/m5-forecasting-accuracy](https://www.kaggle.com/competitions/m5-forecasting-accuracy)) Mart'tan Haziran 2020'ye kadar sÃ¼rdÃ¼ ve Kaggle'da 7.000'den fazla katÄ±lÄ±mcÄ± yer aldÄ±. OrganizatÃ¶rler, yarÄ±ÅŸmayÄ± iki ayrÄ± parkur halinde dÃ¼zenledi: biri noktasal tahmin (doÄŸruluk parkuru) ve diÄŸeri farklÄ± gÃ¼ven aralÄ±klarÄ±nda gÃ¼venilir deÄŸerleri tahmin etmek iÃ§in (belirsizlik parkuru). Bu bÃ¶lÃ¼mde odak noktamÄ±z, doÄŸruluk parkuru iÃ§in en iyi gÃ¶nderimlerden birini Ã§oÄŸaltmaya Ã§alÄ±ÅŸmak ve aynÄ± zamanda belirsizlik parkurunun da Ã¶nÃ¼nÃ¼ aÃ§mak olacak (Ã§Ã¼nkÃ¼ o da doÄŸruluk tahminlerine dayanÄ±yor).

Walmart verileri saÄŸladÄ±. Veriler, Ã¼Ã§ ABD eyaletine yayÄ±lmÄ±ÅŸ (zaman serileri birbiriyle bir miktar iliÅŸkilidir) departmanlara, kategorilere ve maÄŸazalara gÃ¶re hiyerarÅŸik olarak dÃ¼zenlenmiÅŸ 42.840 gÃ¼nlÃ¼k satÄ±ÅŸ zaman serisinden oluÅŸuyordu. SatÄ±ÅŸlarla birlikte Walmart, kalemlerin fiyatlarÄ±, bazÄ± takvim bilgileri, ilgili promosyonlar veya satÄ±ÅŸlarÄ± etkileyen diÄŸer olaylarÄ±n varlÄ±ÄŸÄ± gibi eÅŸlik eden bilgileri (tahmin problemlerinde genellikle sÄ±k saÄŸlanmayan dÄ±ÅŸsal deÄŸiÅŸkenler) de saÄŸladÄ±.

Kaggle dÄ±ÅŸÄ±nda, veriler Ã¶nceki M YarÄ±ÅŸmasÄ±ndan veri setleriyle birlikte ÅŸu adreste mevcuttur: [https://forecasters.org/resources/time-series-data/](https://forecasters.org/resources/time-series-data/).

YarÄ±ÅŸmanÄ±n ilginÃ§ bir yÃ¶nÃ¼, hem hÄ±zlÄ± hareket eden hem de yavaÅŸ hareket eden tÃ¼ketim mallarÄ± satÄ±ÅŸlarÄ±yla ilgilenmesiydi; Ã¶zellikle yavaÅŸ hareket eden mallar, aralÄ±klÄ± satÄ±ÅŸlarÄ±n (intermittent sales) birÃ§ok Ã¶rneÄŸini sunuyordu (satÄ±ÅŸlar Ã§oÄŸu zaman sÄ±fÄ±rdÄ±r, ancak nadir durumlar dÄ±ÅŸÄ±nda). AralÄ±klÄ± seriler, birÃ§ok sektÃ¶rde yaygÄ±n olmasÄ±na raÄŸmen, birÃ§ok uygulayÄ±cÄ± iÃ§in tahmin etmede hala zorlu bir durumdur.

YarÄ±ÅŸma takvimi iki bÃ¶lÃ¼m halinde dÃ¼zenlenmiÅŸtir. Ä°lk bÃ¶lÃ¼mde, Mart 2020 baÅŸÄ±ndan 1 Haziran'a kadar, yarÄ±ÅŸmacÄ±lar 1.913. gÃ¼ne kadar olan gÃ¼nler aralÄ±ÄŸÄ±nda modeller eÄŸitebilir ve herkese aÃ§Ä±k test setinde (1.914. gÃ¼nden 1.941. gÃ¼ne kadar) gÃ¶nderimlerini puanlayabilirlerdi. Bu tarihten sonra, 1 Temmuz'daki yarÄ±ÅŸmanÄ±n sonuna kadar, herkese aÃ§Ä±k test seti eÄŸitim setinin bir parÃ§asÄ± olarak kullanÄ±ma sunuldu ve katÄ±lÄ±mcÄ±larÄ±n 1.942. gÃ¼nden 1.969. gÃ¼ne kadar tahmin yapmak iÃ§in modellerini ayarlamalarÄ±na olanak saÄŸlandÄ± (28 gÃ¼nlÃ¼k, yani dÃ¶rt haftalÄ±k bir zaman penceresi). O dÃ¶nemde, gÃ¶nderimler liderlik tablosunda puanlanmadÄ±.

YarÄ±ÅŸmanÄ±n bÃ¶yle bir dÃ¼zenlemesinin ardÄ±ndaki mantÄ±k, ekiplerin baÅŸlangÄ±Ã§ta modellerini liderlik tablosunda test etmelerine ve en iyi performans gÃ¶steren yÃ¶ntemlerini not defterlerinde ve tartÄ±ÅŸmalarda paylaÅŸmalarÄ± iÃ§in zemin oluÅŸturmaktÄ±. Ä°lk aÅŸamadan sonra, organizatÃ¶rler, liderlik tablosunun aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) amaÃ§larÄ± veya modellerin hiperparametre ayarÄ± iÃ§in kullanÄ±lmasÄ±nÄ± engellemek istediler ve gerÃ§ek dÃ¼nyada olacaÄŸÄ± gibi bir tahmin durumunu taklit etmek istediler. Ek olarak, nihai olarak yalnÄ±zca tek bir gÃ¶nderimi seÃ§me gerekliliÄŸi, aynÄ± gerÃ§ekÃ§ilik ihtiyacÄ±nÄ± yansÄ±tÄ±yordu. GerÃ§ek dÃ¼nyada, bir MLOps ÅŸampiyon/meydan okuyucu stratejisi benimseseniz bile (bkz. [https://www.datarobot.com/blog/introducing-mlops-champion-challenger-models/](https://www.datarobot.com/blog/introducing-mlops-champion-challenger-models/)), belli bir noktada, kararlarÄ±nÄ±z iÃ§in hangi modele gÃ¼veneceÄŸinize karar vermeniz gerekir ve seÃ§imin sonucunu ancak ondan sonra alÄ±rsÄ±nÄ±z.

Verilere gelince, verilerin Walmart tarafÄ±ndan saÄŸlandÄ±ÄŸÄ±nÄ± ve ABD pazarÄ±nÄ± temsil ettiÄŸini belirtmiÅŸtik: Kaliforniya, Wisconsin ve Teksas'taki 10 maÄŸazadan kaynaklanmÄ±ÅŸtÄ±r. Spesifik olarak, veriler 3.049 Ã¼rÃ¼nÃ¼n satÄ±ÅŸlarÄ±ndan oluÅŸuyordu; bu Ã¼rÃ¼nler 3 kategoriye (hobiler, yiyecek ve ev eÅŸyalarÄ±) ayrÄ±lmÄ±ÅŸtÄ± ve bu kategoriler de her biri 7 departmana daha fazla bÃ¶lÃ¼nebiliyordu. BÃ¶yle bir hiyerarÅŸik yapÄ± kesinlikle bir meydan okumadÄ±r Ã§Ã¼nkÃ¼ satÄ±ÅŸ dinamiklerini ABD pazarÄ±, eyalet pazarÄ±, tek bir maÄŸaza, Ã¼rÃ¼n kategorisi, kategori departmanÄ± ve son olarak belirli bir Ã¼rÃ¼n dÃ¼zeyinde modelleyebilirsiniz. TÃ¼m bu seviyeler, ikinci parkurda, belirsizlik parkurunda tahmin edilmesi gereken farklÄ± toplamlar (aggregates) olarak da birleÅŸebilir:

| Seviye ID | Seviye AÃ§Ä±klamasÄ± | Toplama Seviyesi | Seri SayÄ±sÄ± |
| :---: | :--- | :--- | :---: |
| 1 | TÃ¼m Ã¼rÃ¼nler, tÃ¼m maÄŸazalar ve eyaletler iÃ§in toplanmÄ±ÅŸ | Toplam | 1 |
| 2 | TÃ¼m Ã¼rÃ¼nler, her eyalet iÃ§in toplanmÄ±ÅŸ | Eyalet | 3 |
| 3 | TÃ¼m Ã¼rÃ¼nler, her maÄŸaza iÃ§in toplanmÄ±ÅŸ | MaÄŸaza | 10 |
| 4 | TÃ¼m Ã¼rÃ¼nler, her kategori iÃ§in toplanmÄ±ÅŸ | Kategori | 3 |
| 5 | TÃ¼m Ã¼rÃ¼nler, her departman iÃ§in toplanmÄ±ÅŸ | Departman | 7 |
| 6 | TÃ¼m Ã¼rÃ¼nler, her eyalet ve kategori iÃ§in toplanmÄ±ÅŸ | Eyalet-Kategori | 9 |
| 7 | TÃ¼m Ã¼rÃ¼nler, her maÄŸaza ve kategori iÃ§in toplanmÄ±ÅŸ | MaÄŸaza-Kategori | 30 |
| 8 | TÃ¼m Ã¼rÃ¼nler, her eyalet ve departman iÃ§in toplanmÄ±ÅŸ | Eyalet-Departman | 21 |
| 9 | TÃ¼m Ã¼rÃ¼nler, her maÄŸaza ve departman iÃ§in toplanmÄ±ÅŸ | MaÄŸaza-Departman | 70 |
| 10 | Her Ã¼rÃ¼n, tÃ¼m maÄŸazalar/eyaletler iÃ§in toplanmÄ±ÅŸ | ÃœrÃ¼n | 3,049 |
| 11 | Her Ã¼rÃ¼n, her eyalet iÃ§in toplanmÄ±ÅŸ | ÃœrÃ¼n-Eyalet | 9,147 |
| 12 | Her Ã¼rÃ¼n, her maÄŸaza iÃ§in toplanmÄ±ÅŸ | ÃœrÃ¼n-MaÄŸaza | 30,490 |
| **Toplam** | | | **42,840** |

Zaman aÃ§Ä±sÄ±ndan bakÄ±ldÄ±ÄŸÄ±nda, ayrÄ±ntÄ± dÃ¼zeyi gÃ¼nlÃ¼k satÄ±ÅŸ kaydÄ±dÄ±r ve **29 Ocak 2011'den 19 Haziran 2016'ya kadar** olan dÃ¶nemi kapsar, bu da toplamda 1.969 gÃ¼ne eÅŸittir: 1.913 gÃ¼n eÄŸitim iÃ§in, 28 gÃ¼n doÄŸrulama iÃ§in (herkese aÃ§Ä±k liderlik tablosu) ve 28 gÃ¼n test iÃ§in (Ã¶zel liderlik tablosu). **28 gÃ¼nlÃ¼k bir tahmin ufku**, perakende sektÃ¶rÃ¼nde Ã§oÄŸu mal iÃ§in stoklarÄ± ve yeniden sipariÅŸ iÅŸlemlerini yÃ¶netmek iÃ§in uygun ufuk olarak kabul edilmektedir.

YarÄ±ÅŸma iÃ§in aldÄ±ÄŸÄ±nÄ±z farklÄ± verileri inceleyelim. `sales_train_evaluation.csv`, `sell_prices.csv` ve `calendar.csv` dosyalarÄ±nÄ± alÄ±rsÄ±nÄ±z. Zaman serilerini tutan dosya `sales_train_evaluation.csv`'dir. TanÄ±mlayÄ±cÄ± gÃ¶revi gÃ¶ren alanlardan (`item_id`, `dept_id`, `cat_id`, `store_id` ve `state_id`) ve bu gÃ¼nlerin satÄ±ÅŸlarÄ±nÄ± temsil eden `d_1`'den `d_1941`'e kadar olan sÃ¼tunlardan oluÅŸur:

![](im/1001.png)

`sell_prices.csv` dosyasÄ± ise **kalemlerin fiyatlarÄ±** hakkÄ±ndaki bilgileri iÃ§erir. Buradaki zorluk, `wm_yr_wk`'yi (haftanÄ±n kimliÄŸi) eÄŸitim verilerindeki sÃ¼tunlarla **birleÅŸtirmektir (join)**:

![](im/1002.png)

Son dosya olan **`calendar.csv`**, satÄ±ÅŸlarÄ± etkilemiÅŸ olabilecek olaylarla ilgili verileri iÃ§erir:

![](im/1003.png)

Yine, temel zorluk verileri eÄŸitim tablosundaki sÃ¼tunlarla birleÅŸtirmek gibi gÃ¶rÃ¼nÃ¼yor. Ancak, burada sÃ¼tunlarÄ± (`d` alanÄ±) `wm_yr_wk` ile baÄŸlamak iÃ§in kolay bir anahtar bulabilirsiniz. Ek olarak, tabloda belirli gÃ¼nlerde meydana gelebilecek farklÄ± olaylarÄ±n yanÄ± sÄ±ra, dÃ¼ÅŸÃ¼k gelirli ailelere yardÄ±mcÄ± olmak iÃ§in beslenme yardÄ±mÄ± avantajlarÄ±nÄ±n kullanÄ±labileceÄŸi Ã¶zel gÃ¼nler olan **Ek Beslenme YardÄ±mÄ± ProgramÄ± (SNAP) gÃ¼nleri** de temsil edilmiÅŸtir.

## DeÄŸerlendirme MetriÄŸini Anlama *(Understanding the Evaluation Metric)*

DoÄŸruluk yarÄ±ÅŸmasÄ± yeni bir deÄŸerlendirme metriÄŸi tanÄ±ttÄ±: **AÄŸÄ±rlÄ±klÄ± Ortalama KarekÃ¶k Ã–lÃ§ekli Hata (Weighted Root Mean Squared Scaled Error - WRMSSE)**. Ä°lk olarak, incelenen bireysel zaman serilerinin **RMSSE'si (Root Mean Squared Scaled Error)** ile baÅŸlarsÄ±nÄ±z. Bu metrik, nokta tahminlerinin, tahmin edilen serinin gerÃ§ekleÅŸen deÄŸerlerinin ortalamasÄ± etrafÄ±ndaki sapmasÄ±nÄ± deÄŸerlendirir:

$$RMSSE = \sqrt{\frac{1}{h}\frac{\sum_{t=n+1}^{n+h} (Y_t - \hat{Y}_t)^2}{\frac{1}{n-1} \sum_{t=2}^{n} (Y_t - Y_{t-1})^2}}$$

Burada:

* $n$ eÄŸitim Ã¶rneÄŸinin uzunluÄŸudur
* $h$ tahmin ufkudur (bizim durumumuzda, $h = 28$)
* $Y_t$ $t$ anÄ±ndaki satÄ±ÅŸ deÄŸeridir; $\hat{Y}_{t}$ $t$ anÄ±ndaki tahmin edilen deÄŸerdir.

YarÄ±ÅŸmadaki tÃ¼m 42.840 zaman serisi iÃ§in RMSSE tahmin edildikten sonra, AÄŸÄ±rlÄ±klÄ± RMSSE (Weighted RMSSE) ÅŸu ÅŸekilde hesaplanacaktÄ±r:

$$
WRMSSE = \sum_{i=1}^{42,840} w_i * RMSSE
$$

Burada $w_i$, yarÄ±ÅŸmadaki $i$. serinin aÄŸÄ±rlÄ±ÄŸÄ±dÄ±r.

YarÄ±ÅŸma kÄ±lavuzlarÄ±nda ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/)), RMSSE ve WRMSSE ile ilgili olarak ÅŸunlar belirtilmiÅŸtir:

  * RMSSE'nin paydasÄ±, yalnÄ±zca incelenen Ã¼rÃ¼n(ler)in aktif olarak satÄ±ldÄ±ÄŸÄ± zaman dilimleri iÃ§in, yani deÄŸerlendirilen seri iÃ§in gÃ¶zlemlenen ilk **sÄ±fÄ±r olmayan talebi** takip eden dÃ¶nemler iÃ§in hesaplanÄ±r.
  * Ã–lÃ§Ã¼ **Ã¶lÃ§ekten baÄŸÄ±msÄ±zdÄ±r**, yani farklÄ± Ã¶lÃ§eklere sahip seriler arasÄ±ndaki tahminleri karÅŸÄ±laÅŸtÄ±rmak iÃ§in etkili bir ÅŸekilde kullanÄ±labilir, bÃ¶ylece modelin etkinliÄŸini farklÄ± satÄ±ÅŸ miktarlarÄ±na sahip Ã¼rÃ¼nler arasÄ±nda karÅŸÄ±laÅŸtÄ±rabilirsiniz.
  * DiÄŸer Ã¶lÃ§Ã¼lerin aksine, sÄ±fÄ±ra eÅŸit veya sÄ±fÄ±ra yakÄ±n olabilecek deÄŸerlere bÃ¶lme iÅŸlemlerine dayanmadÄ±ÄŸÄ± iÃ§in gÃ¼venle hesaplanabilir (Ã¶rneÄŸin, $Y_t = 0$ olduÄŸunda yÃ¼zde hatalarÄ±nda veya Ã¶lÃ§eklendirme iÃ§in kullanÄ±lan karÅŸÄ±laÅŸtÄ±rma Ã¶lÃ§Ã¼tÃ¼nÃ¼n hatasÄ± sÄ±fÄ±r olduÄŸunda gÃ¶receli hatalarda olduÄŸu gibi).
  * Ã–lÃ§Ã¼, pozitif ve negatif tahmin hatalarÄ±nÄ±, ayrÄ±ca bÃ¼yÃ¼k ve kÃ¼Ã§Ã¼k tahminleri **eÅŸit olarak cezalandÄ±rÄ±r**, bu nedenle simetriktir.
  * Her serinin aÄŸÄ±rlÄ±ÄŸÄ±, veri setinin eÄŸitim Ã¶rneÄŸinin **son 28 gÃ¶zlemine** gÃ¶re, yani her serinin o belirli dÃ¶nemde sergilediÄŸi **kÃ¼mÃ¼latif fiili dolar satÄ±ÅŸlarÄ±na** gÃ¶re hesaplanacaktÄ±r (satÄ±lan birimlerin ilgili fiyatlarÄ±yla Ã§arpÄ±mÄ±nÄ±n toplamÄ±).
  * **Daha dÃ¼ÅŸÃ¼k bir WRMSSE daha iyidir.**

Bunun altÄ±nda yatan iÅŸleyiÅŸe dair iyi bir aÃ§Ä±klama Alexander Soare'un ÅŸu gÃ¶nderisinde ([https://www.kaggle.com/alexandersoare](https://www.kaggle.com/alexandersoare)) sunulmuÅŸtur: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/137019](https://www.google.com/search?q=https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/137019). DeÄŸerlendirme metriÄŸini dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼kten sonra, Alexandre daha iyi performanslarÄ±, **tahminlerdeki hata ile satÄ±ÅŸ deÄŸerlerinin gÃ¼nden gÃ¼ne deÄŸiÅŸimi arasÄ±ndaki oranÄ±n iyileÅŸtirilmesine** baÄŸlamaktadÄ±r.

Hata, gÃ¼nlÃ¼k varyasyonlarla aynÄ±ysa (**oran=1**), modelin geÃ§miÅŸteki varyasyonlara dayalÄ± rastgele bir tahminden Ã§ok daha iyi olmadÄ±ÄŸÄ± muhtemeldir. OranÄ±nÄ±z rastgele bir tahminden daha iyiyse, **kuadratik bir ÅŸekilde** WRMSSE'ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (formÃ¼ldeki karekÃ¶k nedeniyle). SonuÃ§ olarak, 0.7'lik bir oran, 0.5'lik bir WRMSSE'ye karÅŸÄ±lÄ±k gelir ve 0.5'lik bir oran, 0.25'lik bir WRMSSE'ye karÅŸÄ±lÄ±k gelir.

YarÄ±ÅŸma sÄ±rasÄ±nda, Kaggle katÄ±lÄ±mcÄ±larÄ± modellerini doÄŸrudan liderlik tablosunda deÄŸerlendirdiler, ancak metriÄŸi doÄŸrudan bir amaÃ§ fonksiyonu olarak kullanmak iÃ§in de birÃ§ok giriÅŸimde bulunuldu. Ã–ncelikle, **Tweedie kaybÄ±** (hem XGBoost hem de LightGBM'de uygulanan), Ã§oÄŸu Ã¼rÃ¼n iÃ§in Ã§arpÄ±k satÄ±ÅŸ daÄŸÄ±lÄ±mlarÄ±nÄ± (birÃ§oÄŸu aralÄ±klÄ± satÄ±ÅŸlara da sahipti ve bu da Tweedie kaybÄ± tarafÄ±ndan gÃ¼zelce ele alÄ±nÄ±r) ele alabildiÄŸi iÃ§in problem iÃ§in oldukÃ§a iyi Ã§alÄ±ÅŸtÄ±. Ä°ÅŸte Tweedie kaybÄ± formÃ¼lÃ¼:

$$
\text{Loss} = - \sum_{i} \left( x_i * \frac{\tilde{x}_i^{1-p}}{1-p} + \frac{\tilde{x}_i^{2-p}}{2-p} \right)
$$

FormÃ¼lde, $x_i$ gerÃ§ek hedefi; $\tilde{x}_i$ ise tahmin edilen deÄŸeri temsil eder. FormÃ¼l ve uygulanmasÄ± hakkÄ±nda daha fazla bilgiyi ÅŸu makalede bulabilirsiniz: [https://towardsdatascience.com/tweedie-loss-function-for-right-skewed-data-2c5ca470678f](https://www.google.com/search?q=https://towardsdatascience.com/tweedie-loss-function-for-right-skewed-data-2c5ca470678f).

Poisson ve Gamma daÄŸÄ±lÄ±mlarÄ±, Tweedie daÄŸÄ±lÄ±mÄ±nÄ±n uÃ§ durumlarÄ± olarak kabul edilebilir: gÃ¼Ã§ parametresi $p$'ye baÄŸlÄ± olarak, $p = 1$ olduÄŸunda bir **Poisson daÄŸÄ±lÄ±mÄ±** ve $p = 2$ olduÄŸunda bir **Gamma daÄŸÄ±lÄ±mÄ±** elde edersiniz. Bu gÃ¼Ã§ parametresi, daÄŸÄ±lÄ±mÄ±n ortalamasÄ±nÄ± ve varyansÄ±nÄ± ÅŸu formÃ¼lle birbirine baÄŸlayan yapÄ±ÅŸtÄ±rÄ±cÄ±dÄ±r:

$$
Var(x) = \Phi \mu^p
$$

Burada $\Phi$ daÄŸÄ±lÄ±m parametresi ve $\mu$ ortalamadÄ±r.

1 ile 2 arasÄ±nda bir gÃ¼Ã§ deÄŸeri ($p$) kullanarak, aslÄ±nda **Poisson ve Gamma daÄŸÄ±lÄ±mlarÄ±nÄ±n bir karÄ±ÅŸÄ±mÄ±nÄ±** elde edersiniz, ki bu da yarÄ±ÅŸma problemine Ã§ok iyi uyum saÄŸlayabilir. GBM Ã§Ã¶zÃ¼mÃ¼ kullanan yarÄ±ÅŸmaya katÄ±lan Kaggle katÄ±lÄ±mcÄ±larÄ±nÄ±n Ã§oÄŸu aslÄ±nda **Tweedie kaybÄ±na** baÅŸvurdu.

Tweedie'nin baÅŸarÄ±sÄ±na raÄŸmen, bazÄ± diÄŸer Kaggle katÄ±lÄ±mcÄ±larÄ±, modelleri iÃ§in WRMSSE'ye daha Ã§ok benzeyen bir amaÃ§ kaybÄ± (objective loss) uygulamak iÃ§in ilginÃ§ yollar buldular:

* Martin Kovacevic Buvinic, **asimetrik kaybÄ±** ile: [https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook](https://www.google.com/search?q=https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook)
* Timetraveller, LightGBM'de uygulanacak herhangi bir tÃ¼revlenebilir sÃ¼rekli kayÄ±p fonksiyonu iÃ§in gradyan ve Hessian elde etmek Ã¼zere **PyTorch Autograd** kullanarak: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837](https://www.google.com/search?q=https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837)

## Monsaraida'nÄ±n 4. sÄ±radaki Ã§Ã¶zÃ¼m fikirlerini inceleme *(Examining the 4th place solutionâ€™s ideas from Monsaraida)*

Elbette, metninizi TÃ¼rkÃ§eye Ã§evirdim:

-----

YarÄ±ÅŸma iÃ§in mevcut birÃ§ok Ã§Ã¶zÃ¼m bulunmaktadÄ±r ve bunlarÄ±n Ã§oÄŸu yarÄ±ÅŸmanÄ±n Kaggle tartÄ±ÅŸma sayfalarÄ±nda bulunabilir. Her iki zorluÄŸun (accuracy ve uncertainty) ilk beÅŸ yÃ¶ntemi de yarÄ±ÅŸma organizatÃ¶rleri tarafÄ±ndan (biri Ã¶zel mÃ¼lkiyet haklarÄ± nedeniyle hariÃ§) toplanmÄ±ÅŸ ve yayÄ±nlanmÄ±ÅŸtÄ±r: [https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods) (bu arada, kazanan gÃ¶nderimlerin sonuÃ§larÄ±nÄ± yeniden Ã¼retmek, bir yarÄ±ÅŸma Ã¶dÃ¼lÃ¼nÃ¼n toplanmasÄ± iÃ§in bir Ã¶n koÅŸuldu).

Dikkat Ã§ekici bir ÅŸekilde, yarÄ±ÅŸmalarÄ±n Ã¼st sÄ±ralarÄ±nda yer alan tÃ¼m Kaggle katÄ±lÄ±mcÄ±larÄ±, iÅŸlenecek ve tahmin edilecek Ã§ok sayÄ±daki zaman serisi nedeniyle yarÄ±ÅŸmada avantaj saÄŸlayan daha az bellek kullanÄ±mÄ± ve hesaplama hÄ±zÄ± nedeniyle, tek model tÃ¼rleri olarak veya harmanlanmÄ±ÅŸ/yÄ±ÄŸÄ±nlanmÄ±ÅŸ (blended/stacked) topluluklarda **LightGBM'i** kullanmÄ±ÅŸlardÄ±r. Ancak baÅŸarÄ±sÄ±nÄ±n baÅŸka nedenleri de vardÄ±r. ARIMA'ya dayalÄ± klasik yÃ¶ntemlerin aksine, otokorelasyon analizine gÃ¼venmeyi ve problemdeki her bir seri iÃ§in spesifik olarak parametreleri bulmayÄ± gerektirmez. Ek olarak, derin Ã¶ÄŸrenmeye dayalÄ± yÃ¶ntemlerden farklÄ± olarak, karmaÅŸÄ±k sinir mimarilerini iyileÅŸtirmeye veya Ã§ok sayÄ±da hiperparametreyi ayarlamaya Ã§alÄ±ÅŸmayÄ± gerektirmez. Zaman serisi problemlerinde gradyan artÄ±rma yÃ¶ntemlerinin (yalnÄ±zca LightGBM deÄŸil, Ã¶rneÄŸin XGBoost da) gÃ¼cÃ¼, **Ã¶zellik mÃ¼hendisliÄŸine** (zaman gecikmeleri, hareketli ortalamalar ve serinin niteliklerinin gruplandÄ±rÄ±lmasÄ±ndan elde edilen ortalamalara dayanarak), doÄŸru amaÃ§ fonksiyonunu seÃ§meye ve hiperparametre ayarlamaya dayanÄ±r. Bu yÃ¶ntemler, yeterince uzun zaman serileri sÃ¶z konusu olduÄŸunda klasik yÃ¶ntemlerden daha etkilidir. Daha kÄ±sa seriler iÃ§in, daha karmaÅŸÄ±k yÃ¶ntemler aÅŸÄ±rÄ± Ã¶ÄŸrenmeye (overfitting) eÄŸilimli olduÄŸu iÃ§in, otoregresif (AR), hareketli ortalamalar (MA) ve ARMA/ARIMA gibi klasik standart ve doÄŸrusal istatistiksel yÃ¶ntemler hala Ã¶nerilen tercihtir.

LightGBM ve XGBoost'un yarÄ±ÅŸmadaki derin Ã¶ÄŸrenme Ã§Ã¶zÃ¼mlerine gÃ¶re bir diÄŸer avantajÄ± da **Tweedie kaybÄ±nÄ±n hazÄ±r bulunmasÄ±ydÄ±**. Ek olarak, DNN'lere gÃ¶re diÄŸer avantajlar arasÄ±nda, herhangi bir Ã¶zellik Ã¶lÃ§eklendirmesi gerektirmemesi (derin Ã¶ÄŸrenme aÄŸlarÄ± kullandÄ±ÄŸÄ±nÄ±z Ã¶lÃ§eklendirmeye Ã¶zellikle duyarlÄ±dÄ±r) ve Ã¶zellik mÃ¼hendisliÄŸini test ederken daha hÄ±zlÄ± yinelemelere izin veren **eÄŸitim hÄ±zlarÄ±** yer almaktadÄ±r.

> Mevcut tÃ¼m bu Ã§Ã¶zÃ¼mler arasÄ±nda, Japon bilgisayar bilimcisi **Monsaraida (Masanori Miyahara)** tarafÄ±ndan Ã¶nerilen Ã§Ã¶zÃ¼mÃ¼ en ilginÃ§ bulduk. Kendisi, Ã¶zel liderlik tablosunda **0.53583'lÃ¼k bir skorla dÃ¶rdÃ¼ncÃ¼ sÄ±rada** yer alan basit ve anlaÅŸÄ±lÄ±r bir Ã§Ã¶zÃ¼m Ã¶nerdi. Ã‡Ã¶zÃ¼m, Ã¶nceden seÃ§im yapÄ±lmaksÄ±zÄ±n sadece genel Ã¶zellikleri (satÄ±ÅŸ istatistikleri, takvimler, fiyatlar ve tanÄ±mlayÄ±cÄ±lar gibi) kullanmaktadÄ±r.

AyrÄ±ca, aynÄ± tÃ¼rden sÄ±nÄ±rlÄ± sayÄ±da model kullanarak, **LightGBM gradyan artÄ±rmayÄ±** kullanmaktadÄ±r; herhangi bir harmanlama, tahminlerin hiyerarÅŸik olarak iliÅŸkili diÄŸer tahminleri beslediÄŸi Ã¶zyinelemeli modelleme veya test setine daha iyi uymasÄ± iÃ§in sabitler seÃ§en Ã§arpanlar gibi yÃ¶ntemlere baÅŸvurmamÄ±ÅŸtÄ±r.

Ä°ÅŸte M Tahmin YarÄ±ÅŸmalarÄ±na sunduÄŸu Ã§Ã¶zÃ¼m sunumundan ([https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4](https://www.google.com/search?q=https://github.com/Mcompetitions/M5-methods/tree/master/Code%2520of%2520Winning%2520Methods/A4)) alÄ±nmÄ±ÅŸ bir ÅŸema; burada, **ileriye bakÄ±lacak dÃ¶rt haftanÄ±n her biri iÃ§in on maÄŸazanÄ±n her birini ele aldÄ±ÄŸÄ±nÄ±** not edebiliriz, bu da sonuÃ§ta **40 model** Ã¼retmeye karÅŸÄ±lÄ±k gelir:

![](im/1004.png)

> ğŸ‘¨â€ğŸ’» Masanori Miyahara (Monsaraida)
> 
> 
> 
> **[https://www.kaggle.com/monsaraida](https://www.kaggle.com/monsaraida)**
> 
> 
> 
> Ä°ncelikle hazÄ±rlanmÄ±ÅŸ Ã§Ã¶zÃ¼mÃ¼ ve Kaggle geÃ§miÅŸi hakkÄ±nda meraklanarak Masanori Miyahara (Monsaraida) ile iletiÅŸime geÃ§tik. Kendisi bize nazikÃ§e yanÄ±t verdi ve bilgisayar bilimleri diplomasÄ±na sahip olduÄŸunu, ÅŸimdiye kadar bir Japon ÅŸirketi iÃ§in yazÄ±lÄ±m geliÅŸtirme ve veri analizi projelerinde proje lideri olarak Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlattÄ±. BaÅŸlangÄ±Ã§ta, iÅŸinde sÄ±kÃ§a kullanmadÄ±ÄŸÄ± Ã§eÅŸitli veri tekniklerini denemesine olanak tanÄ±yacaÄŸÄ± iÃ§in Kaggle ile ilgilenmiÅŸti.
> 
> 
> 
> **En sevdiÄŸiniz yarÄ±ÅŸma tÃ¼rÃ¼ nedir ve neden? Teknikler ve Ã§Ã¶zÃ¼m yaklaÅŸÄ±mlarÄ± aÃ§Ä±sÄ±ndan Kaggle'daki uzmanlÄ±k alanÄ±nÄ±z nedir?**
> 
> 
> 
> BirkaÃ§ gÃ¶rÃ¼ntÃ¼ verisi ve doÄŸal dil iÅŸleme yarÄ±ÅŸmasÄ±na katÄ±ldÄ±m, ancak en sÄ±k tablo verileri yarÄ±ÅŸmalarÄ±na katÄ±lÄ±yorum. Bence tablo verileri yarÄ±ÅŸmalarÄ±, bilgi iÅŸlem kaynaklarÄ±na sahip olmasanÄ±z bile katÄ±lmanÄ±n kolay olmasÄ±, bir deneyi hÄ±zlÄ±ca bitirip bol bol deneme yanÄ±lma yapabilmeniz nedeniyle, kÄ±sa bir sÃ¼re yoÄŸunlaÅŸmak iÃ§in uygun. (Genellikle Ã§ocuk bakÄ±mÄ± ve iÅŸ nedeniyle meÅŸgulÃ¼m, bu yÃ¼zden tatillerde veya hafta sonlarÄ±nda yarÄ±ÅŸmalarla toplu olarak ilgilenme eÄŸilimindeyim.) AyrÄ±ca, verileri dikkatlice keÅŸfederek ve alan bilginize dayalÄ± orijinal fikirler deneyerek sÄ±ralamanÄ±zÄ± kademeli olarak yÃ¼kseltebileceÄŸiniz iÃ§in, ileri dÃ¼zey makine Ã¶ÄŸrenimi bilginiz olmasa bile tablo verileri yarÄ±ÅŸmalarÄ± eÄŸlenceli olabilir.
> 
> 
> 
> **Bir Kaggle yarÄ±ÅŸmasÄ±na nasÄ±l yaklaÅŸÄ±rsÄ±nÄ±z? EÄŸer veri biliminde Ã§alÄ±ÅŸÄ±yorsanÄ±z, bu yaklaÅŸÄ±m gÃ¼nlÃ¼k iÅŸinizde yaptÄ±klarÄ±nÄ±zdan ne kadar farklÄ±dÄ±r?**
> 
> 
> 
> Benim iÃ§in bir Kaggle yarÄ±ÅŸmasÄ±na katÄ±lmak, bir video oyunu alÄ±p oynamaya veya bir geziye Ã§Ä±kmaya benzer; bu tamamen bir hobidir. Bir veri seti veya problemle ilgilenirsem, tatilde veya hafta sonlarÄ±nda yarÄ±ÅŸma Ã¼zerinde Ã§alÄ±ÅŸmak iÃ§in kendime yalnÄ±z zaman yaratÄ±rÄ±m. Genellikle yarÄ±ÅŸmalara tek baÅŸÄ±ma katÄ±lÄ±rÄ±m. Bunun nedeni, Ã§ocuk bakÄ±mÄ± veya iÅŸ nedeniyle yarÄ±ÅŸmalara zaman ayÄ±ramadÄ±ÄŸÄ±m iÃ§in ekip Ã¼yelerimi zor durumda bÄ±rakmak istemememdir. Ã–te yandan, iÅŸ hayatÄ±nda, mÃ¼ÅŸterilerin sorunlarÄ±nÄ± anlamak ve Ã§Ã¶zmek iÃ§in sistematik olarak bir ekip olarak Ã§alÄ±ÅŸÄ±rÄ±z. En Ã¶nemli ÅŸey mÃ¼ÅŸteriye deÄŸer saÄŸlamaktÄ±r ve bir tahmin modelinin doÄŸruluÄŸunu aÅŸÄ±rÄ±ya taÅŸÄ±yacak az fÄ±rsat vardÄ±r. Ä°ÅŸ hayatÄ±nda tekrarlanabilirlik, istikrar, sÃ¼rdÃ¼rÃ¼lebilirlik ve maliyet performansÄ± da gereklidir. Kaggle'da ise, doÄŸruluÄŸu sadece %0.1 bile olsa artÄ±rmak iÃ§in her ÅŸey yapÄ±lmalÄ±dÄ±r ve doÄŸruluk dÄ±ÅŸÄ±ndaki faktÃ¶rler genellikle daha az kritiktir.
> 
> 
> 
> **Kaggle kariyerinize yardÄ±mcÄ± oldu mu? EÄŸer Ã¶yleyse, nasÄ±l?**
> 
> 
> 
> Kaggle benim iÃ§in bir hobi, bu yÃ¼zden kariyerime yardÄ±mcÄ± olacaÄŸÄ±nÄ± dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼m iÃ§in Ã¼zerinde Ã§alÄ±ÅŸmÄ±yorum. Ancak, sonuÃ§ olarak kariyerim iÃ§in faydalÄ± oluyor. Son zamanlarda Kaggle, Japonya'da Ã§ok popÃ¼ler hale geldi ve birÃ§ok kiÅŸi ilgileniyor. Ä°nsanlara Kaggle'da yarÄ±ÅŸmalarla Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ± sÃ¶ylediÄŸimde bana gÃ¼veniyorlar ve sÄ±k sÄ±k veri analizi konusunda benden tavsiye istiyorlar. AyrÄ±ca, aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nleyen ve gerÃ§ek dÃ¼nya ortamÄ±nda doÄŸruluÄŸu garanti eden doÄŸrulama stratejileri hakkÄ±nda Kaggle'dan Ã§ok ÅŸey Ã¶ÄŸrendim ve bu, iÅŸimde Ã§ok faydalÄ± oldu. Ä°ÅŸ hayatÄ±nda yanlÄ±ÅŸ bir doÄŸrulama stratejisi Ã§ok zarar verici olabilir, ancak Kaggle'da sadece bir madalyayÄ± kaÃ§Ä±rabilirim. DahasÄ±, Kaggle deneyimi, verileri kÄ±sa bir kontrol ettikten sonra bile doÄŸruluÄŸu artÄ±rmak iÃ§in ne kadar zaman harcamam gerektiÄŸini tahmin etmemi saÄŸladÄ±.
> 
> 
> 
> **Deneyimlerinize gÃ¶re, deneyimsiz Kaggle katÄ±lÄ±mcÄ±larÄ± genellikle neyi gÃ¶zden kaÃ§Ä±rÄ±r? Åimdi bildiÄŸiniz ama ilk baÅŸladÄ±ÄŸÄ±nÄ±zda bilmek istediÄŸiniz ÅŸey nedir?**
> 
> 
> 
> Bu benim kiÅŸisel gÃ¶rÃ¼ÅŸÃ¼m, ama bir ÅŸey Ã¶ÄŸrenirken, sevdiÄŸiniz, zamanÄ±n nasÄ±l geÃ§tiÄŸini unutup tutkuyla baÄŸlanabileceÄŸiniz bir ÅŸey Ã¼zerinde Ã§alÄ±ÅŸmanÄ±n Ã§ok daha verimli olduÄŸuna inanÄ±yorum. Bazen bana ÅŸu soru sorulur: "Kaggle yarÄ±ÅŸmasÄ±na baÅŸlamadan Ã¶nce yeterli bilgi edinmek iÃ§in Python mÄ± Ã§alÄ±ÅŸmalÄ±yÄ±m yoksa bir makine Ã¶ÄŸrenimi ders kitabÄ± mÄ± okumalÄ±yÄ±m?" Bence mevcut yarÄ±ÅŸmaya yine de katÄ±lmalÄ±sÄ±nÄ±z. Bir yarÄ±ÅŸmaya katÄ±ldÄ±ÄŸÄ±nÄ±zda ve tahminlerinizi gÃ¶nderdiÄŸinizde, nerede sÄ±ralandÄ±ÄŸÄ±nÄ±zÄ± bilecek ve sÄ±ralamanÄ±zÄ± yÃ¼kseltmek isteyeceksiniz. SÄ±ralamanÄ±zÄ± yÃ¼kseltmek iÃ§in kodlama becerilerine ve makine Ã¶ÄŸrenimi ve veri analizi bilgisine ihtiyacÄ±nÄ±z olacak ve bunun iÃ§in gerekli bilgileri Ã¶ÄŸrenmeye baÅŸlamalÄ±sÄ±nÄ±z. Net bir hedefiniz olduÄŸu iÃ§in verimli bir ÅŸekilde Ã¶ÄŸrenebilirsiniz ve Ã¶ÄŸrendiklerinizin sonuÃ§larÄ± hemen sÄ±ralamanÄ±za yansÄ±yacak, bu da sizi motive edecektir.
> 
> 
> 
> **Veri analizi/makine Ã¶ÄŸrenimi iÃ§in kullanmayÄ± Ã¶nereceÄŸiniz belirli araÃ§lar veya kÃ¼tÃ¼phaneler var mÄ±?**
> 
> 
> 
> Elbette, pandas/scikit-learn/LightGBM/XGBoost gibi veri analizi ve makine Ã¶ÄŸrenimi kÃ¼tÃ¼phaneleri Ã¶nerilir, ancak MLflow gibi deney yÃ¶netimi araÃ§larÄ±nÄ±n da verimli deneyler iÃ§in gerekli olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum. Bir deney yÃ¶netimi aracÄ± olmadan tekrar tekrar deneme-yanÄ±lma deneyleri yaparsanÄ±z, deneysel koÅŸullarÄ±n ve ayarlarÄ±n izini kaybedersiniz. Ã–zellikle yarÄ±ÅŸmanÄ±n ikinci yarÄ±sÄ±nda, bir deney yÃ¶netimi aracÄ±yla tekrarlanabilirliÄŸi saÄŸlayarak deneyleri verimli bir ÅŸekilde yÃ¼rÃ¼tebilirsiniz.
> 
> 
> 
> **Bir yarÄ±ÅŸmaya girerken akÄ±lda tutulmasÄ±/yapÄ±lmasÄ± gereken en Ã¶nemli ÅŸey nedir?**
> 
> 
> 
> Bence en Ã¶nemli ÅŸey **Kaggle'dan keyif almaktÄ±r**. Kaggle'dan keyif almanÄ±n birÃ§ok yolu vardÄ±r: yarÄ±ÅŸmadan keyif almak, yeni verilerden ve alan bilgisinden keyif almak, en son tekniklerden ve araÃ§lardan keyif almak, baÅŸkalarÄ±yla yapÄ±lan tartÄ±ÅŸmalardan keyif almak vb. SonuÃ§ ne olursa olsun, eÄŸlenerek harcanan zaman ve edinilen bilgi Ã§ok deÄŸerlidir.

Monsaraida'nÄ±n Ã§Ã¶zÃ¼mÃ¼nÃ¼, gerÃ§ek dÃ¼nya tahmin projesinde olduÄŸu gibi, basit ve pratik tuttuÄŸu gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, bu bÃ¶lÃ¼mde onun Ã¶rneÄŸini, Kaggle not defterlerinde Ã§alÄ±ÅŸacak ÅŸekilde kodunu yeniden dÃ¼zenleyerek Ã§oÄŸaltmaya Ã§alÄ±ÅŸacaÄŸÄ±z (kodu birden fazla not defterine bÃ¶lerek bellek ve Ã§alÄ±ÅŸma sÃ¼resi sÄ±nÄ±rlamalarÄ±nÄ± aÅŸacaÄŸÄ±z). Bu ÅŸekilde, okuyuculara tahmin problemlerine yaklaÅŸmak iÃ§in **gradyan artÄ±rmaya dayalÄ± basit ve etkili bir yol** sunmayÄ± amaÃ§lÄ±yoruz.

## Belirli tarihler ve zaman ufuklarÄ± (time horizons) iÃ§in tahminleri hesaplama *(Computing predictions for specific dates and time horizons)*

Monsaraida'nÄ±n Ã§Ã¶zÃ¼mÃ¼nÃ¼ Ã§oÄŸaltma planÄ±, eÄŸitim ve test veri setleri iÃ§in gerekli iÅŸlenmiÅŸ verileri ve tahminler iÃ§in LightGBM modellerini Ã¼retmek Ã¼zere girdi parametreleriyle Ã¶zelleÅŸtirilebilir bir not defteri oluÅŸturmaktÄ±r. Modeller, geÃ§miÅŸteki veriler verildiÄŸinde, gelecekteki belirli bir gÃ¼n sayÄ±sÄ±ndaki deÄŸerleri tahmin etmeyi Ã¶ÄŸrenmek Ã¼zere eÄŸitilecektir. En iyi sonuÃ§lar, her modelin gelecekteki belirli bir hafta aralÄ±ÄŸÄ±ndaki deÄŸerleri tahmin etmeyi Ã¶ÄŸrenmesiyle elde edilebilir. 28 gÃ¼n sonrasÄ±na kadar tahmin etmemiz gerektiÄŸinden, gelecekteki **+1. gÃ¼nden +7. gÃ¼ne** kadar tahmin eden bir modele, ardÄ±ndan **+8. gÃ¼nden +14. gÃ¼ne** kadar tahmin edebilen baÅŸka bir modele, **+15. gÃ¼nden +21. gÃ¼ne** kadar bir baÅŸkasÄ±na ve son olarak **+22. gÃ¼nden +28. gÃ¼ne** kadar olan tahminleri ele alabilecek bir diÄŸerine ihtiyacÄ±mÄ±z var. Bu zaman aralÄ±klarÄ±nÄ±n her biri iÃ§in bir Kaggle not defterine ihtiyacÄ±mÄ±z olacak, yani **dÃ¶rt not defterine** ihtiyacÄ±mÄ±z var. Bu not defterlerinin her biri, yarÄ±ÅŸmanÄ±n parÃ§asÄ± olan **10 maÄŸazanÄ±n her biri iÃ§in** gelecekteki zaman aralÄ±ÄŸÄ±nÄ± tahmin etmek Ã¼zere eÄŸitilecektir. Toplamda, her not defteri on model Ã¼retecektir. Hep birlikte, not defterleri tÃ¼m gelecek aralÄ±klarÄ±nÄ± ve tÃ¼m maÄŸazalarÄ± kapsayan **40 model** Ã¼retecektir.

Hem herkese aÃ§Ä±k liderlik tablosu hem de Ã¶zel liderlik tablosu iÃ§in tahmin yapmamÄ±z gerektiÄŸinden, bu sÃ¼reci iki kez tekrarlamak gereklidir; herkese aÃ§Ä±k test seti gÃ¶nderimi iÃ§in eÄŸitimi **1.913. gÃ¼nde** durdurmak (1.914'ten 1.941'e kadar olan gÃ¼nleri tahmin etmek) ve Ã¶zel gÃ¶nderim iÃ§in **1.941. gÃ¼nde** durdurmak (1.942'den 1.969'a kadar olan gÃ¼nleri tahmin etmek).

CPU tabanlÄ± Kaggle not defterlerini Ã§alÄ±ÅŸtÄ±rmaya yÃ¶nelik mevcut sÄ±nÄ±rlamalar gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, bu sekiz not defterinin tamamÄ± paralel olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir (tÃ¼m sÃ¼reÃ§ yaklaÅŸÄ±k 6 buÃ§uk saat sÃ¼rer). Her not defteri, adÄ±nda son eÄŸitim gÃ¼nÃ¼ne ve ileriye dÃ¶nÃ¼k ufuk gÃ¼n sayÄ±sÄ±na iliÅŸkin parametre deÄŸerlerini iÃ§ererek diÄŸerlerinden ayÄ±rt edilebilir. Bu not defterlerinden birine Ã¶rnek olarak [https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7) adresinden ulaÅŸÄ±labilir.

Åimdi, kodun nasÄ±l dÃ¼zenlendiÄŸini ve Monsaraida'nÄ±n Ã§Ã¶zÃ¼mÃ¼nden neler Ã¶ÄŸrenebileceÄŸimizi birlikte inceleyelim.

Sadece gerekli paketleri iÃ§e aktararak baÅŸlÄ±yoruz. NumPy ve pandas dÄ±ÅŸÄ±nda, tek veri bilimi uzmanlÄ±k paketi olarak LightGBM'i fark edebilirsiniz. AyrÄ±ca `gc` (Ã§Ã¶p toplama) kullanacaÄŸÄ±mÄ±zÄ± da fark edebilirsiniz: bunun nedeni, betik tarafÄ±ndan kullanÄ±lan bellek miktarÄ±nÄ± sÄ±nÄ±rlamamÄ±z gerekmesi ve kullanÄ±lmayan belleÄŸi sÄ±k sÄ±k toplamamÄ±z ve geri dÃ¶nÃ¼ÅŸtÃ¼rmemizdir. Bu stratejinin bir parÃ§asÄ± olarak, modelleri ve veri yapÄ±larÄ±nÄ± bellekte tutmak yerine sÄ±k sÄ±k diske kaydederiz:

```python
import numpy as np
import pandas as pd
import os
import random
import math
from decimal import Decimal as dec
import datetime
import time
import gc
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
```

Bellek kullanÄ±mÄ±nÄ± sÄ±nÄ±rlama stratejisinin bir parÃ§asÄ± olarak, Kaggle kitabÄ±nda aÃ§Ä±klanan ve baÅŸlangÄ±Ã§ta Arjan Groen tarafÄ±ndan Zillow yarÄ±ÅŸmasÄ± sÄ±rasÄ±nda geliÅŸtirilen pandas DataFrame bellek ayak izini azaltma fonksiyonuna baÅŸvuruyoruz (tartÄ±ÅŸmayÄ± okuyun: [https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844](https://www.google.com/search?q=https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844)):

```python
def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df
```

Bu Ã§Ã¶zÃ¼m iÃ§in fonksiyonlarÄ± tanÄ±mlamaya devam ediyoruz, Ã§Ã¼nkÃ¼ Ã§Ã¶zÃ¼mÃ¼ daha kÃ¼Ã§Ã¼k parÃ§alara ayÄ±rmak yardÄ±mcÄ± olur ve bir fonksiyondan dÃ¶ndÃ¼ÄŸÃ¼nÃ¼zde kullanÄ±lan tÃ¼m deÄŸiÅŸkenleri temizlemek daha kolaydÄ±r (yalnÄ±zca diske kaydettiklerinizi tutarsÄ±nÄ±z). Bir sonraki fonksiyonumuz, mevcut tÃ¼m verileri yÃ¼klememize ve sÄ±kÄ±ÅŸtÄ±rmamÄ±za yardÄ±mcÄ± olur:

```python
def load_data():
    train_df = reduce_mem_usage(pd.read_csv("../input/m5-forecasting-accuracy/sales_train_evaluation.csv"))
    prices_df = reduce_mem_usage(pd.read_csv("../input/m5-forecasting-accuracy/sell_prices.csv"))
    calendar_df = reduce_mem_usage(pd.read_csv("../input/m5-forecasting-accuracy/calendar.csv"))
    submission_df = reduce_mem_usage(pd.read_csv("../input/m5-forecasting-accuracy/sample_submission.csv"))
    return train_df, prices_df, calendar_df, submission_df
```

Fonksiyon tanÄ±mlandÄ±ktan sonra, onu Ã§alÄ±ÅŸtÄ±rÄ±yoruz:

```python
train_df, prices_df, calendar_df, submission_df = load_data()
```

Fiyatlar, hacimler ve takvim bilgileriyle ilgili verileri almak iÃ§in kodu hazÄ±rladÄ±ktan sonra, `item_id`, `dept_id`, `cat_id`, `state_id` ve `store_id`'ye satÄ±r anahtarÄ±, bir gÃ¼n sÃ¼tunu ve hacimleri iÃ§eren bir deÄŸerler sÃ¼tunu olarak sahip olacak temel bir bilgi tablosu oluÅŸturma rolÃ¼ne sahip olacak ilk iÅŸleme fonksiyonunu hazÄ±rlamaya geÃ§iyoruz. Bu, tÃ¼m gÃ¼nlerin veri sÃ¼tunlarÄ±na sahip satÄ±rlardan baÅŸlayarak pandas'Ä±n `melt` komutu ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)) kullanÄ±larak gerÃ§ekleÅŸtirilir.

Komut, DataFrame'in indeksini referans olarak alÄ±r ve ardÄ±ndan kalan tÃ¼m Ã¶zellikleri seÃ§er, adlarÄ±nÄ± bir sÃ¼tuna ve deÄŸerlerini baÅŸka bir sÃ¼tuna yerleÅŸtirir (`var_name` ve `value_name` parametreleri bu yeni sÃ¼tunlarÄ±n adÄ±nÄ± tanÄ±mlamanÄ±za yardÄ±mcÄ± olur). Bu ÅŸekilde, belirli bir Ã¼rÃ¼ne ait belirli bir maÄŸazadaki satÄ±ÅŸ serisini temsil eden bir satÄ±rÄ±, her biri tek bir gÃ¼nÃ¼ temsil eden birden Ã§ok satÄ±ra aÃ§abilirsiniz. AÃ§Ä±lmÄ±ÅŸ sÃ¼tunlarÄ±n konum sÄ±rasÄ±nÄ±n korunmasÄ±, zaman serinizin artÄ±k dikey eksende uzanmasÄ±nÄ± garanti eder (bu nedenle Ã¼zerine hareketli ortalamalar gibi daha fazla dÃ¶nÃ¼ÅŸÃ¼m uygulayabilirsiniz).

Ne olduÄŸunu size fikir vermek iÃ§in, `pd.melt` dÃ¶nÃ¼ÅŸÃ¼mÃ¼nden Ã¶nceki `train_df`'yi burada gÃ¶rebilirsiniz. FarklÄ± gÃ¼nlerin hacimlerinin nasÄ±l sÃ¼tun Ã¶zellikleri olduÄŸuna dikkat edin:

![](im/1005.png)

DÃ¶nÃ¼ÅŸÃ¼mden sonra, sÃ¼tunlarÄ±n satÄ±rlara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼ ve gÃ¼nlerin artÄ±k yeni bir sÃ¼tunda bulunduÄŸu bir `grid_df` elde edersiniz:

![](im/1006.png)

Elbette, metninizi TÃ¼rkÃ§eye Ã§evirdim:

-----

`d` Ã¶zelliÄŸi, indekste yer almayan sÃ¼tunlara, esasen `d_1`'den `d_1935`'e kadar olan tÃ¼m Ã¶zelliklere referansÄ± iÃ§erir. Bu, veri setindeki satÄ±r sayÄ±sÄ±nÄ±n 1.935 kat artmasÄ± anlamÄ±na gelir. DeÄŸerlerinden basitÃ§e `d_` Ã¶nekini kaldÄ±rÄ±p bunlarÄ± tam sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rerek artÄ±k bir **gÃ¼n Ã¶zelliÄŸine** sahip olursunuz.

Bunun dÄ±ÅŸÄ±nda, kod parÃ§acÄ±ÄŸÄ± satÄ±rlarÄ±n bir tutma kÃ¼mesini (holdout) de ayÄ±rÄ±r. Bu tutma kÃ¼mesi sizin **doÄŸrulama setinizdir**. DoÄŸrulama stratejisi, eÄŸitim verilerinin bir kÄ±smÄ±nÄ± zamana dayalÄ± olarak ayÄ±rmaya dayanÄ±r. EÄŸitim kÄ±smÄ±na, saÄŸladÄ±ÄŸÄ±nÄ±z tahmin ufkuna (gelecekte tahmin etmek istediÄŸiniz gÃ¼n sayÄ±sÄ±) gÃ¶re tahminleriniz iÃ§in gerekli satÄ±rlarÄ± da ekleyecektir.

Ä°ÅŸte temel Ã¶zellik ÅŸablonumuzu oluÅŸturan fonksiyon. GiriÅŸ olarak `train_df` DataFrame'ini, eÄŸitimin bittiÄŸi gÃ¼nÃ¼n sayÄ±sÄ±nÄ± ve tahmin ufkunu alÄ±r:

```python
def generate_base_grid(train_df, end_train_day_x, predict_horizon):
    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']
    
    # Veri setini uzun formata dÃ¶nÃ¼ÅŸtÃ¼r (melt)
    grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name='sales')
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    
    # Orijinal gÃ¼n sÃ¼tununu sakla ve sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼r
    grid_df['d_org'] = grid_df['d']
    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)
    
    # DoÄŸrulama (holdout) setini ayÄ±r
    time_mask = (grid_df['d'] > end_train_day_x) &  (grid_df['d'] <= end_train_day_x + predict_horizon)
    holdout_df = grid_df.loc[time_mask, ["id", "d", "sales"]].reset_index(drop=True)
    holdout_df.to_feather(f"holdout_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(holdout_df)
    gc.collect()
    
    # EÄŸitim verilerini son eÄŸitim gÃ¼nÃ¼ne kadar filtrele
    grid_df = grid_df[grid_df['d'] <= end_train_day_x]
    
    # d sÃ¼tununu orijinal 'd_xxx' formatÄ±na geri Ã§evir
    grid_df['d'] = grid_df['d_org']
    grid_df = grid_df.drop('d_org', axis=1)
    
    # Tahmin ufku iÃ§in boÅŸ satÄ±rlarÄ± ekle (NaN satÄ±ÅŸlarla)
    add_grid = pd.DataFrame()
    for i in range(predict_horizon):
        temp_df = train_df[index_columns]
        temp_df = temp_df.drop_duplicates()
        temp_df['d'] = 'd_' + str(end_train_day_x + i + 1)
        temp_df['sales'] = np.nan
        add_grid = pd.concat([add_grid, temp_df])
    
    grid_df = pd.concat([grid_df, add_grid])
    grid_df = grid_df.reset_index(drop=True)
    
    # Ä°ndeks sÃ¼tunlarÄ±nÄ± kategori tipine dÃ¶nÃ¼ÅŸtÃ¼r
    for col in index_columns:
        grid_df[col] = grid_df[col].astype('category')
    
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    # SonuÃ§ DataFrame'i diske kaydet
    grid_df.to_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(grid_df)
    gc.collect()
```

Temel Ã¶zellik ÅŸablonunu oluÅŸturma fonksiyonunu hallettikten sonra, pandas DataFrame'leri iÃ§in bellek alanÄ±ndan tasarruf etmeye ve bÃ¼yÃ¼k veri setlerini iÅŸlerken bellek hatalarÄ±nÄ± Ã¶nlemeye yardÄ±mcÄ± olacak bir birleÅŸtirme (merge) fonksiyonu hazÄ±rlÄ±yoruz. Ä°ki DataFrame (`df1` ve `df2`) ve birleÅŸtirilmesi gereken yabancÄ± anahtarlar kÃ¼mesi verildiÄŸinde, fonksiyon yeni bir birleÅŸtirilmiÅŸ nesne oluÅŸturmadan sadece mevcut `df1` DataFrame'ini geniÅŸleterek `df1` ve `df2` arasÄ±nda bir sol dÄ±ÅŸ birleÅŸtirme (left outer join) uygular.

Fonksiyon ilk olarak `df1`'den yabancÄ± anahtarlarÄ± Ã§Ä±kararak Ã§alÄ±ÅŸÄ±r, ardÄ±ndan Ã§Ä±karÄ±lan anahtarlarÄ± `df2` ile birleÅŸtirir. Bu ÅŸekilde, fonksiyon `df1` ile aynÄ± sÄ±rada olan `merged_gf` adlÄ± yeni bir DataFrame oluÅŸturur. Bu noktada, `merged_gf` sÃ¼tunlarÄ±nÄ± `df1`'e atarÄ±z. Dahili olarak, `df1` dahili veri yapÄ±larÄ±na referansÄ± `merged_gf`'den alacaktÄ±r. BÃ¶yle bir yaklaÅŸÄ±m, yalnÄ±zca gerekli kullanÄ±lan veriler her an oluÅŸturulduÄŸu iÃ§in bellek kullanÄ±mÄ±nÄ± en aza indirmeye yardÄ±mcÄ± olur (belleÄŸi doldurabilecek kopyalar yoktur). Fonksiyon `df1`'i dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nde, `merged_gf` artÄ±k `df1` tarafÄ±ndan kullanÄ±lan veriler hariÃ§ iptal edilir.

Ä°ÅŸte bu yardÄ±mcÄ± fonksiyonun kodu:

```python
def merge_by_concat(df1, df2, merge_on):
    merged_gf = df1[merge_on]
    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')
    new_columns = [col for col in list(merged_gf) 
                   if col not in merge_on]
    df1[new_columns] = merged_gf[new_columns]
    return df1
```

Bu gerekli adÄ±mdan sonra, verileri iÅŸlemek iÃ§in yeni bir fonksiyon programlamaya devam ediyoruz. Bu sefer fiyat verilerini, yani her Ã¼rÃ¼nÃ¼n her maÄŸaza iÃ§in tÃ¼m haftalardaki fiyatlarÄ±nÄ± iÃ§eren bir veri kÃ¼mesini ele alÄ±yoruz. Bir maÄŸazada yeni bir Ã¼rÃ¼nÃ¼n gÃ¶rÃ¼nÃ¼p gÃ¶rÃ¼nmediÄŸini anlamak Ã¶nemli olduÄŸundan, fonksiyon fiyat mevcudiyetinin ilk tarihini (fiyat tablosundaki haftanÄ±n kimliÄŸini temsil eden `wm_yr_wk` Ã¶zelliÄŸini kullanarak) alÄ±r ve bunu Ã¶zellik ÅŸablonumuza kopyalar.

Ä°ÅŸte yayÄ±n tarihlerini iÅŸleme kodu:

```python
def calc_release_week(prices_df, end_train_day_x, predict_horizon):
    index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']
    
    grid_df = pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    # MaÄŸaza ve Ã¼rÃ¼n bazÄ±nda ilk (minimum) fiyatÄ±n gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ haftayÄ± bul
    release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()
    release_df.columns = ['store_id', 'item_id', 'release']
    
    # grid_df'ye bu "yayÄ±nlanma" haftasÄ±nÄ± ekle
    grid_df = merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])
    
    del release_df
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    gc.collect()
    
    # Takvim verilerini (wm_yr_wk ve d sÃ¼tunlarÄ±nÄ±) grid_df'ye ekle
    grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])
 
    grid_df = grid_df.reset_index(drop=True)
    # HaftalarÄ± normalize et (ilk haftadan itibaren kaÃ§ hafta geÃ§tiÄŸini bul)
    grid_df['release'] = grid_df['release'] - grid_df['release'].min()
    grid_df['release'] = grid_df['release'].astype(np.int16)
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    
    # GÃ¼ncellenmiÅŸ DataFrame'i diske kaydet
    grid_df.to_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    del(grid_df)
    gc.collect()
```

ÃœrÃ¼nÃ¼n bir maÄŸazada gÃ¶rÃ¼nme gÃ¼nÃ¼nÃ¼ hallettikten sonra, fiyatlarla ilgilenmeye devam ediyoruz. Her maÄŸazadaki her bir Ã¼rÃ¼n iÃ§in temel fiyat Ã¶zelliklerini hazÄ±rlÄ±yoruz, bunlar:

  * GerÃ§ek fiyat (maksimuma gÃ¶re normalize edilmiÅŸ)
  * Maksimum fiyat
  * Minimum fiyat
  * Ortalama fiyat
  * FiyatÄ±n standart sapmasÄ±
  * ÃœrÃ¼nÃ¼n aldÄ±ÄŸÄ± farklÄ± fiyat sayÄ±sÄ±
  * MaÄŸazada aynÄ± fiyata sahip Ã¼rÃ¼n sayÄ±sÄ±

FiyatlarÄ±n bu temel tanÄ±mlayÄ±cÄ± istatistiklerinin yanÄ± sÄ±ra, bir maÄŸazadaki her Ã¼rÃ¼n iÃ§in farklÄ± zaman ayrÄ±ntÄ±larÄ±na dayalÄ± olarak fiyat dinamiklerini tanÄ±mlayan bazÄ± Ã¶zellikler de ekliyoruz:

  * **GÃ¼n momentumu**, yani gerÃ§ek fiyatÄ±n bir Ã¶nceki gÃ¼nkÃ¼ fiyatÄ±na oranÄ±
  * **Ay momentumu**, yani gerÃ§ek fiyatÄ±n aynÄ± aydaki ortalama fiyatÄ±na oranÄ±
  * **YÄ±l momentumu**, yani gerÃ§ek fiyatÄ±n aynÄ± yÄ±ldaki ortalama fiyatÄ±na oranÄ±

> ğŸ“ AlÄ±ÅŸtÄ±rma 1
> 
> 
> 
> Daha fazla fiyata dayalÄ± Ã¶zellik oluÅŸturabilir misiniz? Ã–rneÄŸin, ortalama ve varyans dÄ±ÅŸÄ±ndaki diÄŸer tanÄ±mlayÄ±cÄ± istatistikleri kullanarak veya baÅŸka zaman ayrÄ±ntÄ±larÄ±nÄ± (Ã¶rneÄŸin, hafta veya Ã¼Ã§ aylÄ±k dÃ¶nem bazlÄ±) iÅŸleyerek?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Burada zaman serisi Ã¶zellik iÅŸleme iÃ§in iki ilginÃ§ ve temel pandas metodu kullanÄ±yoruz:

  * **`shift`**: Bu, indeksi $n$ adÄ±m ileri veya geri hareket ettirebilir ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html))
  * **`transform`**: Bu, her gruba uygulandÄ±ÄŸÄ±nda, aynÄ± indeksli bir Ã¶zelliÄŸi dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ deÄŸerlerle doldurur ([https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html))

AyrÄ±ca, Ã¼rÃ¼nÃ¼n **psikolojik fiyat eÅŸiklerinde** satÄ±ldÄ±ÄŸÄ± bir durumu ortaya Ã§Ä±karmak amacÄ±yla fiyatÄ±n ondalÄ±k kÄ±smÄ± bir Ã¶zellik olarak iÅŸlenir (Ã¶rneÄŸin, $19.99 veya $2.98 - ÅŸu tartÄ±ÅŸmaya bakÄ±nÄ±z: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011)). `math.modf` fonksiyonu ([https://docs.python.org/3.8/library/math.html\#math.modf](https://docs.python.org/3.8/library/math.html#math.modf)) herhangi bir kayan noktalÄ± sayÄ±yÄ± kesirli ve tam sayÄ± kÄ±sÄ±mlarÄ±na (iki Ã¶ÄŸeli bir demet) ayÄ±rdÄ±ÄŸÄ± iÃ§in bunu yapmaya yardÄ±mcÄ± olur.

Son olarak, ortaya Ã§Ä±kan tablo diske kaydedilir.

Ä°ÅŸte fiyatlar Ã¼zerindeki tÃ¼m Ã¶zellik mÃ¼hendisliÄŸini yapan fonksiyon:

```python
def generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    # Fiyat tanÄ±mlayÄ±cÄ± istatistiklerini hesapla (transform kullanarak)
    prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')
    prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')
    prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')
    prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')
    prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']
    prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')
    prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')
    
    calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]
    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])
    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')
    del calendar_prices
    gc.collect()
    
    # Fiyat momentumu Ã¶zelliklerini hesapla (shift ve transform kullanarak)
    prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform(lambda x: x.shift(1))
    prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])['sell_price'].transform('mean')
    prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])['sell_price'].transform('mean')
    
    # OndalÄ±k (kuruÅŸ) fiyat Ã¶zelliklerini oluÅŸtur
    prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]
    prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]
    prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]
    
    del prices_df['month'], prices_df['year']
    prices_df = reduce_mem_usage(prices_df, verbose=False)
    gc.collect()
    
    # Ana tabloya birleÅŸtir
    original_columns = list(grid_df)
    grid_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')
    del(prices_df)
    gc.collect()
    
    # Yeni sÃ¼tunlarÄ± seÃ§ ve kaydet
    keep_columns = [col for col in list(grid_df) if col not in original_columns]
    grid_df = grid_df[['id', 'd'] + keep_columns]
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(grid_df)
    gc.collect()
```

Bir sonraki fonksiyon, bunun yerine ayÄ±n evresini hesaplar ve sekiz evresinden birini dÃ¶ndÃ¼rÃ¼r (yeni aydan hilale kadar). Ay evreleri satÄ±ÅŸlarÄ± doÄŸrudan etkilemese de (hava koÅŸullarÄ± etkiler, ancak verilerde hava bilgisi yok), periyodik alÄ±ÅŸveriÅŸ davranÄ±ÅŸlarÄ±na iyi uyum saÄŸlayabilecek **29 buÃ§uk gÃ¼nlÃ¼k periyodik bir dÃ¶ngÃ¼yÃ¼** temsil ederler.

Ay evrelerinin neden bir tahmin edici olarak iÅŸe yarayabileceÄŸine dair farklÄ± hipotezlerin yer aldÄ±ÄŸÄ± ilginÃ§ bir tartÄ±ÅŸma, bu yarÄ±ÅŸma gÃ¶nderisinde bulunmaktadÄ±r: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776](https://www.google.com/search?q=https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776):

```python
def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase
    diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)
    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))
    lunations = dec("0.20439731") + (days * dec("0.03386319269"))
    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))
    return int(phase_index) & 7
```

Ay evresi fonksiyonu, zamana dayalÄ± Ã¶zellikler oluÅŸturmak iÃ§in genel bir fonksiyonun parÃ§asÄ±dÄ±r. Fonksiyon, takvim veri seti bilgilerini alÄ±r ve Ã¶zellikler arasÄ±na yerleÅŸtirir. Bu bilgiler, olaylarÄ± ve bunlarÄ±n tÃ¼rÃ¼nÃ¼, ayrÄ±ca temel mallarÄ±n satÄ±ÅŸlarÄ±nÄ± daha da artÄ±rabilecek **SNAP** (Ek Beslenme YardÄ±mÄ± ProgramÄ±) dÃ¶nemlerinin bir gÃ¶stergesini iÃ§erir. Fonksiyon ayrÄ±ca gÃ¼n, ay, yÄ±l, haftanÄ±n gÃ¼nÃ¼, ayÄ±n haftasÄ± ve hafta sonu olup olmadÄ±ÄŸÄ± gibi sayÄ±sal Ã¶zellikler de Ã¼retir. Ä°ÅŸte kod:

```python
def generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon):
    
    grid_df = pd.read_feather(
                f"grid_df_{end_train_day_x}_to_{end_train_day_x +      
                predict_horizon}.feather")
    grid_df = grid_df[['id', 'd']]
    gc.collect()

    # Ay evresini hesapla ve takvim verisine ekle
    calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)
    
    # Takvimi kÄ±smen birleÅŸtir
    icols = ['date',
 'd',
 'event_name_1',
 'event_type_1',
 'event_name_2',
 'event_type_2',
 'snap_CA',
 'snap_TX',
 'snap_WI',
 'moon',
             ]
    grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')
    
    # Olay ve SNAP sÃ¼tunlarÄ±nÄ± kategori tipine dÃ¶nÃ¼ÅŸtÃ¼r
    icols = ['event_name_1',
 'event_type_1',
 'event_name_2',
 'event_type_2',
 'snap_CA',
 'snap_TX',
 'snap_WI']
    for col in icols:
        grid_df[col] = grid_df[col].astype('category')
        
    # Tarih bazlÄ± sayÄ±sal Ã¶zellikleri oluÅŸtur
    grid_df['date'] = pd.to_datetime(grid_df['date'])
    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)
    grid_df['tm_w'] = grid_df['date'].dt.isocalendar().week.astype(np.int8)
    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)
    grid_df['tm_y'] = grid_df['date'].dt.year
    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)
    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: math.ceil(x / 7)).astype(np.int8)
    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)
    grid_df['tm_w_end'] = (grid_df['tm_dw'] >= 5).astype(np.int8)
    
    del(grid_df['date'])
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    
    # Sonucu kaydet
    grid_df.to_feather(f"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    del(grid_df)
    del(calendar_df)
    gc.collect()
```

AÅŸaÄŸÄ±daki fonksiyon bunun yerine sadece `wm_yr_wk` Ã¶zelliÄŸini kaldÄ±rÄ±r ve `d` (gÃ¼n) Ã¶zelliÄŸini sayÄ±sal bir Ã¶zelliÄŸe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Bu, sonraki Ã¶zellik dÃ¶nÃ¼ÅŸÃ¼m fonksiyonlarÄ± iÃ§in gerekli bir adÄ±mdÄ±r:

```python
def modify_grid_base(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)
    del grid_df['wm_yr_wk']
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(grid_df)
    gc.collect()
```

Son iki Ã¶zellik oluÅŸturma fonksiyonumuz, zaman serileri iÃ§in daha geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi Ã¼retecektir. Ä°lk fonksiyon hem **gecikmeli satÄ±ÅŸlarÄ± (lagged sales)** hem de **hareketli ortalamalarÄ±nÄ±** Ã¼retecektir. Ä°lk olarak, `shift` metodu kullanÄ±larak geÃ§miÅŸe dÃ¶nÃ¼k 15 gÃ¼ne kadar bir gecikmeli satÄ±ÅŸ aralÄ±ÄŸÄ± oluÅŸturulacaktÄ±r. Daha sonra, `shift`, `rolling` ([https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html)) ile birlikte kullanÄ±larak 7, 14, 30, 60 ve 180 gÃ¼nlÃ¼k pencerelerle hareketli ortalamalar oluÅŸturulacaktÄ±r.

`shift` komutu gereklidir Ã§Ã¼nkÃ¼ indeksi hareket ettirerek hesaplamalarÄ±nÄ±z iÃ§in her zaman **mevcut verileri** dikkate almanÄ±zÄ± saÄŸlar. DolayÄ±sÄ±yla, tahmin ufkunuz yedi gÃ¼ne kadar Ã§Ä±kÄ±yorsa, hesaplamalar sadece yedi gÃ¼n Ã¶nceki mevcut verileri dikkate alacaktÄ±r. ArdÄ±ndan, `rolling` komutu, Ã¶zetlenebilecek (bu durumda ortalama ile) bir hareketli gÃ¶zlem penceresi oluÅŸturacaktÄ±r. Bir dÃ¶nem Ã¼zerindeki ortalamaya (hareketli pencere) sahip olmak ve bunun evrimlerini takip etmek, eÄŸilimlerdeki herhangi bir deÄŸiÅŸikliÄŸi daha iyi tespit etmenize yardÄ±mcÄ± olacaktÄ±r, Ã§Ã¼nkÃ¼ zaman pencereleri boyunca tekrarlanmayan modeller dengelenecektir. Bu, gÃ¼rÃ¼ltÃ¼yÃ¼ ve ilgi Ã§ekmeyen modelleri kaldÄ±rmak iÃ§in zaman serisi analizinde yaygÄ±n bir stratejidir. Ã–rneÄŸin, yedi gÃ¼nlÃ¼k hareketli ortalama ile tÃ¼m gÃ¼nlÃ¼k modelleri iptal edecek ve sadece satÄ±ÅŸlarÄ±nÄ±zÄ±n haftalÄ±k bazda neler olduÄŸunu gÃ¶stereceksiniz.

> ğŸ“ AlÄ±ÅŸtÄ±rma 2
> 
> 
> 
> FarklÄ± hareketli ortalama pencereleriyle denemeler yapabilir misiniz? AyrÄ±ca farklÄ± stratejiler denemek de yardÄ±mcÄ± olabilir. Ã–rneÄŸin, zaman serilerine adanmÄ±ÅŸ olan Tabular Playground Ocak 2022'yi ([https://www.kaggle.com/competitions/tabular-playground-series-jan-2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022)) keÅŸfederek daha fazla fikir bulabilirsiniz, Ã§Ã¼nkÃ¼ Ã§Ã¶zÃ¼mlerin Ã§oÄŸu gradyan artÄ±rma kullanÄ±larak oluÅŸturulmuÅŸtur.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Ä°ÅŸte **gecikme (lag) ve hareketli ortalama** Ã¶zelliklerini oluÅŸturan kod:

```python
def generate_lag_feature(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    grid_df = grid_df[['id', 'd', 'sales']]
    num_lag_day_list = []
    num_lag_day = 15
    
    # Gecikme (lag) gÃ¼nleri listesini oluÅŸtur (tahmin ufkundan baÅŸlayarak 15 gÃ¼n Ã¶ncesine kadar)
    for col in range(predict_horizon, predict_horizon + num_lag_day):
        num_lag_day_list.append(col)
        
    # Gecikmeli satÄ±ÅŸ Ã¶zelliklerini oluÅŸtur
    grid_df = grid_df.assign(**{
        '{}_lag_{}'.format('sales', l): grid_df.groupby(['id'])['sales'].transform(lambda x: x.shift(l))
        for l in num_lag_day_list
    })
    
    # Bellek kullanÄ±mÄ±nÄ± optimize et
    for col in list(grid_df):
        if 'lag' in col:
            grid_df[col] = grid_df[col].astype(np.float16)
            
    # Hareketli ortalama pencerelerini tanÄ±mla
    num_rolling_day_list = [7, 14, 30, 60, 180]
    
    # Hareketli ortalama ve standart sapma Ã¶zelliklerini oluÅŸtur
    for num_rolling_day in num_rolling_day_list:
        grid_df['rolling_mean_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(
            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).mean()).astype(np.float16)
        grid_df['rolling_std_' + str(num_rolling_day)] = grid_df.groupby(['id'])['sales'].transform(
            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).std()).astype(np.float16)
            
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    
    # Sonucu diske kaydet
    grid_df.to_feather(f"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(grid_df)
    gc.collect()
```

Ä°kinci geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi fonksiyonu ise, eyalet, maÄŸaza, kategori, departman ve satÄ±lan Ã¼rÃ¼n arasÄ±ndaki belirli deÄŸiÅŸken gruplarÄ±nÄ± alÄ±p bunlarÄ±n **ortalama (mean)** ve **standart sapmasÄ±nÄ± (standard deviation)** temsil eden bir **kodlama fonksiyonudur**. Bu gÃ¶mÃ¼lmeler (embeddings) **zamandan baÄŸÄ±msÄ±zdÄ±r** (zaman, gruplandÄ±rmanÄ±n bir parÃ§asÄ± deÄŸildir) ve eÄŸitim algoritmasÄ±nÄ±n Ã¼rÃ¼nlerin, kategorilerin ve maÄŸazalarÄ±n (ve bunlarÄ±n kombinasyonlarÄ±nÄ±n) kendi aralarÄ±nda nasÄ±l farklÄ±laÅŸtÄ±ÄŸÄ±nÄ± ayÄ±rt etmesine yardÄ±mcÄ± olma rolÃ¼ne sahiptir.

> ğŸ“ AlÄ±ÅŸtÄ±rma 3
> 
> 
> 
> Ã–nerilen gÃ¶mÃ¼lmeler (embeddings), **Kaggle KitabÄ±'nÄ±n 216. sayfasÄ±nda** aÃ§Ä±klandÄ±ÄŸÄ± gibi **hedef kodlama (target encoding)** kullanÄ±larak da hesaplanabilir. Hedef kodlama gÃ¶mÃ¼lmelerini uygulayabilir ve daha iyi sonuÃ§larÄ± nasÄ±l elde edeceÄŸinizi bulabilir misiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Kod, Ã¶zellikleri gruplayarak, tanÄ±mlayÄ±cÄ± istatistiklerini (bizim durumumuzda ortalama veya standart sapma) hesaplayarak ve ardÄ±ndan sonuÃ§larÄ± daha Ã¶nce tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z **`transform`** metodunu kullanarak veri setine uygulayarak Ã§alÄ±ÅŸÄ±r:

```python
def generate_target_encoding_feature(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    # Ä°leriye dÃ¶nÃ¼k tahmin yapÄ±lacak gÃ¼nlerdeki 'sales' deÄŸerlerini NaN yap (hedef sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek iÃ§in)
    grid_df.loc[grid_df['d'] > (end_train_day_x - predict_horizon), 'sales'] = np.nan
    base_cols = list(grid_df)
    
    # GruplandÄ±rma kombinasyonlarÄ±
    icols = [
        ['state_id'],
        ['store_id'],
        ['cat_id'],
        ['dept_id'],
        ['state_id', 'cat_id'],
        ['state_id', 'dept_id'],
        ['store_id', 'cat_id'],
        ['store_id', 'dept_id'],
        ['item_id'],
        ['item_id', 'state_id'],
        ['item_id', 'store_id']
    ]
    
    # Her bir gruplandÄ±rma iÃ§in ortalama ve standart sapma (hedef kodlama) hesapla
    for col in icols:
        col_name = '_' + '_'.join(col) + '_'
        grid_df['enc' + col_name + 'mean'] = grid_df.groupby(col)['sales'].transform('mean').astype(np.float16)
        grid_df['enc' + col_name + 'std'] = grid_df.groupby(col)['sales'].transform('std').astype(np.float16)
        
    # Yeni oluÅŸturulan Ã¶zellikleri seÃ§
    keep_cols = [col for col in list(grid_df) if col not in base_cols]
    grid_df = grid_df[['id', 'd'] + keep_cols]
    
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    
    # Sonucu diske kaydet
    grid_df.to_feather(f"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    del(grid_df)
    gc.collect()
```

Ã–zellik mÃ¼hendisliÄŸi bÃ¶lÃ¼mÃ¼nÃ¼ tamamladÄ±ktan sonra, ÅŸimdi Ã¶zellikleri oluÅŸtururken diskte sakladÄ±ÄŸÄ±mÄ±z tÃ¼m dosyalarÄ± bir araya getirmeye geÃ§iyoruz. AÅŸaÄŸÄ±daki fonksiyon sadece temel Ã¶zellikler, fiyat Ã¶zellikleri, takvim Ã¶zellikleri, gecikme/hareketli ve gÃ¶mÃ¼lÃ¼ Ã¶zelliklerin farklÄ± veri setlerini yÃ¼kler ve hepsini birleÅŸtirir. Kod daha sonra yalnÄ±zca belirli bir maÄŸazaya ait satÄ±rlarÄ± filtreler ve ayrÄ± bir veri seti olarak kaydeder.

BÃ¶yle bir yaklaÅŸÄ±m, **belirli bir zaman aralÄ±ÄŸÄ± iÃ§in tahmin yapmayÄ± amaÃ§layan belirli bir maÄŸaza Ã¼zerinde eÄŸitilmiÅŸ bir modele sahip olma** stratejisiyle eÅŸleÅŸir:

```python
def assemble_grid_by_store(train_df, end_train_day_x, predict_horizon):
    # Temel, fiyat ve takvim Ã¶zelliklerini birleÅŸtir
    grid_df = pd.concat([pd.read_feather(f"grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather"),
                         pd.read_feather(f"grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather").iloc[:, 2:],
                         pd.read_feather(f"grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather").iloc[:, 2:]],
                         axis=1)
    gc.collect()
    
    store_id_set_list = list(train_df['store_id'].unique())
    index_store = dict()
    
    # Her maÄŸaza iÃ§in veriyi ayÄ±r ve kaydet
    for store_id in store_id_set_list:
        extract = grid_df[grid_df['store_id'] == store_id]
        index_store[store_id] = extract.index.to_numpy() # Orijinal indeksi sakla
        extract = extract.reset_index(drop=True)
        extract.to_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
        
    del(grid_df)
    gc.collect()
    
    # Hedef kodlama Ã¶zelliklerini birleÅŸtir
    mean_features = [
 'enc_cat_id_mean', 'enc_cat_id_std',
 'enc_dept_id_mean', 'enc_dept_id_std',
 'enc_item_id_mean', 'enc_item_id_std'
        ]
    df2 = pd.read_feather(f"target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")[mean_features]
    
    for store_id in store_id_set_list:
        df = pd.read_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
        # YalnÄ±zca ilgili satÄ±rlarÄ± (saklanan indekslere gÃ¶re) df2'den al ve birleÅŸtir
        df = pd.concat([df, df2[df2.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)
        df.to_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
        
    del(df2)
    gc.collect()
    
    # Gecikme Ã¶zelliklerini birleÅŸtir
    df3 = pd.read_feather(f"lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather").iloc[:, 3:]
    
    for store_id in store_id_set_list:
        df = pd.read_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
        # YalnÄ±zca ilgili satÄ±rlarÄ± (saklanan indekslere gÃ¶re) df3'ten al ve birleÅŸtir
        df = pd.concat([df, df3[df3.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)
        df.to_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
        
    del(df3)
    del(store_id_set_list)
    gc.collect()
```

AÅŸaÄŸÄ±daki fonksiyon ise, bir Ã¶ncekinden yapÄ±lan seÃ§imi daha fazla iÅŸleyerek kullanÄ±lmayan Ã¶zellikleri kaldÄ±rÄ±r ve sÃ¼tunlarÄ± yeniden sÄ±ralar, eÄŸitilecek bir model iÃ§in verileri dÃ¶ndÃ¼rÃ¼r:

```python
def load_grid_by_store(end_train_day_x, predict_horizon, store_id):
    df = pd.read_feather(f"grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather")
    
    # KaldÄ±rÄ±lacak ve etkinleÅŸtirilecek Ã¶zellikleri belirle
    remove_features = ['id', 'state_id', 'store_id', 'date', 'wm_yr_wk', 'd', 'sales']
    enable_features = [col for col in list(df) if col not in remove_features]
    
    # Gerekli sÃ¼tunlarÄ± yeniden sÄ±rala
    df = df[['id', 'd', 'sales'] + enable_features]
    
    df = reduce_mem_usage(df, verbose=False)
    gc.collect()
    return df, enable_features
```

Son olarak, artÄ±k **eÄŸitim aÅŸamasÄ±yla** ilgilenebiliriz. AÅŸaÄŸÄ±daki kod parÃ§acÄ±ÄŸÄ±, Monsaraida'nÄ±n problem Ã¼zerinde en etkili olduÄŸunu aÃ§Ä±kladÄ±ÄŸÄ± eÄŸitim parametrelerini tanÄ±mlayarak baÅŸlar. EÄŸitim sÃ¼resi nedenleriyle, artÄ±rma tÃ¼rÃ¼nÃ¼ deÄŸiÅŸtirerek **Gradyan ArtÄ±rma Karar AÄŸacÄ± (GBDT)** yerine **Gradyan TabanlÄ± Tek TaraflÄ± Ã–rnekleme (GOSS)** kullanmayÄ± seÃ§tik, Ã§Ã¼nkÃ¼ bu, performanstan Ã§ok fazla kayÄ±p olmadan eÄŸitimi gerÃ§ekten hÄ±zlandÄ±rabilir. Modele iyi bir hÄ±z artÄ±ÅŸÄ±, ayrÄ±ca `subsample` parametresi ve `feature fraction` ile de saÄŸlanÄ±r: gradyan artÄ±rmanÄ±n her Ã¶ÄŸrenme adÄ±mÄ±nda, Ã¶rneklerin sadece yarÄ±sÄ± ve Ã¶zelliklerin sadece yarÄ±sÄ± dikkate alÄ±nacaktÄ±r.

> LightGBM'i doÄŸru derleme seÃ§enekleriyle makinenizde derlemek de hÄ±zÄ±nÄ±zÄ± artÄ±rabilir, bu ilginÃ§ yarÄ±ÅŸma tartÄ±ÅŸmasÄ±nda aÃ§Ä±klandÄ±ÄŸÄ± gibi: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.google.com/search?q=https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273).

GÃ¼Ã§ deÄŸeri 1.1 olan **Tweedie kaybÄ±** (dolayÄ±sÄ±yla Poisson'a daha yakÄ±n bir temel daÄŸÄ±lÄ±ma sahip), **aralÄ±klÄ± serileri** (sÄ±fÄ±r satÄ±ÅŸlarÄ±n baskÄ±n olduÄŸu yerlerde) modellemede Ã¶zellikle etkili gÃ¶rÃ¼nmektedir. KullanÄ±lan metrik sadece **karekÃ¶k ortalama karesel hatadÄ±r (root mean squared error)** (yarÄ±ÅŸma metriÄŸini temsil etmek iÃ§in Ã¶zel bir metrik kullanmaya gerek yoktur). AyrÄ±ca Kaggle not defterinde bellekten tasarruf etmek iÃ§in `force_row_wise` parametresini kullanÄ±yoruz. DiÄŸer tÃ¼m parametreler, Monsaraida'nÄ±n Ã§Ã¶zÃ¼mÃ¼nde sunduklarÄ±yla tamamen aynÄ±dÄ±r (ancak `goss` artÄ±rma tÃ¼rÃ¼yle uyumsuzluÄŸu nedeniyle devre dÄ±ÅŸÄ± bÄ±rakÄ±lan alt Ã¶rnekleme parametresi hariÃ§).

> ğŸ“ AlÄ±ÅŸtÄ±rma 4
> 
> 
> 
> Tweedie kaybÄ± baÅŸka hangi Kaggle yarÄ±ÅŸmasÄ±nda faydalÄ± olmuÅŸtur? Meta Kaggle veri setindeki ([https://www.kaggle.com/datasets/kaggle/meta-kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle)) **ForumTopics** ve **ForumMessages** CSV tablolarÄ±nÄ± keÅŸfederek bu kayÄ±p (loss) ve kullanÄ±mÄ±na dair faydalÄ± tartÄ±ÅŸmalar bulabilir misiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

EÄŸitim parametrelerini tanÄ±mladÄ±ktan sonra, sadece maÄŸazalar Ã¼zerinde dÃ¶ngÃ¼ kuruyoruz, her seferinde tek bir maÄŸazanÄ±n eÄŸitim verilerini yÃ¼klÃ¼yor ve LightGBM modelini eÄŸitiyoruz. Her model daha sonra **pickle dump** ile kaydedilir. AyrÄ±ca, her bir modelden **Ã¶zellik Ã¶nemini (feature importance)** Ã§Ä±karÄ±yoruz, bu bilgiyi bir dosyada birleÅŸtirmek ve ardÄ±ndan toplamak amacÄ±yla, bÃ¶ylece o tahmin ufku iÃ§in her Ã¶zellik bazÄ±nda **tÃ¼m maÄŸazalardaki ortalama Ã¶nemi** elde ediyoruz.

> ğŸ“ AlÄ±ÅŸtÄ±rma 5
> 
> 
> 
> Her modelden gelen farklÄ± **Ã¶zellik Ã¶nem raporlarÄ±nÄ±** analiz edin. BunlarÄ± Ã§izdirip **ortak Ã¶rÃ¼ntÃ¼ler** arayabilir misiniz? Modelin, zamanda gittikÃ§e **daha uzak tahminlerle** uÄŸraÅŸmak zorunda kaldÄ±kÃ§a gÃ¶sterdiÄŸi davranÄ±ÅŸ hakkÄ±nda ne anlayabilirsiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Ä°ÅŸte belirli bir tahmin ufku iÃ§in tÃ¼m modelleri eÄŸitmek Ã¼zere hazÄ±rlanmÄ±ÅŸ **tam fonksiyon**:

```python
def train(train_df, seed, end_train_day_x, predict_horizon):
    lgb_params = {
        'boosting_type': 'goss', # Gradyan TabanlÄ± Tek TaraflÄ± Ã–rnekleme (daha hÄ±zlÄ±)
        'objective': 'tweedie',
        'tweedie_variance_power': 1.1, # Poisson'a yakÄ±n
        'metric': 'rmse',
        #'subsample': 0.5, # GOSS ile uyumsuz olduÄŸu iÃ§in devre dÄ±ÅŸÄ±
        #'subsample_freq': 1,
        'learning_rate': 0.03,
        'num_leaves': 2 ** 11 - 1,
        'min_data_in_leaf': 2 ** 12 - 1,
        'feature_fraction': 0.5, # Ã–zelliklerin %50'si kullanÄ±lÄ±r
        'max_bin': 100,
        'boost_from_average': False,
        'num_boost_round': 1400,
        'verbose': -1,
        'num_threads': os.cpu_count(),
        'force_row_wise': True, # Bellek tasarrufu iÃ§in
    }
    
    # Rastgelelik tohumlarÄ±nÄ± ayarla
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    lgb_params['seed'] = seed
    
    store_id_set_list = list(train_df['store_id'].unique())
    print(f"training stores: {store_id_set_list}")
    feature_importance_all_df = pd.DataFrame()
    
    for store_index, store_id in enumerate(store_id_set_list):
        print(f'now training {store_id} store')
        
        # MaÄŸazaya gÃ¶re veriyi yÃ¼kle
        grid_df, enable_features = load_grid_by_store(end_train_day_x, predict_horizon, store_id)
        
        # EÄŸitim ve doÄŸrulama maskelerini oluÅŸtur
        train_mask = grid_df['d'] <= end_train_day_x
        valid_mask = train_mask & (grid_df['d'] > (end_train_day_x - predict_horizon))
        preds_mask = grid_df['d'] > (end_train_day_x - 100) # Tahminler iÃ§in son 100 gÃ¼nÃ¼ al
        
        # LightGBM veri setlerini oluÅŸtur
        train_data = lgb.Dataset(grid_df[train_mask][enable_features],
                                 label=grid_df[train_mask]['sales'])
        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],
                                 label=grid_df[valid_mask]['sales'])
                                 
        # Ä°lerideki tahminler iÃ§in veri setinin bir kÄ±smÄ±nÄ± kaydet
        grid_df = grid_df[preds_mask].reset_index(drop=True)
        grid_df.to_feather(f'test_{store_id}_{predict_horizon}.feather')
        del(grid_df)
        gc.collect()
        
        # LightGBM modelini eÄŸit
        estimator = lgb.train(lgb_params,
                              train_data,
                              valid_sets=[valid_data],
                              callbacks=[lgb.log_evaluation(period=100, show_stdv=False)],
                              )
                              
        # Ã–zellik Ã¶nemini Ã§Ä±kar ve kaydet
        model_name = str(f'lgb_model_{store_id}_{predict_horizon}.bin')
        feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),
                                                   columns=['feature_name', 'importance'])
        feature_importance_store_df = feature_importance_store_df.sort_values('importance', ascending=False)
        feature_importance_store_df['store_id'] = store_id
        feature_importance_store_df.to_csv(f'feature_importance_{store_id}_{predict_horizon}.csv', index=False)
        
        # TÃ¼m maÄŸazalarÄ±n Ã¶zellik Ã¶nemini birleÅŸtir
        feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])
        
        # Modeli kaydet (pickle)
        pickle.dump(estimator, open(model_name, 'wb'))
        
        del([train_data, valid_data, estimator])
        gc.collect()
        
    # TÃ¼m maÄŸazalarÄ±n birleÅŸtirilmiÅŸ Ã¶zellik Ã¶nemini kaydet
    feature_importance_all_df.to_csv(f'feature_importance_all_{predict_horizon}.csv', index=False)
    
    # Ortalama Ã¶zellik Ã¶nemini hesapla ve kaydet
    feature_importance_agg_df = feature_importance_all_df.groupby('feature_name')['importance'].agg(['mean', 'std']).reset_index()
    feature_importance_agg_df.columns = ['feature_name', 'importance_mean', 'importance_std']
    feature_importance_agg_df = feature_importance_agg_df.sort_values('importance_mean', ascending=False)
    feature_importance_agg_df.to_csv(f'feature_importance_agg_{predict_horizon}.csv', index=False)
```

Son fonksiyon da hazÄ±rlandÄ±ÄŸÄ±na gÃ¶re, tÃ¼m gerekli kodlar iÅŸlem hattÄ±mÄ±z iÃ§in hazÄ±r demektir. TÃ¼m operasyonlarÄ± bir araya getiren fonksiyon iÃ§in, girdi veri setlerine (zaman serisi veri seti, fiyat veri seti ve takvim bilgileri) son eÄŸitim gÃ¼nÃ¼ (herkese aÃ§Ä±k liderlik tablosunda tahmin iÃ§in 1.913, Ã¶zel liderlik tablosu iÃ§in 1.941) ve tahmin ufku (7, 14, 21 veya 28 gÃ¼n olabilir) ile birlikte ihtiyacÄ±mÄ±z var:

```python
def train_pipeline(train_df, prices_df, calendar_df,  
                   end_train_day_x_list, prediction_horizon_list):
    for end_train_day_x in end_train_day_x_list:
        for predict_horizon in prediction_horizon_list:
            print(f"end training point day: {end_train_day_x} - prediction horizon: {predict_horizon} days")
            
            # Veri hazÄ±rlama
            generate_base_grid(train_df, end_train_day_x, predict_horizon)
            calc_release_week(prices_df, end_train_day_x, predict_horizon)
            generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)
            generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon)
            modify_grid_base(end_train_day_x, predict_horizon)
            generate_lag_feature(end_train_day_x, predict_horizon)
            generate_target_encoding_feature(end_train_day_x, predict_horizon)
            assemble_grid_by_store(train_df, end_train_day_x, predict_horizon)
            
            # Modelleme
            train(train_df, seed, end_train_day_x, predict_horizon)
```

Kaggle not defterlerinin sÄ±nÄ±rlÄ± bir Ã§alÄ±ÅŸma sÃ¼resi ve sÄ±nÄ±rlÄ± miktarda hem bellek hem de disk alanÄ± olduÄŸundan, Ã¶nerilen stratejimiz, burada sunulan kodla dÃ¶rt not defterini Ã§oÄŸaltmak ve bunlarÄ± farklÄ± tahmin ufku parametreleriyle eÄŸitmektir. Not defterleri iÃ§in aynÄ± adÄ± kullanmak, ancak tahmin parametresinin deÄŸerini iÃ§eren bir kÄ±smÄ± eklemek, modelleri daha sonra baÅŸka bir not defterinde harici veri setleri olarak toplama ve iÅŸleme konusunda yardÄ±mcÄ± olacaktÄ±r. Bu not defterlerinin her birinin standart bir Kaggle not defterinde Ã§alÄ±ÅŸmasÄ± yaklaÅŸÄ±k **6 buÃ§uk saat** sÃ¼recektir.

Ä°ÅŸte ilk not defteri, **m5-train-day-1941-horizon-7** ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7)):

```python
end_train_day_x_list = [1941]
prediction_horizon_list = [7]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)
```

Ä°kinci not defteri, **m5-train-day-1941-horizon-14** ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14](https://www.google.com/search?q=https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14)):

```python
end_train_day_x_list = [1941]
prediction_horizon_list = [14]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)
```

ÃœÃ§Ã¼ncÃ¼ not defteri, **m5-train-day-1941-horizon-21** ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21](https://www.google.com/search?q=https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21)):

```python
end_train_day_x_list = [1941]
prediction_horizon_list = [21]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)
```

Ve nihayet sonuncusu, **m5-train-day-1941-horizon-28** ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28](https://www.google.com/search?q=https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28)):

```python
end_train_day_x_list = [1941]
prediction_horizon_list = [28]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)
```

EÄŸer yeterli disk alanÄ± ve bellek kaynaklarÄ±na sahip yerel bir bilgisayarda Ã§alÄ±ÅŸÄ±yorsanÄ±z, tÃ¼m dÃ¶rt tahmin ufkunu iÃ§eren listeyi, yani `[7, 14, 21, 28]`'i girdi olarak kullanarak hepsini birlikte Ã§alÄ±ÅŸtÄ±rabilirsiniz. Åimdi, tahminimizi gÃ¶nderebilmemizden Ã¶nceki son adÄ±m, tahminleri bir araya getirmektir.

## Herkese aÃ§Ä±k (public) ve Ã¶zel (private) tahminleri bir araya getirme *(Assembling public and private predictions)*

Hem herkese aÃ§Ä±k hem de Ã¶zel liderlik tablolarÄ± iÃ§in tahminleri nasÄ±l bir araya getirdiÄŸimize dair bir Ã¶rneÄŸi burada gÃ¶rebilirsiniz:

  * **Herkese aÃ§Ä±k liderlik tablosu Ã¶rneÄŸi**: [https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard](https://www.google.com/search?q=https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard)
  * **Ã–zel liderlik tablosu Ã¶rneÄŸi**: [https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard](https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard)

Herkese aÃ§Ä±k ve Ã¶zel gÃ¶nderimler arasÄ±nda deÄŸiÅŸen tek ÅŸey, **farklÄ± son eÄŸitim gÃ¼nÃ¼dÃ¼r**: bu, hangi gÃ¼nleri tahmin edeceÄŸimizi belirler. Herkese aÃ§Ä±k liderlik tablosu not defterinde son eÄŸitim gÃ¼nÃ¼ **1.913** olarak ayarlanmÄ±ÅŸtÄ±r ve Ã¶zel olanda **1.941** olarak ayarlanmÄ±ÅŸtÄ±r.

AslÄ±nda, sadece doÄŸrulama amaÃ§larÄ± iÃ§in, geÃ§miÅŸ tutma (holdout) doÄŸrulama setleri oluÅŸturmak Ã¼zere ÅŸu tarihleri kullanarak herkese aÃ§Ä±k sÃ¼rÃ¼m not defterinin baÅŸka versiyonlarÄ±nÄ± oluÅŸturabilirsiniz: **[1885, 1857, 1829, 1577]**. BÃ¶ylece not defteri, modelin tahmin yeteneÄŸini doÄŸrulamak iÃ§in yerel olarak test edebileceÄŸiniz tahminler Ã¼retecektir.

> ğŸ“ AlÄ±ÅŸtÄ±rma 6
> 
> 
> 
> LÃ¼tfen **farklÄ± tutma (holdout) sÃ¼relerini** deneyin ve doÄŸrulama skorlarÄ±nÄ± kaydedin. Model nasÄ±l davranÄ±yor? GeÃ§miÅŸte de iyi Ã§alÄ±ÅŸtÄ±ÄŸÄ± (saÄŸlam bir Ã§Ã¶zÃ¼m olduÄŸu) **doÄŸrulandÄ± mÄ±**?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Bu sonuÃ§ kodu parÃ§acÄ±ÄŸÄ±nda, LightGBM gibi gerekli paketleri yÃ¼kledikten sonra, her bir **eÄŸitim bitiÅŸ gÃ¼nÃ¼** ve her bir **tahmin ufku** iÃ§in doÄŸru not defterini verileriyle birlikte kurtarÄ±yoruz. ArdÄ±ndan, tÃ¼m maÄŸazalar Ã¼zerinde dÃ¶ngÃ¼ kuruyor ve Ã¶nceki tahmin ufkundan ÅŸimdiki tahmin ufkuna kadar olan zaman aralÄ±ÄŸÄ±ndaki tÃ¼m Ã¼rÃ¼nler iÃ§in satÄ±ÅŸlarÄ± tahmin ediyoruz. Bu ÅŸekilde, her model sadece eÄŸitildiÄŸi tek bir hafta iÃ§in tahmin yapacaktÄ±r:

```python
import numpy as np
import pandas as pd
import os
import random
import math
from decimal import Decimal as dec
import datetime
import time
import gc
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

store_id_set_list = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 
'TX_3', 'WI_1', 'WI_2', 'WI_3']
end_train_day_x_list = [1913, 1941]
prediction_horizon_list = [7, 14, 21, 28]
pred_v_all_df = list()

for end_train_day_x in end_train_day_x_list:
    previous_prediction_horizon = 0
    for prediction_horizon in prediction_horizon_list:
        notebook_name = f"../input/m5-train-day-{end_train_day_x}-horizon-{prediction_horizon}"
        pred_v_df = pd.DataFrame()
        
        for store_index, store_id in enumerate(store_id_set_list):
            
            # Modeli yÃ¼kle
            model_path = str(f'{notebook_name}/lgb_model_{store_id}_{prediction_horizon}.bin')
            print(f'loading {model_path}')
            estimator = pickle.load(open(model_path, 'rb'))
            
            # Test verisini yÃ¼kle
            base_test = pd.read_feather(f"{notebook_name}/test_{store_id}_{prediction_horizon}.feather")
            enable_features = [col for col in base_test.columns if col not in ['id', 'd', 'sales']]
            
            # Her gÃ¼n iÃ§in tahmin yap
            for predict_day in range(previous_prediction_horizon + 1, prediction_horizon + 1):
                print('[{3} -> {4}] predict {0}/{1} {2} day {5}'.format(
                    store_index + 1, len(store_id_set_list), store_id,
                    previous_prediction_horizon + 1, prediction_horizon, 
predict_day))
                    
                mask = base_test['d'] == (end_train_day_x + predict_day)
                # Tahmin yap ve sonucu 'sales' sÃ¼tununa yerleÅŸtir (Ã–zyinelemeli tahmin)
                base_test.loc[mask, 'sales'] = estimator.predict(base_test[mask][enable_features])
                
            # Ä°lgili tahmin ufkundaki sonuÃ§larÄ± al
            temp_v_df = base_test[
                (base_test['d'] >= end_train_day_x + previous_prediction_horizon + 1) &
                (base_test['d'] < end_train_day_x + prediction_horizon + 1)
                ][['id', 'd', 'sales']]
                
            if len(pred_v_df)!=0:
                pred_v_df = pd.concat([pred_v_df, temp_v_df])
            else:
                pred_v_df = temp_v_df.copy()
            del(temp_v_df)
            gc.collect()
            
        previous_prediction_horizon = prediction_horizon
        pred_v_all_df.append(pred_v_df)

pred_v_all_df = pd.concat(pred_v_all_df)
```

TÃ¼m tahminler toplandÄ±ktan sonra, hem gerekli satÄ±rlar hem de sÃ¼tun formatÄ± iÃ§in (Kaggle, ilerleyen sÃ¼tunlarda gÃ¼nlÃ¼k satÄ±ÅŸlarla doÄŸrulama veya test dÃ¶nemlerindeki Ã¼rÃ¼nler iÃ§in ayrÄ± satÄ±rlar bekler) **Ã¶rnek gÃ¶nderim dosyasÄ±nÄ±** referans alarak bunlarÄ± birleÅŸtiriyoruz:

```python
submission = pd.read_csv("../input/m5-forecasting-accuracy/sample_submission.csv")

# GÃ¼n numarasÄ±nÄ± baÅŸlangÄ±Ã§ noktasÄ±na gÃ¶re ayarla (Ã–rn: 1914 -> 1)
pred_v_all_df.d = pred_v_all_df.d - end_train_day_x_list[0] # Burada 1913 kullanÄ±lÄ±rsa F1-F28, 1941 kullanÄ±lÄ±rsa F29-F56

# Uzun formattan geniÅŸ formata dÃ¶nÃ¼ÅŸtÃ¼r (pivot)
pred_h_all_df = pred_v_all_df.pivot(index='id', columns='d', values='sales')
pred_h_all_df = pred_h_all_df.reset_index()

# SÃ¼tun adlarÄ±nÄ± gÃ¶nderim formatÄ±yla eÅŸleÅŸtir
pred_h_all_df.columns = submission.columns

# Tahminleri Ã¶rnek gÃ¶nderim dosyasÄ±yla birleÅŸtir ve NaN'larÄ± 0 ile doldur
submission = submission[['id']].merge(pred_h_all_df, on=['id'], how='left').fillna(0)

submission.to_csv("m5_predictions.csv", index=False)
```

Bu Ã§Ã¶zÃ¼m, Ã¶zel liderlik tablosunda yaklaÅŸÄ±k **0.54907** puana ulaÅŸarak **12. sÄ±ra** almÄ±ÅŸtÄ±r, bu da final sÄ±ralamasÄ±nda altÄ±n madalya alanÄ±na girdiÄŸi anlamÄ±na gelir. Monsaraida'nÄ±n LightGBM parametrelerine geri dÃ¶nmek (Ã¶rneÄŸin, artÄ±rma parametresi iÃ§in `goss` yerine `gbdt` kullanmak) daha da yÃ¼ksek performanslar saÄŸlayabilir (ancak kodu yerel bir bilgisayarda veya Google Cloud Platform'da Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekir).

> ğŸ“ AlÄ±ÅŸtÄ±rma 7
> 
> 
> 
> EÄŸer yerel bir makineniz veya bulut biliÅŸim kaynaÄŸÄ±nÄ±z varsa ve 12 saati aÅŸabilecek bir eÄŸitimi bekleyebilirseniz, bir alÄ±ÅŸtÄ±rma olarak, aynÄ± iterasyon sayÄ±sÄ±yla LightGBM eÄŸitimini **`goss`** yerine **`gbdt`** olarak ayarlanmÄ±ÅŸ `boosting` parametresiyle karÅŸÄ±laÅŸtÄ±rmayÄ± deneyin. Performans ve eÄŸitim sÃ¼resi arasÄ±ndaki fark ne kadardÄ±r?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

## Ã–zet *(Summary)*

Bu ikinci bÃ¶lÃ¼mde, oldukÃ§a karmaÅŸÄ±k bir zaman serisi yarÄ±ÅŸmasÄ±nÄ± ele aldÄ±k; bu nedenle denediÄŸimiz en kolay Ã¼st Ã§Ã¶zÃ¼m bile aslÄ±nda oldukÃ§a karmaÅŸÄ±k ve oldukÃ§a fazla iÅŸleme fonksiyonu kodlamayÄ± gerektiriyor. Bu bÃ¶lÃ¼mÃ¼ tamamladÄ±ktan sonra, zaman serilerinin nasÄ±l iÅŸleneceÄŸi ve gradyan artÄ±rma (gradient boosting) kullanÄ±larak nasÄ±l tahmin edileceÄŸi konusunda daha iyi bir fikre sahip olmalÄ±sÄ±nÄ±z. Elinizde yeterli veri olduÄŸunda, bu problemde olduÄŸu gibi, geleneksel yÃ¶ntemler yerine **gradyan artÄ±rma Ã§Ã¶zÃ¼mlerini tercih etmek**; hiyerarÅŸik korelasyonlar, aralÄ±klÄ± seriler ve olaylar, fiyatlar veya piyasa koÅŸullarÄ± gibi ortak deÄŸiÅŸkenlerin (covariates) mevcudiyeti gibi karmaÅŸÄ±k sorunlar iÃ§in gÃ¼Ã§lÃ¼ Ã§Ã¶zÃ¼mler oluÅŸturmanÄ±za yardÄ±mcÄ± olacaktÄ±r.

Ä°lerleyen bÃ¶lÃ¼mlerde, gÃ¶rÃ¼ntÃ¼ler ve metinlerle ilgili daha da karmaÅŸÄ±k Kaggle yarÄ±ÅŸmalarÄ±yla mÃ¼cadele edeceksiniz. En yÃ¼ksek puan alan Ã§Ã¶zÃ¼mleri yeniden oluÅŸturarak ve iÃ§ iÅŸleyiÅŸlerini anlayarak ne kadar Ã§ok ÅŸey Ã¶ÄŸrenebileceÄŸinize ÅŸaÅŸÄ±racaksÄ±nÄ±z.

---

# BÃ¶lÃ¼m 3: GÃ¶rsel YarÄ±ÅŸma: Manyok YapraÄŸÄ± HastalÄ±ÄŸÄ± YarÄ±ÅŸmasÄ± *(Chapter 3: Vision Competition: Cassava Leaf Disease Competition)*

Bu bÃ¶lÃ¼mde, **tablo verileri alanÄ±ndan ayrÄ±lÄ±p gÃ¶rÃ¼ntÃ¼ iÅŸleme** Ã¼zerine odaklanacaÄŸÄ±z. SÄ±nÄ±flandÄ±rma yarÄ±ÅŸmalarÄ±nda baÅŸarÄ±lÄ± olmak iÃ§in gerekli adÄ±mlarÄ± gÃ¶stermek amacÄ±yla, **Manyok YapraÄŸÄ± HastalÄ±ÄŸÄ± (Cassava Leaf Disease)** yarÄ±ÅŸmasÄ±nÄ±n verilerini kullanacaÄŸÄ±z: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification](https://www.kaggle.com/competitions/cassava-leaf-disease-classification).

Bir Kaggle yarÄ±ÅŸmasÄ±na baÅŸlarken yapÄ±lmasÄ± gereken ilk ÅŸey, aÃ§Ä±klamayÄ± dÃ¼zgÃ¼nce okumaktÄ±r:

> "Afrika'daki en bÃ¼yÃ¼k ikinci karbonhidrat kaynaÄŸÄ± olan manyok, zorlu koÅŸullara dayanabildiÄŸi iÃ§in **kÃ¼Ã§Ã¼k Ã§iftÃ§iler tarafÄ±ndan yetiÅŸtirilen kilit bir gÄ±da gÃ¼venliÄŸi mahsulÃ¼dÃ¼r**. Sahra AltÄ± Afrika'daki hane halkÄ± Ã§iftliklerinin en az %80'i bu niÅŸastalÄ± kÃ¶kÃ¼ yetiÅŸtiriyor, ancak **viral hastalÄ±klar kÃ¶tÃ¼ verimin baÅŸlÄ±ca kaynaÄŸÄ±dÄ±r**. Veri biliminin yardÄ±mÄ±yla, yaygÄ±n hastalÄ±klarÄ±n tanÄ±mlanmasÄ± ve bÃ¶ylece tedavi edilebilmesi mÃ¼mkÃ¼n olabilir."

GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi, bu yarÄ±ÅŸma Ã¶nemli bir gerÃ§ek hayat sorunuyla ilgilidir:

> "Mevcut hastalÄ±k tespit yÃ¶ntemleri, Ã§iftÃ§ilerin bitkileri gÃ¶rsel olarak incelemesi ve teÅŸhis etmesi iÃ§in hÃ¼kÃ¼met tarafÄ±ndan finanse edilen ziraat uzmanlarÄ±ndan yardÄ±m istemesini gerektiriyor. Bu durum, **emek yoÄŸun, arzÄ± dÃ¼ÅŸÃ¼k ve maliyetli** olmasÄ±ndan muzdariptir. Ek bir zorluk olarak, AfrikalÄ± Ã§iftÃ§ilerin yalnÄ±zca **dÃ¼ÅŸÃ¼k bant geniÅŸliÄŸine sahip mobil kalitede kameralara** eriÅŸimi olabileceÄŸinden, Ã§iftÃ§iler iÃ§in etkili Ã§Ã¶zÃ¼mlerin Ã¶nemli kÄ±sÄ±tlamalar altÄ±nda iyi performans gÃ¶stermesi gerekir."

Bu paragraf â€“ Ã¶zellikle son cÃ¼mle â€“ beklentileri belirliyor: veriler Ã§eÅŸitli kaynaklardan geldiÄŸi iÃ§in, gÃ¶rÃ¼ntÃ¼ kalitesi ve (muhtemelen) **daÄŸÄ±lÄ±m kaymasÄ± (distribution shift)** ile ilgili bazÄ± zorluklarla karÅŸÄ±laÅŸmamÄ±z muhtemeldir.

> "GÃ¶reviniz, her manyok gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ **dÃ¶rt hastalÄ±k kategorisinden birine veya saÄŸlÄ±klÄ± bir yapraÄŸÄ±** gÃ¶steren beÅŸinci bir kategoriye gÃ¶re sÄ±nÄ±flandÄ±rmaktÄ±r. Sizin yardÄ±mÄ±nÄ±zla Ã§iftÃ§iler hastalÄ±klÄ± bitkileri hÄ±zla tespit edebilir, potansiyel olarak telafisi mÃ¼mkÃ¼n olmayan hasara yol aÃ§madan mahsullerini kurtarabilirler."

Bu kÄ±sÄ±m oldukÃ§a Ã¶nemlidir: bunun bir **sÄ±nÄ±flandÄ±rma yarÄ±ÅŸmasÄ±** olduÄŸunu ve sÄ±nÄ±f sayÄ±sÄ±nÄ±n az olduÄŸunu (bu durumda 5) belirtir.

Bu bÃ¶lÃ¼mde ÅŸunlarÄ± Ã¶ÄŸreneceksiniz:

* YarÄ±ÅŸma verileri ve metrikleri
* Bir **temel modelin (baseline model)** nasÄ±l oluÅŸturulacaÄŸÄ±
* En iyi Ã§Ã¶zÃ¼mlerden elde edilen iÃ§gÃ¶rÃ¼ler

GiriÅŸ aÅŸamasÄ±nÄ± tamamladÄ±ÄŸÄ±mÄ±za gÃ¶re, ÅŸimdi verilere bir gÃ¶z atalÄ±m.

> Bu bÃ¶lÃ¼mÃ¼n kod dosyalarÄ± [https://packt.link/kwbchp3](https://packt.link/kwbchp3) adresinde bulunabilir.

## Veriyi ve metrikleri anlama *(Understanding the data and metrics)*

Bu yarÄ±ÅŸmanÄ±n **Veri (Data)** sekmesine girdiÄŸimizde, saÄŸlanan verilerin Ã¶zetini gÃ¶rÃ¼yoruz:

![](im/1007.png)

Bundan ne anlayabiliriz?

* Veriler oldukÃ§a **basit bir formattadÄ±r** ve organizatÃ¶rler hastalÄ±k adlarÄ± ile sayÄ±sal kodlar arasÄ±ndaki **eÅŸleÅŸtirmeyi** bile saÄŸlamÄ±ÅŸtÄ±r.
* Verilere **TFRecord** formatÄ±nda sahibiz, bu da bir **TPU** kullanmak isteyen herkes iÃ§in iyi bir haberdir.
* SaÄŸlanan test seti, deÄŸerlendirme iÃ§in kullanÄ±lan asÄ±l test setinin yalnÄ±zca kÃ¼Ã§Ã¼k bir alt kÃ¼mesidir ve gÃ¶nderim deÄŸerlendirme zamanÄ±nda ilki ikincisiyle deÄŸiÅŸtirilir. Bu, deÄŸerlendirme sÄ±rasÄ±nda **Ã¶nceden eÄŸitilmiÅŸ bir modelin yÃ¼klenmesi ve Ã§Ä±karÄ±m (inference) iÃ§in kullanÄ±lmasÄ±** stratejisinin tercih edildiÄŸini gÃ¶sterir.

**ğŸ¯ DeÄŸerlendirme MetriÄŸi ve Zorluklar**

**Kategorizasyon doÄŸruluÄŸu (accuracy)** ([https://developers.google.com/machine-learning/crash-course/classification/accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)) deÄŸerlendirme metriÄŸi olarak seÃ§ilmiÅŸtir:

$$\text{doÄŸruluk} = \frac{\text{doÄŸru tahmin sayÄ±sÄ±}}{\text{toplam tahmin sayÄ±sÄ±}}$$

Bu metrik, girdi olarak **ayrÄ±k (discrete) deÄŸerler** alÄ±r, bu da potansiyel **topluluk oluÅŸturma (ensembling) stratejilerinin** biraz daha karmaÅŸÄ±k hale geldiÄŸi anlamÄ±na gelir. Topluluk oluÅŸturma, daha doÄŸru bir tahmin oluÅŸturmak iÃ§in birden fazla modelin tahminlerini birleÅŸtirme sÃ¼recidir.

SÄ±nÄ±flandÄ±rma etiketleri gibi ayrÄ±k deÄŸerlere sahip metrikleri topluluk oluÅŸturma, gerÃ§ek deÄŸerli sayÄ±lar gibi sÃ¼rekli (continuous) deÄŸerlere sahip metrikleri topluluk oluÅŸturmaktan daha zor olabilir. Bunun nedeni, birleÅŸtirilen modellerin Ã§Ä±ktÄ±sÄ±nÄ±n birleÅŸtirilebilmesi iÃ§in aynÄ± tipte olmasÄ± gerektiÄŸidir ve ayrÄ±k deÄŸerleri birleÅŸtirmek, sÃ¼rekli deÄŸerleri birleÅŸtirmekten daha zordur.

Ã–rneÄŸin, sÄ±nÄ±flandÄ±rma modellerini topluluk oluÅŸtururken, bireysel modellerin Ã§Ä±ktÄ±larÄ±, sÄ±nÄ±flandÄ±rma etiketlerinin ayrÄ±k doÄŸasÄ±nÄ± koruyacak ÅŸekilde birleÅŸtirilmelidir. Bu zor olabilir, Ã§Ã¼nkÃ¼ birden fazla modelin Ã§Ä±ktÄ±larÄ±nÄ±n hem doÄŸru hem de sÄ±nÄ±flandÄ±rma etiketlerinin ayrÄ±k doÄŸasÄ±nÄ± koruyacak ÅŸekilde nasÄ±l birleÅŸtirileceÄŸi her zaman net deÄŸildir. Buna karÅŸÄ±lÄ±k, regresyon modellerini topluluk oluÅŸtururken, bireysel modellerin Ã§Ä±ktÄ±larÄ± basitÃ§e tahminlerin **ortalamasÄ± alÄ±narak** birleÅŸtirilebilir, bu da Ã§Ä±ktÄ±nÄ±n sÃ¼rekli doÄŸasÄ±nÄ± koruyan basit bir iÅŸlemdir.

**ğŸ“‰ KayÄ±p Fonksiyonu ve Metrik FarkÄ±**

**KayÄ±p fonksiyonu (loss function)**, eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenme fonksiyonunu optimize etmek iÃ§in uygulanÄ±r ve **gradyan iniÅŸ (gradient descent)** tabanlÄ± yÃ¶ntemleri kullanmak istediÄŸimiz sÃ¼rece, bunun **sÃ¼rekli** olmasÄ± gerekir. Ã–te yandan, **deÄŸerlendirme metriÄŸi** eÄŸitimden sonra genel performansÄ± Ã¶lÃ§mek iÃ§in kullanÄ±lÄ±r ve bu nedenle **ayrÄ±k** olabilir.

### ğŸ“‰ KayÄ±p Fonksiyonu ve Metrik FarkÄ±

**KayÄ±p fonksiyonu (loss function)**, eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenme fonksiyonunu optimize etmek iÃ§in uygulanÄ±r ve **gradyan iniÅŸ (gradient descent)** tabanlÄ± yÃ¶ntemleri kullanmak istediÄŸimiz sÃ¼rece, bunun **sÃ¼rekli** olmasÄ± gerekir. Ã–te yandan, **deÄŸerlendirme metriÄŸi** eÄŸitimden sonra genel performansÄ± Ã¶lÃ§mek iÃ§in kullanÄ±lÄ±r ve bu nedenle **ayrÄ±k** olabilir.

> ğŸ“ AlÄ±ÅŸtÄ±rma 1
> 
> 
> 
> Bir model oluÅŸturmadan, **temel bir KeÅŸifÃ§i Veri Analizi (EDA)** yapmak iÃ§in bir kod yazÄ±n.
> 
> 
> 
> SÄ±nÄ±flandÄ±rma problemimizdeki sÄ±nÄ±flarÄ±n **kardinalitesini** (sayÄ±sal daÄŸÄ±lÄ±mÄ±nÄ±) karÅŸÄ±laÅŸtÄ±rÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Normalde, bu aynÄ± zamanda **daÄŸÄ±lÄ±m kaymasÄ±nÄ± (distribution shift)** kontrol etme anÄ± olacaktÄ±r: bu, bir makine Ã¶ÄŸrenimi modeli iÃ§in eÄŸitim ve test verilerinin altÄ±nda yatan daÄŸÄ±lÄ±mÄ±ndaki farktÄ±r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ± baÄŸlamÄ±nda, bu durum Ã§eÅŸitli ÅŸekillerde ortaya Ã§Ä±kabilir: eÄŸitim verileri belirli bir tÃ¼rde veya belirli bir konumdan alÄ±nan gÃ¶rÃ¼ntÃ¼ler iÃ§erebilirken, test verileri bir ÅŸekilde farklÄ± olan gÃ¶rÃ¼ntÃ¼ler iÃ§erebilir. Bu, modelin eÄŸitim verilerinde iyi performans gÃ¶sterdiÄŸi, ancak test verilerinde kÃ¶tÃ¼ performans gÃ¶sterdiÄŸi bir duruma yol aÃ§abilir, Ã§Ã¼nkÃ¼ model test setinde bulunan gÃ¶rÃ¼ntÃ¼ tÃ¼rlerinin Ã¶rneklerini gÃ¶rmemiÅŸtir.

Bu sorunu Ã§Ã¶zmek iÃ§in, eÄŸitim ve test verilerinin, model daÄŸÄ±tÄ±ldÄ±ÄŸÄ±nda karÅŸÄ±laÅŸacaÄŸÄ± verilerin gerÃ§ek dÃ¼nya daÄŸÄ±lÄ±mÄ±nÄ± temsil ettiÄŸinden emin olmak Ã¶nemlidir. Uygulamada **kavram kaymasÄ±nÄ±n (concept drift)** etkisini azaltmaya yÃ¶nelik Ã¼Ã§ ana yaklaÅŸÄ±m vardÄ±r:

* **DÃ¼zenlileÅŸtirme (Regularization)**: Verilerdeki daÄŸÄ±lÄ±msal deÄŸiÅŸikliklerin etkisini azaltmaya yardÄ±mcÄ± olur.
* **DaÄŸÄ±lÄ±mdaki deÄŸiÅŸiklikleri taklit etmek iÃ§in tasarlanmÄ±ÅŸ artÄ±rmalar (Augmentations)**: Ã–rneÄŸin, test verilerinin daha dÃ¼ÅŸÃ¼k kaliteli kameralarla Ã§ekilmiÅŸ gÃ¶rÃ¼ntÃ¼ler iÃ§erebileceÄŸinden ÅŸÃ¼pheleniyorsak, orijinal fotoÄŸraflara gÃ¼rÃ¼ltÃ¼ ekleyebiliriz.
* **Ã‡ekiÅŸmeli doÄŸrulama (Adversarial validation)**: Veri kÃ¼meleri arasÄ±ndaki kavram kaymasÄ±nÄ± tespit etmek iÃ§in popÃ¼ler bir teknik olan Ã§ekiÅŸmeli doÄŸrulama tartÄ±ÅŸmasÄ± iÃ§in lÃ¼tfen The Kaggle Book'un 6. BÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z.

Ancak, bu durumda veri setinin tamamÄ±na eriÅŸimimiz olmadÄ±ÄŸÄ± iÃ§in, bu adÄ±m atlanmÄ±ÅŸ ve esas olarak aÃ§Ä±klamanÄ±n eksiksizliÄŸi iÃ§in bahsedilmiÅŸtir.

## Bir temel model (baseline model) oluÅŸturma *(Building a baseline model)*

YaklaÅŸÄ±mÄ±mÄ±za bir **temel Ã§Ã¶zÃ¼m (baseline solution)** oluÅŸturarak baÅŸlÄ±yoruz. UÃ§tan uca Ã§Ã¶zÃ¼mÃ¼ Ã§alÄ±ÅŸtÄ±ran not defteri ÅŸurada mevcuttur: [https://www.kaggle.com/code/konradb/ch3-end-to-end-image-classification](https://www.google.com/search?q=https://www.kaggle.com/code/konradb/ch3-end-to-end-image-classification).

DiÄŸer yarÄ±ÅŸmalar iÃ§in bir baÅŸlangÄ±Ã§ noktasÄ± olarak faydalÄ± olsa da, bu bÃ¶lÃ¼mde aÃ§Ä±klanan akÄ±ÅŸÄ± takip etmek, yani kodu hÃ¼cre hÃ¼cre kopyalamak daha Ã¶ÄŸreticidir, bÃ¶ylece daha iyi anlayabilir (ve tabii ki geliÅŸtirebilirsiniz â€“ sonuÃ§ta buna bir nedeni var: temel Ã§Ã¶zÃ¼m deniyor):

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam
# ignoring warnings
import warnings
warnings.simplefilter("ignore")
import os, cv2, json
from PIL import Image
```

Gerekli paketleri iÃ§e aktararak baÅŸlÄ±yoruz â€“ tarzda kiÅŸisel farklÄ±lÄ±klar doÄŸal olsa da, bizim gÃ¶rÃ¼ÅŸÃ¼mÃ¼z, iÃ§e aktarmalarÄ± tek bir yerde toplamanÄ±n, yarÄ±ÅŸma ilerledikÃ§e ve daha ayrÄ±ntÄ±lÄ± Ã§Ã¶zÃ¼mlere doÄŸru ilerledikÃ§e kodun bakÄ±mÄ±nÄ± kolaylaÅŸtÄ±rdÄ±ÄŸÄ± yÃ¶nÃ¼ndedir.

AyrÄ±ca, Ã¶ÄŸrenme sÃ¼recimizi karakterize eden tÃ¼m parametreler iÃ§in bir yer tutucu olan bir **yapÄ±landÄ±rma sÄ±nÄ±fÄ± (configuration class)** oluÅŸturuyoruz:

```python
class CFG:    
    # config
    WORK_DIR = '../input/cassava-leaf-disease-classification'
    BATCH_SIZE = 8
    EPOCHS = 5
    TARGET_SIZE = 256
    NCLASSES = 5
```

BileÅŸenler ÅŸunlarÄ± iÃ§erir:

  * **`WORK_DIR`** (Veri KlasÃ¶rÃ¼): Ã‡oÄŸunlukla modelleri bazen Kaggle dÄ±ÅŸÄ±nda (Ã¶rneÄŸin Google Colab'de veya yerel makinenizde) eÄŸitiyorsanÄ±z kullanÄ±ÅŸlÄ±dÄ±r.
  * **`BATCH_SIZE`**: EÄŸitim sÃ¼recinizi optimize etmek (veya kÄ±sÄ±tlÄ± bellek ortamÄ±nda bÃ¼yÃ¼k gÃ¶rÃ¼ntÃ¼ler iÃ§in mÃ¼mkÃ¼n kÄ±lmak) istiyorsanÄ±z bazen ayarlanmasÄ± gereken bir parametredir.
  * **`EPOCHS`**: Hata ayÄ±klama (debugging) iÃ§in faydalÄ±dÄ±r: Ã§Ã¶zÃ¼mÃ¼nÃ¼zÃ¼n baÅŸtan sona sorunsuz Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrulamak iÃ§in az sayÄ±da epoch ile baÅŸlayÄ±n ve uygun bir Ã§Ã¶zÃ¼me doÄŸru ilerledikÃ§e artÄ±rÄ±n.
  * **`TARGET_SIZE`**: GÃ¶rÃ¼ntÃ¼lerinizi yeniden boyutlandÄ±rmak istediÄŸiniz boyutu tanÄ±mlar.
  * **`NCLASSES`**: SÄ±nÄ±flandÄ±rma probleminizdeki olasÄ± sÄ±nÄ±f sayÄ±sÄ±na karÅŸÄ±lÄ±k gelir.

Bir Ã§Ã¶zÃ¼m kodlamak iÃ§in iyi bir uygulama, Ã¶nemli parÃ§alarÄ± fonksiyonlar iÃ§ine kapsÃ¼llemektir â€“ ve eÄŸitilebilir modelimizi oluÅŸturmak kesinlikle Ã¶nemli olarak nitelendirilir:

```python
def create_model():
    conv_base = EfficientNetB0(include_top = False, weights = None,
                               input_shape = (CFG.TARGET_SIZE, CFG.TARGET_SIZE, 3))
    model = conv_base.output
    model = layers.GlobalAveragePooling2D()(model)
    model = layers.Dense(CFG.NCLASSES, activation = "softmax")(model)
    model = models.Model(conv_base.input, model)
    model.compile(optimizer = Adam(lr = 0.001),
                  loss = "sparse_categorical_crossentropy",
                  metrics = ["acc"])
    return model
```

Bu adÄ±mla ilgili birkaÃ§ not:

* Daha etkileyici seÃ§enekler mevcut olsa da, **hÄ±zla Ã¼zerinde tekrar yapÄ±labilen (iterated upon)** hÄ±zlÄ± bir modelle baÅŸlamak pratiktir. **EfficientNet** ([https://paperswithcode.com/method/efficientnet](https://paperswithcode.com/method/efficientnet)) mimarisi bu gereksinime oldukÃ§a iyi uyar.
* **DÃ¼zenlileÅŸtirme (regularization)** amaÃ§larÄ± iÃ§in bir **havuzlama katmanÄ± (pooling layer)** ekliyoruz.
* Bir **sÄ±nÄ±flandÄ±rma baÅŸlÄ±ÄŸÄ± (classification head)** ekleyin â€“ sÄ±nÄ±flandÄ±rÄ±cÄ± iÃ§in olasÄ± sonuÃ§ sayÄ±sÄ±nÄ± belirten `CFG.NCLASSES` ile bir **Dense** katmanÄ±.
* Son olarak, bu yarÄ±ÅŸmanÄ±n gereksinimlerine karÅŸÄ±lÄ±k gelen **kayÄ±p (loss)** ve **metrikler** ile modeli derliyoruz.

> ğŸ“ AlÄ±ÅŸtÄ±rma 2
> 
> 
> 
> KayÄ±p (loss) ve metrik iÃ§in olasÄ± seÃ§imleri inceleyin â€“ faydalÄ± bir rehber ÅŸurada bulunmaktadÄ±r: [https://keras.io/api/losses/](https://keras.io/api/losses/).
> 
> 
> 
> DiÄŸer makul seÃ§enekler neler olurdu?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

SÄ±radaki adÄ±m **veri** aÅŸamasÄ±dÄ±r:

```python
train_labels = pd.read_csv(os.path.join(CFG.WORK_DIR, "train.csv"))
STEPS_PER_EPOCH = len(train_labels)*0.8 / CFG.BATCH_SIZE
VALIDATION_STEPS = len(train_labels)*0.2 / CFG.BATCH_SIZE

# SÄ±nÄ±f etiketini string (dize) tÃ¼rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼r
train_labels.label = train_labels.label.astype('str')

# EÄŸitim iÃ§in veri artÄ±rma (Data Augmentation) tanÄ±mla
train_datagen = ImageDataGenerator(validation_split = 0.2, preprocessing_function = None,
                                     rotation_range = 45, zoom_range = 0.2,
                                     horizontal_flip = True, vertical_flip = True,
                                     fill_mode = 'nearest', shear_range = 0.1,
                                     height_shift_range = 0.1, width_shift_range = 0.1)
                                     
# EÄŸitim veri Ã¼reteci (generator)
train_generator = train_datagen.flow_from_dataframe(train_labels, 
                         directory = os.path.join(CFG.WORK_DIR, "train_images"),
                         subset = "training", x_col = "image_id",
                         y_col = "label", target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),
                         batch_size = CFG.BATCH_SIZE, class_mode = "sparse")

# DoÄŸrulama iÃ§in veri Ã¼reteci tanÄ±mla (ArtÄ±rma olmadan)
validation_datagen = ImageDataGenerator(validation_split = 0.2)
validation_generator = validation_datagen.flow_from_dataframe(train_labels,
                         directory = os.path.join(CFG.WORK_DIR, "train_images"),
                         subset = "validation", x_col = "image_id",
                         y_col = "label", target_size = (CFG.TARGET_SIZE, CFG.TARGET_SIZE),
                         batch_size = CFG.BATCH_SIZE, class_mode = "sparse")
```

ArdÄ±ndan, modeli kuruyoruz â€“ yukarÄ±da tanÄ±mladÄ±ÄŸÄ±mÄ±z fonksiyon sayesinde bu oldukÃ§a basittir:

```python
model = create_model()
model.summary()
```

**ğŸ§  Callbacks (Geri Ã‡aÄŸÄ±rmalar) Kurulumu**

Modele eÄŸitim vermeye geÃ§meden Ã¶nce, **geri Ã§aÄŸÄ±rmalara (callbacks)** biraz dikkat ayÄ±rmalÄ±yÄ±z:

```python
model_save = ModelCheckpoint('./EffNetB0_512_8_best_weights.h5', 
                             save_best_only = True, 
                             save_weights_only = True,
                             monitor = 'val_loss', 
                             mode = 'min', verbose = 1)

early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, 
                           patience = 5, mode = 'min', verbose = 1,
                           restore_best_weights = True)
                           
reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, 
                              patience = 2, min_delta = 0.001, 
                              mode = 'min', verbose = 1)
```

Bahsetmeye deÄŸer bazÄ± noktalar:

* **`ModelCheckpoint`**: YalnÄ±zca en iyi modelin aÄŸÄ±rlÄ±klarÄ±nÄ± tuttuÄŸumuzdan emin olmak iÃ§in kullanÄ±lÄ±r; bu durumda en iyi olma durumu izlenecek metrik (bu Ã¶rnekte doÄŸrulama kaybÄ± - **`val_loss`**) tarafÄ±ndan belirlenir.

* **`EarlyStopping`** (Erken Durdurma): Keras'ta, bir derin Ã¶ÄŸrenme modelinin doÄŸrulama kÃ¼mesindeki performansÄ± iyileÅŸmeyi bÄ±raktÄ±ÄŸÄ±nda eÄŸitimi otomatik olarak durdurmak iÃ§in kullanÄ±lan bir yÃ¶ntemdir. Bu, eÄŸitim sÃ¼recinin modelin eÄŸitim verilerine aÅŸÄ±rÄ± uyum saÄŸlamasÄ±ndan (overfitting) Ã¶nce sonlandÄ±rÄ±lmasÄ±na olanak tanÄ±dÄ±ÄŸÄ± iÃ§in faydalÄ± olabilir, aksi takdirde yeni, gÃ¶rÃ¼lmemiÅŸ verilerde dÃ¼ÅŸÃ¼k performansa yol aÃ§abilir. `EarlyStopping` geri Ã§aÄŸÄ±rmasÄ±, genellikle bir derin Ã¶ÄŸrenme modelinin performansÄ±nÄ± artÄ±rmak iÃ§in dÃ¼zenlileÅŸtirme ve Ã¶ÄŸrenme oranÄ± Ã§izelgeleri gibi diÄŸer tekniklerle birlikte kullanÄ±lÄ±r.

* **`ReduceLROnPlateau`** (Platoda Ã–ÄŸrenme OranÄ±nÄ± Azaltma): Keras'ta, modelin doÄŸrulama kÃ¼mesindeki performansÄ± **platoya ulaÅŸtÄ±ÄŸÄ±nda** Ã¶ÄŸrenme oranÄ±nÄ± azaltan bir Ã¶ÄŸrenme oranÄ± Ã§izelgesidir. Bu, modelin performansÄ±nÄ±n belirli bir epoch sayÄ±sÄ±ndan sonra iyileÅŸmeyi durdurduÄŸu anlamÄ±na gelir. Bu olduÄŸunda, `ReduceLROnPlateau` ÅŸemasÄ±, Ã¶ÄŸrenme oranÄ±nÄ± belirtilen bir faktÃ¶r kadar azaltÄ±r, modelin Ã¶ÄŸrenmeye devam etmesine ancak daha yavaÅŸ bir tempoda izin verir. Bu, modelin aÅŸÄ±rÄ± uyum saÄŸlamasÄ±nÄ± Ã¶nlemeye yardÄ±mcÄ± olabilir ve ayrÄ±ca modelin bir Ã§Ã¶zÃ¼me daha hÄ±zlÄ± yakÄ±nsamasÄ±na da yardÄ±mcÄ± olabilir.

> ğŸ“ AlÄ±ÅŸtÄ±rma 3
> 
> 
> 
> YukarÄ±daki kurulumda hangi parametreleri deÄŸiÅŸtirmek **mantÄ±klÄ±** olur ve hangileri varsayÄ±lan deÄŸerlerinde bÄ±rakÄ±labilir?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Hiperparametre ayarlamasÄ± (hyperparameter tuning) iÃ§in TensorFlow ile mÃ¼kemmel bir baÅŸlangÄ±Ã§ noktasÄ± resmi dokÃ¼mantasyonda bulunabilir:

[https://www.tensorflow.org/tutorials/keras/keras\_tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)

Bu kurulumla, modeli eÄŸitebiliriz:

```python
history = model.fit(
    train_generator,
    steps_per_epoch = STEPS_PER_EPOCH,
    epochs = CFG.EPOCHS,
    validation_data = validation_generator,
    validation_steps = VALIDATION_STEPS,
    callbacks = [model_save, early_stop, reduce_lr]
)
```

EÄŸitim tamamlandÄ±ktan sonra, test setindeki her bir gÃ¶rÃ¼ntÃ¼ iÃ§in gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±fÄ± tahminini oluÅŸturmak Ã¼zere modeli kullanabiliriz. HatÄ±rlayÄ±n ki, bu yarÄ±ÅŸmada, herkese aÃ§Ä±k (gÃ¶rÃ¼nÃ¼r) test seti tek bir gÃ¶rÃ¼ntÃ¼den oluÅŸuyordu ve tam olanÄ±n boyutu bilinmiyordu â€“ bu nedenle gÃ¶nderim DataFrame'ini oluÅŸturmak iÃ§in biraz dolambaÃ§lÄ± bir yÃ¶nteme ihtiyaÃ§ duyuluyordu:

```python
submit_df = pd.read_csv(os.path.join(CFG.WORK_DIR, "sample_submission.csv"))
preds = []

# GÃ¶nderim dosyasÄ±ndaki her bir image_id iÃ§in dÃ¶ngÃ¼
for image_id in submit_df.image_id:
    # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle ve yeniden boyutlandÄ±r
    image = Image.open(os.path.join(CFG.WORK_DIR, "test_images", image_id))
    image = image.resize((CFG.TARGET_SIZE, CFG.TARGET_SIZE))
    
    # Modele beslemek iÃ§in boyutu geniÅŸlet (batch boyutu ekle)
    image = np.expand_dims(image, axis = 0)
    
    # Tahmin yap ve en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±f indeksini kaydet
    preds.append(np.argmax(model.predict(image)))

submit_df ['label'] = preds
submit_df.to_csv('submission.csv', index = False)
```

Bu bÃ¶lÃ¼mde, gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ±na odaklanan bir yarÄ±ÅŸmada nasÄ±l rekabet etmeye baÅŸlanacaÄŸÄ±nÄ± gÃ¶sterdik â€“ bu yaklaÅŸÄ±mÄ±, temel KeÅŸifÃ§i Veri Analizinden (EDA) iÅŸlevsel bir gÃ¶nderime hÄ±zlÄ±ca geÃ§mek iÃ§in kullanabilirsiniz. Ancak, bunun gibi basit bir yaklaÅŸÄ±mÄ±n Ã§ok rekabetÃ§i sonuÃ§lar Ã¼retmesi olasÄ± deÄŸildir.

Bu nedenle, bir sonraki bÃ¶lÃ¼mde, en yÃ¼ksek puan alan Ã§Ã¶zÃ¼mlerde kullanÄ±lan daha Ã¶zelleÅŸmiÅŸ teknikleri tartÄ±ÅŸÄ±yoruz.

## En iyi Ã§Ã¶zÃ¼mlerden ders alma *(Learning from top solutions)*

Bu bÃ¶lÃ¼mde, temel Ã§Ã¶zÃ¼m seviyesinin Ã¼zerine Ã§Ä±kmamÄ±zÄ± saÄŸlayacak **en iyi Ã§Ã¶zÃ¼mlerin** yÃ¶nlerini bir araya getiriyoruz. Bu yarÄ±ÅŸmadaki liderlik tablolarÄ±nÄ±n (hem herkese aÃ§Ä±k hem de Ã¶zel) oldukÃ§a yakÄ±n olduÄŸunu unutmayÄ±n; bu durum, birkaÃ§ faktÃ¶rÃ¼n birleÅŸiminden kaynaklanÄ±yordu:

* **GÃ¼rÃ¼ltÃ¼lÃ¼ veri (Noisy data)**: EÄŸitim verisinin bÃ¼yÃ¼k bir kÄ±smÄ±nÄ± doÄŸru bir ÅŸekilde tanÄ±mlayarak %0.89 doÄŸruluÄŸa ulaÅŸmak kolaydÄ± ve ardÄ±ndan gelen her yeni doÄŸru tahmin, yalnÄ±zca kÃ¼Ã§Ã¼k bir yukarÄ± hareket saÄŸlÄ±yordu.
* **Veri boyutunun sÄ±nÄ±rlÄ± olmasÄ±**.

### Ã–n eÄŸitim *(Pretraining)*

**ğŸ“ˆ SÄ±nÄ±rlÄ± Veri Boyutuna Ã‡Ã¶zÃ¼m: Ã–n EÄŸitim**

SÄ±nÄ±rlÄ± veri boyutu sorununa ilk ve en belirgin Ã§Ã¶zÃ¼m, daha fazla veri kullanarak **Ã¶n eÄŸitim (pretraining)** yapmaktÄ±. Bir derin Ã¶ÄŸrenme modelini daha fazla veri Ã¼zerinde Ã¶n eÄŸitmek  faydalÄ± olabilir, Ã§Ã¼nkÃ¼ bu, modelin verinin **daha iyi temsillerini** Ã¶ÄŸrenmesine yardÄ±mcÄ± olabilir, bu da modelin sonraki gÃ¶revlerdeki performansÄ±nÄ± artÄ±rabilir. Bir derin Ã¶ÄŸrenme modeli bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde eÄŸitildiÄŸinde, o gÃ¶revle ilgili **faydalÄ± Ã¶zellikleri** veriden Ã§Ä±karmayÄ± Ã¶ÄŸrenebilir. Bu, model iÃ§in gÃ¼Ã§lÃ¼ bir temel saÄŸlayarak, daha kÃ¼Ã§Ã¼k ve spesifik bir veri kÃ¼mesi Ã¼zerinde **ince ayar (fine-tuning)** yapÄ±ldÄ±ÄŸÄ±nda daha etkili Ã¶ÄŸrenmesine olanak tanÄ±r.

Ek olarak, bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde Ã¶n eÄŸitim, modelin yeni, gÃ¶rÃ¼lmemiÅŸ verilere **daha iyi genelleme** yapmasÄ±na yardÄ±mcÄ± olabilir. Model, Ã¶n eÄŸitim sÄ±rasÄ±nda geniÅŸ bir Ã¶rnek yelpazesi gÃ¶rdÃ¼ÄŸÃ¼ iÃ§in, bazÄ± aÃ§Ä±lardan eÄŸitim verilerinden farklÄ± olabilecek yeni verilere daha iyi uyum saÄŸlayabilir. Bu, Ã¶zellikle Ã§ok sayÄ±da parametreye sahip olabilen ve sÄ±fÄ±rdan etkili bir ÅŸekilde eÄŸitilmesi zor olabilen derin Ã¶ÄŸrenme modelleriyle Ã§alÄ±ÅŸÄ±rken Ã¶nemli olabilir.

**ğŸ“… Ã–nceki YarÄ±ÅŸma Verilerinden Yararlanma**

Manyok yarÄ±ÅŸmasÄ± bir yÄ±l Ã¶nce de dÃ¼zenlenmiÅŸti: [https://www.kaggle.com/competitions/cassava-disease/overview](https://www.kaggle.com/competitions/cassava-disease/overview).

Minimum ayarlamalarla, **2019 sÃ¼rÃ¼mÃ¼nden** elde edilen veriler mevcut yarÄ±ÅŸma baÄŸlamÄ±nda kullanÄ±labilirdi. BirkaÃ§ yarÄ±ÅŸmacÄ± bu konuya deÄŸinmiÅŸtir:

* **TFRecords** formatÄ±nda birleÅŸtirilmiÅŸ bir **2019 + 2020 veri kÃ¼mesi** Kaggle forumunda yayÄ±nlandÄ±: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131)
* 2019 sÃ¼rÃ¼mÃ¼nÃ¼n **kazanan Ã§Ã¶zÃ¼mÃ¼** yararlÄ± bir baÅŸlangÄ±Ã§ noktasÄ± olarak hizmet etti: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985)
* 2019 verileri Ã¼zerinde tahminler oluÅŸturmak ve veri kÃ¼mesini bÃ¼yÃ¼tmek iÃ§in **sÃ¶zde etiketler (pseudo-labels)** kullanmanÄ±n bazÄ± (kÃ¼Ã§Ã¼k) iyileÅŸtirmeler saÄŸladÄ±ÄŸÄ± bildirildi: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594)

### Test zamanÄ± artÄ±rÄ±mÄ± *(Test time augmentation)*

**Test ZamanÄ± ArtÄ±rÄ±mÄ± (TTA)** arkasÄ±ndaki fikir, test gÃ¶rÃ¼ntÃ¼sÃ¼ne farklÄ± dÃ¶nÃ¼ÅŸÃ¼mler uygulamaktÄ±r: dÃ¶ndÃ¼rmeler, Ã§evirmeler ve Ã¶telemeler. Bu, test gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n birkaÃ§ farklÄ± versiyonunu oluÅŸturur ve bunlarÄ±n her biri iÃ§in bir tahmin Ã¼retiriz. Ortaya Ã§Ä±kan sÄ±nÄ±f olasÄ±lÄ±klarÄ±, daha gÃ¼venilir bir yanÄ±t elde etmek iÃ§in daha sonra **ortalamasÄ± alÄ±nÄ±r**.  Bu tekniÄŸin mÃ¼kemmel bir gÃ¶sterimi Andrew Khael'in bir not defterinde verilmiÅŸtir: [https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it](https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it).

TTA, Manyok yarÄ±ÅŸmasÄ±ndaki en iyi Ã§Ã¶zÃ¼mler tarafÄ±ndan yaygÄ±n olarak kullanÄ±lmÄ±ÅŸtÄ±r. En iyi Ã¼Ã§ Ã¶zel liderlik tablosu sonucu buna mÃ¼kemmel bir Ã¶rnektir: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150).

### DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ler *(Transformers)*

YarÄ±ÅŸma boyunca **ResNeXt** ve **EfficientNet** gibi daha yaygÄ±n bilinen mimariler Ã§okÃ§a kullanÄ±lmÄ±ÅŸ olsa da, sÄ±kÄ±ÅŸÄ±k bir liderlik tablosunda ilerleme arayÄ±ÅŸÄ±ndaki birÃ§ok yarÄ±ÅŸmacÄ±ya ekstra avantaj saÄŸlayan ÅŸey, daha yeni olanlarÄ±n eklenmesiydi.

**Transformer'lar**, 2017'de DoÄŸal Dil Ä°ÅŸleme (NLP) iÃ§in devrim niteliÄŸinde bir mimari olarak ortaya Ã§Ä±ktÄ± (eÄŸer bir ÅŸekilde her ÅŸeyi baÅŸlatan makaleyi kaÃ§Ä±rdÄ±ysanÄ±z, iÅŸte burada: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)) ve Ã¶yle muazzam bir baÅŸarÄ± elde ettiler ki, kaÃ§Ä±nÄ±lmaz olarak birÃ§ok kiÅŸi bunlarÄ±n baÅŸka modalitelere de uygulanÄ±p uygulanamayacaÄŸÄ±nÄ± merak etmeye baÅŸladÄ± â€“ **gÃ¶rÃ¼ntÃ¼ (vision)** bariz bir adaydÄ±.

Uygun bir ÅŸekilde adlandÄ±rÄ±lan **Vision Transformer (ViT)**, ilk ortaya Ã§Ä±kÄ±ÅŸlarÄ±ndan birini Manyok yarÄ±ÅŸmasÄ±nda bir Kaggle yarÄ±ÅŸmasÄ±nda yaptÄ±. 

ViT iÃ§in mÃ¼kemmel bir eÄŸitim (tutorial) kamuya aÃ§Ä±klandÄ±: [https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline](https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline).

### BirleÅŸtirme *(Ensembling)*

**ğŸ¤ Topluluk OluÅŸturma (Ensembling)**

Topluluk oluÅŸturma (Ensembling), Kaggle'da Ã§ok popÃ¼lerdir (daha ayrÄ±ntÄ±lÄ± bir aÃ§Ä±klama iÃ§in The Kaggle Book'un 9. BÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z) ve Manyok yarÄ±ÅŸmasÄ± da bir istisna deÄŸildi. AnlaÅŸÄ±ldÄ±ÄŸÄ± Ã¼zere, **Ã§eÅŸitli mimarileri** birleÅŸtirmek (sÄ±nÄ±f olasÄ±lÄ±klarÄ±nÄ±n ortalamasÄ±nÄ± alarak) Ã§ok faydalÄ± oldu: **EfficientNet, ResNext ve ViT** birbirinden yeterince farklÄ±dÄ±r, bu nedenle tahminleri birbirini tamamlar.

Bir makine Ã¶ÄŸrenimi topluluÄŸu oluÅŸtururken, birbirinden farklÄ± modelleri birleÅŸtirmek faydalÄ±dÄ±r, Ã§Ã¼nkÃ¼ bu, topluluÄŸun genel performansÄ±nÄ± iyileÅŸtirmeye yardÄ±mcÄ± olabilir.

Topluluk oluÅŸturma, daha doÄŸru bir tahmin oluÅŸturmak iÃ§in birden fazla modelin tahminlerini birleÅŸtirme sÃ¼recidir. FarklÄ± gÃ¼Ã§lÃ¼ ve zayÄ±f yÃ¶nlere sahip modelleri birleÅŸtirerek, topluluk daha doÄŸru tahminler yapmak iÃ§in her bir bireysel modelin gÃ¼Ã§lÃ¼ yÃ¶nlerinden yararlanabilir.

Ã–rneÄŸin, bir topluluktaki bireysel modellerin tÃ¼mÃ¼ aynÄ± algoritma tÃ¼rÃ¼ne dayanÄ±yorsa, belirli veri tÃ¼rlerinde hepsi benzer hatalar yapabilir. FarklÄ± algoritmalar kullanan modelleri birleÅŸtirerek, topluluk potansiyel olarak her bir bireysel modelin yaptÄ±ÄŸÄ± hatalarÄ± dÃ¼zeltebilir ve bu da daha iyi genel performansa yol aÃ§ar. Ek olarak, farklÄ± veriler Ã¼zerinde veya farklÄ± parametreler kullanÄ±larak eÄŸitilmiÅŸ modelleri birleÅŸtirerek, topluluk potansiyel olarak verideki temel varyasyonun daha fazlasÄ±nÄ± yakalayabilir ve bu da daha doÄŸru tahminlere yol aÃ§ar.

**ğŸ§± YÄ±ÄŸÄ±nlama (Stacking)**

BaÅŸka bir Ã¶nemli yaklaÅŸÄ±m da **yÄ±ÄŸÄ±nlamaydÄ± (stacking)**, yani modelleri iki aÅŸamada kullanmaktÄ±. Ä°lk olarak, Ã§eÅŸitli modellerden Ã§ok sayÄ±da tahmin oluÅŸturulur ve bunlar daha sonra ikinci seviye bir model iÃ§in girdi olarak kullanÄ±lÄ±r: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751).

Kazanan Ã§Ã¶zÃ¼m farklÄ± bir yaklaÅŸÄ±m iÃ§eriyordu (nihai karÄ±ÅŸtÄ±rmada daha az modelle), ancak aynÄ± temel mantÄ±ÄŸa dayanÄ±yordu: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957).

## KapsamlÄ± bir Ã§Ã¶zÃ¼m *(A complete solution)*

Bu bÃ¶lÃ¼mÃ¼n daha Ã¶nceki kÄ±sÄ±mlarÄ±nda, bir gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma yarÄ±ÅŸmasÄ± iÃ§in temel bir Ã§Ã¶zÃ¼mle nasÄ±l baÅŸlanacaÄŸÄ±nÄ± aÃ§Ä±klamÄ±ÅŸtÄ±k. Bu bÃ¶lÃ¼mde, **Abhishek** ([https://www.kaggle.com/abhishek](https://www.kaggle.com/abhishek)) ve **Tanul** ([https://www.kaggle.com/tanulsingh077](https://www.kaggle.com/tanulsingh077)) gibi rockstar'larÄ±n yer aldÄ±ÄŸÄ± bir topluluk ekibinin, yukarÄ±da tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z bileÅŸenlerin zekice yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir uygulamasÄ±yla nasÄ±l bir **gÃ¼mÃ¼ÅŸ madalya bÃ¶lgesi Ã§Ã¶zÃ¼mÃ¼ne** (36. sÄ±ra) ulaÅŸtÄ±ÄŸÄ±nÄ± gÃ¶steriyoruz. Ã‡Ã¶zÃ¼mlerini Ã¶zetleyen gÃ¶nderiye buradan ulaÅŸÄ±labilir: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220628](https://www.google.com/search?q=https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220628).

OnlarÄ±n nihai Ã§Ã¶zÃ¼mÃ¼ Ã¼Ã§ modelin birleÅŸimiydi: **EfficientNet-B7, EfficientNet-B3a ve SE-ResNext50**. Modeller, ÅŸu not defterinde aÃ§Ä±klanan iÅŸlem hattÄ±nÄ± takip etti: [https://www.kaggle.com/code/abhishek/tez-faster-and-easier-training-for-leaf-detection/](https://www.google.com/search?q=https://www.kaggle.com/code/abhishek/tez-faster-and-easier-training-for-leaf-detection/). Bu not defterinde dikkat Ã§ekici olan bir ÅŸey, **tez**'i kullanmasÄ±dÄ±r: Abhishek tarafÄ±ndan geliÅŸtirilen bir PyTorch eÄŸiticisi (trainer) ([https://github.com/abhishekkrthakur/tez](https://github.com/abhishekkrthakur/tez)). Tez'in ardÄ±ndaki fikir, **yineleyici (boilerplate) koda** olan ihtiyacÄ± ortadan kaldÄ±rarak PyTorch modellerinin oluÅŸturulmasÄ±nÄ± ve daÄŸÄ±tÄ±lmasÄ±nÄ± basitleÅŸtirmektir. Bu amaÃ§, iÅŸlem hattÄ±nÄ±n temel bileÅŸenlerinin standartlaÅŸtÄ±rÄ±lmasÄ±yla, Ã¶ncelikle model sÄ±nÄ±fÄ± ile, elde edilir.

AÅŸaÄŸÄ±da, [https://www.kaggle.com/code/abhishek/tez-faster-and-easier-training-for-leaf-detection/](https://www.google.com/search?q=https://www.kaggle.com/code/abhishek/tez-faster-and-easier-training-for-leaf-detection/) adresinde gÃ¶sterilen Ã§Ã¶zÃ¼mÃ¼n bazÄ± kÄ±sÄ±mlarÄ±nÄ± tartÄ±ÅŸÄ±yoruz:

```python
class LeafModel(tez.Model):
    def __init__(self, num_classes):
        super().__init__()
        self.effnet = EfficientNet.from_pretrained("efficientnet-b4")
        self.dropout = nn.Dropout(0.1)
        self.out = nn.Linear(1792, num_classes)
        self.step_scheduler_after = "epoch"

    def monitor_metrics(self, outputs, targets):
        if targets is None:
            return {}
        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()
        targets = targets.cpu().detach().numpy()
        accuracy = metrics.accuracy_score(targets, outputs)
        return {"accuracy": accuracy}

    def fetch_optimizer(self):
        opt = torch.optim.Adam(self.parameters(), lr=3e-4)
        return opt

    def fetch_scheduler(self):
        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1
        )
        return sch

    def forward(self, image, targets=None):
        batch_size, _, _, _ = image.shape
        x = self.effnet.extract_features(image)
        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)
        outputs = self.out(self.dropout(x))

        if targets is not None:
            loss = nn.CrossEntropyLoss()(outputs, targets)
            metrics = self.monitor_metrics(outputs, targets)
            return outputs, loss, metrics
        return outputs, None, None
```

Bu kod ne yapÄ±yor? BileÅŸenlere gÃ¶z atalÄ±m:

1.  Ä°lk blok, modeli PyTorch tarzÄ±nda tanÄ±mlar: **Ã¶nceden eÄŸitilmiÅŸ gÃ¶vde**, ardÄ±ndan **dropout dÃ¼zenlileÅŸtirmesi** ve gÃ¶revimize uygun bir **sÄ±nÄ±flandÄ±rma baÅŸlÄ±ÄŸÄ±**.
2.  Bunu, bizim durumumuzda modelin doÄŸruluÄŸunu dÃ¶ndÃ¼ren **`monitor_metrics`** metodu takip eder.
3.  Bundan sonraki iki metot isteÄŸe baÄŸlÄ±dÄ±r (varsayÄ±lan iyileÅŸtirici (optimizer) deÄŸerleri genellikle kabul edilebilir; ayarlanmasÄ± gereken en Ã¶nemli parametre **Ã¶ÄŸrenme oranÄ±dÄ±r**).
4.  Son olarak, modelimizin girdiden Ã§Ä±ktÄ±ya nasÄ±l Ã§alÄ±ÅŸacaÄŸÄ±nÄ± tanÄ±mlayan **`forward`** metodu bulunur.

Tez gibi bir Ã§atÄ±nÄ±n avantajÄ±, modellerde tekrarlanabilir bir yapÄ±yÄ± zorunlu kÄ±lmasÄ± ve bÃ¶ylece **yineleyici koddan kurtulmamÄ±zÄ±** saÄŸlamasÄ±dÄ±r.

**ğŸ–¼ï¸ `ImageDataset` KullanÄ±mÄ±**

Bu Ã§Ã¶zÃ¼mde gÃ¶sterilen Tez'in bir diÄŸer kullanÄ±ÅŸlÄ± Ã¶zelliÄŸi de **`ImageDataset`** iÅŸlevselliÄŸidir â€“ bu, scikit-learn tarzÄ±na alÄ±ÅŸkÄ±n yeni baÅŸlayanlar iÃ§in zaman zaman bunaltÄ±cÄ± olabilen veri yapÄ±sÄ± oluÅŸturma sÃ¼recini basitleÅŸtirir:

```python
image_path = "../input/cassava-leaf-disease-classification/train_images/"
train_image_paths = [os.path.join(image_path, x) for x in df_train.image_id.values]
valid_image_paths = [os.path.join(image_path, x) for x in df_valid.image_id.values]
train_targets = df_train.label.values
valid_targets = df_valid.label.values

# train_aug ve valid_aug'un daha Ã¶nce tanÄ±mlandÄ±ÄŸÄ±nÄ± varsayÄ±yoruz
train_dataset = ImageDataset(
    image_paths=train_image_paths,
    targets=train_targets,
    resize=None,
    augmentations=train_aug,
)

valid_dataset = ImageDataset(
    image_paths=valid_image_paths,
    targets=valid_targets,
    resize=None,
    augmentations=valid_aug,
)
```

Veri setini katlamalara (folds) gÃ¶re bÃ¶ldÃ¼kten sonra, sadece **`ImageDataset`** sÄ±nÄ±fÄ±nÄ±n iki Ã¶rneÄŸini oluÅŸtururuz ve veri iÅŸleme, kullanÄ±cÄ±nÄ±n minimum katÄ±lÄ±mÄ±yla perde arkasÄ±nda halledilir. Bu, PyTorch hata ayÄ±klamasÄ± yerine asÄ±l modelleme gÃ¶revine konsantre olmamÄ±zÄ± saÄŸlar.

**ğŸš€ Model EÄŸitimi**

Model eÄŸitimi Ã§ok basittir ve esnektir â€“ Ã¶zellikle, erken durdurma gibi geri Ã§aÄŸÄ±rmalarÄ± (callbacks) dahil etmek, daha Ã¶nce Keras/TensorFlow modelleme yaklaÅŸÄ±mÄ±nÄ± denemiÅŸ olan herkes iÃ§in tanÄ±dÄ±k gelecektir:

```python
model = LeafModel(num_classes=dfx.label.nunique())
es = EarlyStopping(
    monitor="valid_loss", model_path="model.bin", patience=3, mode="min"
)
model.fit(
    train_dataset,
    valid_dataset=valid_dataset,
    train_bs=32,
    valid_bs=64,
    device="cuda",
    epochs=10,
    callbacks=[es],
    fp16=True,
)
model.save("model.bin")
```

Bu bÃ¶lÃ¼mde, temel Ã§Ã¶zÃ¼mÃ¼mÃ¼zde tanÄ±ttÄ±ÄŸÄ±mÄ±z temel fikirleri alan, bir PyTorch eÄŸiticisi kullanan ve **daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde eÄŸitilmiÅŸ farklÄ± mimarileri topluluk oluÅŸturan** ve olaÄŸanÃ¼stÃ¼ iyi performans gÃ¶steren bir gÃ¼mÃ¼ÅŸ madalya bÃ¶lgesi Ã§Ã¶zÃ¼mÃ¼nÃ¼n parÃ§alarÄ±nÄ± gÃ¶sterdik.

> ğŸ“ AlÄ±ÅŸtÄ±rma 4
> 
> 
> 
> ÃœÃ§ farklÄ± model (mimari) eÄŸittiÄŸinizi varsayalÄ±m. **Ã‡oÄŸunluk oyu (majority vote)** yÃ¶nteminin, **sÄ±nÄ±f olasÄ±lÄ±k vektÃ¶rlerini birleÅŸtirmeye** gÃ¶re daha dÃ¼ÅŸÃ¼k performans verebileceÄŸi durumlar dÃ¼ÅŸÃ¼nebilir misiniz? SÄ±nÄ±f iliÅŸkisini gÃ¶steren olasÄ±lÄ±k vektÃ¶rlerini birleÅŸtirmenin farklÄ± yollarÄ± nelerdir?
> 
> 
> 
> **Ä°pucu**: Ã‡oÄŸunluk oyu yÃ¶nteminin, noktasal ortalama (pointwise mean), medyan veya diÄŸer konum Ã¶lÃ§Ã¼lerinden neden farklÄ± bir performansa yol aÃ§abileceÄŸini karÅŸÄ±laÅŸtÄ±rarak baÅŸlayÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

## Ã–zet *(Summary)*

Bu bÃ¶lÃ¼mde, bir gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma yarÄ±ÅŸmasÄ± iÃ§in temel bir Ã§Ã¶zÃ¼mle nasÄ±l baÅŸlanacaÄŸÄ±nÄ± aÃ§Ä±kladÄ±k ve rekabetÃ§i (madalya) bÃ¶lgesine geÃ§mek iÃ§in bir dizi olasÄ± uzantÄ±yÄ± tartÄ±ÅŸtÄ±k. Bir sonraki bÃ¶lÃ¼mde, metin sÄ±nÄ±flandÄ±rma yarÄ±ÅŸmasÄ±nda doÄŸru bir yolculuÄŸa nasÄ±l baÅŸlanacaÄŸÄ±nÄ± gÃ¶stererek, **DoÄŸal Dil Ä°ÅŸleme (NLP)** ile benzer bir yolculuÄŸa Ã§Ä±kacaÄŸÄ±z.

---

# BÃ¶lÃ¼m 4: NLP YarÄ±ÅŸmasÄ± â€“ Google Quest Soru-Cevap Etiketleme *(Chapter 4: NLP Competition â€“ Google Quest Q&A Labeling)*

Bu bÃ¶lÃ¼mde, **DoÄŸal Dil Ä°ÅŸleme (NLP)** uygulamalarÄ±, Ã¶zellikle de **metin sÄ±nÄ±flandÄ±rmasÄ±** hakkÄ±nda konuÅŸacaÄŸÄ±z. YaklaÅŸÄ±mÄ±mÄ±zÄ± gÃ¶stermek amacÄ±yla, **Google Quest Soru-Cevap Etiketleme** yarÄ±ÅŸmasÄ±nÄ±n verilerini kullanacaÄŸÄ±z: [https://www.kaggle.com/competitions/google-quest-challenge](https://www.kaggle.com/competitions/google-quest-challenge).

Bu yarÄ±ÅŸma ne hakkÄ±ndaydÄ±? Ä°ÅŸte resmi aÃ§Ä±klama:

> Bilgisayarlar, tek ve doÄŸrulanabilir cevaplarÄ± olan sorularÄ± yanÄ±tlama konusunda gerÃ§ekten iyidir. Ancak insanlar, genellikle gÃ¶rÃ¼ÅŸ, tavsiye veya kiÅŸisel deneyimlerle ilgili sorularÄ± yanÄ±tlama konusunda hala daha iyidir.
>
> Ä°nsanlar, derin, Ã§ok boyutlu bir baÄŸlam anlayÄ±ÅŸÄ± gerektiren **Ã¶znel sorulara** yanÄ±t vermede daha iyidir â€“ bilgisayarlarÄ±n henÃ¼z iyi yapmasÄ± iÃ§in eÄŸitilmediÄŸi bir ÅŸey... Sorular birÃ§ok biÃ§imde olabilir; bazÄ±larÄ±nÄ±n Ã§ok cÃ¼mleli aÃ§Ä±klamalarÄ± varken, diÄŸerleri basit bir merak veya tam olarak geliÅŸtirilmiÅŸ bir problem olabilir. Birden fazla amacÄ± olabilir veya tavsiye ve gÃ¶rÃ¼ÅŸ arayabilirler. BazÄ±larÄ± yararlÄ±, bazÄ±larÄ± ilginÃ§ olabilir. BazÄ±larÄ± ise basitÃ§e doÄŸru veya yanlÄ±ÅŸtÄ±r.
>
> Ne yazÄ±k ki, veri ve tahmine dayalÄ± modellerin eksikliÄŸi nedeniyle daha iyi Ã¶znel soru-cevap algoritmalarÄ± oluÅŸturmak zordur. Bu nedenle, kitlesel kaynak (crowdsourcing) yoluyla NLP ve diÄŸer ML bilimi tÃ¼rlerini ilerletmeye adanmÄ±ÅŸ bir grup olan Google Research'teki **CrowdSource ekibi**, bu kalite puanlama yÃ¶nlerinin birÃ§oÄŸu hakkÄ±nda veri toplamÄ±ÅŸtÄ±r.
>
> Bu yarÄ±ÅŸmada, **soru-cevabÄ±n farklÄ± Ã¶znel yÃ¶nleri iÃ§in tahmine dayalÄ± algoritmalar oluÅŸturmak** Ã¼zere bu yeni veri setini kullanmaya davet ediliyorsunuz. Soru-cevap Ã§iftleri, "saÄŸduyulu" bir yaklaÅŸÄ±mla yaklaÅŸÄ±k 70 farklÄ± web sitesinden toplandÄ±. Derecelendirmecilerimiz minimum rehberlik ve eÄŸitim aldÄ± ve bÃ¼yÃ¼k Ã¶lÃ§Ã¼de istemlerin Ã¶znel yorumlarÄ±na gÃ¼vendiler. Bu nedenle, her istem, derecelendirmecilerin gÃ¶revi tamamlamak iÃ§in sadece saÄŸduyularÄ±nÄ± kullanabilmeleri iÃ§in en sezgisel ÅŸekilde hazÄ±rlandÄ±. KarmaÅŸÄ±k ve anlaÅŸÄ±lmasÄ± zor derecelendirme yÃ¶nergelerine olan baÄŸÄ±mlÄ±lÄ±ÄŸÄ±mÄ±zÄ± azaltarak, bu veri setinin yeniden kullanÄ±m deÄŸerini artÄ±rmayÄ± umuyoruz. Ne gÃ¶rÃ¼yorsanÄ±z, onu alÄ±rsÄ±nÄ±z!
>
> Bu Ã¶znel etiketlerin gÃ¼venilir bir ÅŸekilde tahmin edilebileceÄŸini gÃ¶stermek, bu araÅŸtÄ±rma alanÄ±na yeni bir Ä±ÅŸÄ±k tutabilir. Bu yarÄ±ÅŸmanÄ±n sonuÃ§larÄ±, gelecekteki akÄ±llÄ± Soru-Cevap sistemlerinin nasÄ±l inÅŸa edileceÄŸini bilgilendirecek ve umarÄ±z bu sistemlerin daha insana benzer hale gelmesine katkÄ±da bulunacaktÄ±r.

Bu giriÅŸten ne Ã§Ä±karabiliriz? Birincisi, burada oluÅŸturacaÄŸÄ±mÄ±z algoritmalarÄ±n, insan deÄŸerlendiricinin verdiÄŸi geri bildirimi taklit etmesi gerekiyor; bu geri bildirim temel doÄŸruluk (ground truth) verimizi oluÅŸturduÄŸundan, etiketlerde bir miktar **gÃ¼rÃ¼ltÃ¼ (noise)** bekleyebiliriz. Ä°kincisi, tahmin edilecek her cevabÄ±n **birden Ã§ok yÃ¶nÃ¼** var ve bunlar deÄŸerlendiriciler arasÄ±nda **ortalamasÄ± alÄ±nmÄ±ÅŸ** durumda; bu da problemimizin **Ã§ok deÄŸiÅŸkenli regresyon (multivariate regression)** ile iyi temsil edilme olasÄ±lÄ±ÄŸÄ±nÄ±n yÃ¼ksek olduÄŸu anlamÄ±na geliyor.

Bu bÃ¶lÃ¼mÃ¼, Ã¶nceki bilgisayarla gÃ¶rme problemleri hakkÄ±ndaki bÃ¶lÃ¼me benzer ÅŸekilde yapÄ±landÄ±rdÄ±k:

* **Temel bir Ã§Ã¶zÃ¼mÃ¼n (baseline solution)** nasÄ±l oluÅŸturulacaÄŸÄ±na dair tartÄ±ÅŸma.
* ArdÄ±ndan **en iyi performans gÃ¶steren Ã§Ã¶zÃ¼mleri** inceleme.

Bu bÃ¶lÃ¼mÃ¼n kodu [https://packt.link/kwbchp4](https://packt.link/kwbchp4) adresinde bulunabilir.

## Temel Ã§Ã¶zÃ¼m *(The baseline solution)*

Elbette, metninizi Ã§evirdim:

-----

YÄ±l 2023, yani NLP'deki hemen hemen her ÅŸey bir **Transformer** mimarisiyle baÅŸlÄ±yor â€“ ve uygulama sÃ¶z konusu olduÄŸunda, **Hugging Face (HF)** alana hakim. HF'nin **Transformers** kÃ¼tÃ¼phanesi, en son teknoloji NLP modellerini uygulamak iÃ§in popÃ¼ler bir aÃ§Ä±k kaynak kÃ¼tÃ¼phanesidir. PyTorch kÃ¼tÃ¼phanesinin Ã¼zerine inÅŸa edilmiÅŸtir ve dil Ã§evirisi, metin sÄ±nÄ±flandÄ±rmasÄ± ve dil Ã¼retimi gibi Ã§eÅŸitli NLP gÃ¶revleriyle Ã§alÄ±ÅŸmak iÃ§in yÃ¼ksek seviyeli bir arayÃ¼z saÄŸlar.

Transformers kÃ¼tÃ¼phanesinin ana Ã¶zelliklerinden biri, bÃ¼yÃ¼k **Ã¶nceden eÄŸitilmiÅŸ dil modellerinde** belirli gÃ¶revlere gÃ¶re **ince ayar (fine-tune)** yapma yeteneÄŸi saÄŸlamasÄ±dÄ±r. BERT'in birden fazla varyantÄ± ile GPT'yi iÃ§eren bu modeller, devasa veri kÃ¼meleri Ã¼zerinde eÄŸitilmiÅŸtir ve doÄŸal dilin kalÄ±plarÄ±nÄ± ve yapÄ±sÄ±nÄ± yÃ¼ksek doÄŸrulukla yakalayabilirler. Bu modellerde belirli bir gÃ¶reve gÃ¶re ince ayar yaparak, kullanÄ±cÄ±lar nispeten az eÄŸitim verisiyle gÃ¼Ã§lÃ¼ sonuÃ§lar elde etmek iÃ§in modelin Ã¶nceden eÄŸitilmiÅŸ bilgisinden yararlanabilirler. KÃ¼tÃ¼phane ayrÄ±ca, metin verilerini Ã¶n iÅŸleme ve son iÅŸleme iÃ§in geniÅŸ bir araÃ§ yelpazesi ile modelleri eÄŸitmek ve deÄŸerlendirmek iÃ§in yardÄ±mcÄ± programlarÄ± iÃ§erir.

Kodumuz, Transformers kÃ¼tÃ¼phanesinin en son sÃ¼rÃ¼mÃ¼nÃ¼ kurarak ve iÃ§e aktararak baÅŸlar:

```bash
!pip install transformers
import transformer
```

Gerekli iÃ§e aktarmalarÄ± takip ediyoruz:

```python
import sys, glob, torch, random
import os, re, gc, pickle, string
import numpy as np
import pandas as pd
from scipy import stats
from transformers import DistilBertTokenizer, DistilBertModel
import math
from scipy.stats import spearmanr, rankdata
from os.path import join as path_join
from numpy.random import seed
from urllib.parse import urlparse
from sklearn.preprocessing import OneHotEncoder

seed(42)
random.seed(42)
```

SÄ±rada, scikit-learn'den farklÄ± iÅŸlevleri bir araya getiren iÃ§e aktarmalar grubu geliyor. Bunlar, **Ã¶zellik Ã§Ä±karma (feature extraction)** ve **normalleÅŸtirme** iÃ§in kullanÄ±lacaktÄ±r:

```python
import nltk
from nltk.corpus import stopwords
from sklearn.base import clone
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler, KBinsDiscretizer, QuantileTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, GroupKFold
from sklearn.multioutput import MultiOutputRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import make_scorer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import LinearSVR, SVR
eng_stopwords = set(stopwords.words("english"))
import tensorflow as tf
import tensorflow_hub as hub
```

BazÄ± genel ayarlarÄ± belirtiyoruz:

```python
# settings
data_dir = '../input/google-quest-challenge/'
RANDOM_STATE = 42
import datetime
```

Temel bir Ã§Ã¶zÃ¼m oluÅŸturma stratejimiz, veriyi Ã¶zetleyen geniÅŸ bir **Ã¶zellik alanÄ±** oluÅŸturmaktÄ±r â€“ bu nedenle, Ã¶ncelikle veri setini incelemek iyi bir baÅŸlangÄ±Ã§ adÄ±mÄ±dÄ±r. Åekil 4.1, veri setinin ilk birkaÃ§ satÄ±rÄ±nÄ± bir Ã¶rnek olarak gÃ¶stermektedir:

![](im/1008.png)

Her satÄ±r iÃ§in, **soru baÅŸlÄ±ÄŸÄ±**, **soru gÃ¶vdesi** ve **cevaba** â€“ ardÄ±ndan insan deÄŸerlendiriciler tarafÄ±ndan ifade edilen gÃ¶rÃ¼ÅŸleri Ã¶zetleyen (aÅŸaÄŸÄ±da bildirilen) hedef sÃ¼tunlar yer almaktadÄ±r.

Ä°lgilendiÄŸimiz hedef sÃ¼tunlarÄ± belirtiyoruz:

```
target_cols = ['question_asker_intent_understanding', 'question_body_critical', 
'question_conversational', 'question_expect_short_answer', 
'question_fact_seeking', 'question_has_commonly_accepted_answer', 
'question_interestingness_others', 'question_interestingness_self', 
'question_multi_intent', 'question_not_really_a_question', 
'question_opinion_seeking', 'question_type_choice', 
'question_type_compare', 'question_type_consequence', 
'question_type_definition', 'question_type_entity', 
'question_type_instructions', 'question_type_procedure', 
'question_type_reason_explanation', 'question_type_spelling', 
'question_well_written', 'answer_helpful', 
'answer_level_of_information', 'answer_plausible', 
'answer_relevance', 'answer_satisfaction', 
'answer_type_instructions', 'answer_type_procedure', 
'answer_type_reason_explanation', 'answer_well_written']
```

Bu hedeflerin anlamÄ± ve yorumlanmasÄ±nÄ±n tam aÃ§Ä±klamasÄ± iÃ§in, yarÄ±ÅŸmanÄ±n Veri bÃ¶lÃ¼mÃ¼ne gÃ¶z atÄ±n: `https://www.kaggle.com/c/google-quest-challenge/data`

Daha Ã¶nce belirtildiÄŸi gibi, soru bileÅŸenlerinden Ã¶znitelikleri (feature) Ã§Ä±karmaya odaklanacaÄŸÄ±z ve bunu yapmak iÃ§in bazÄ± yardÄ±mcÄ± fonksiyonlara ihtiyacÄ±mÄ±z var. Bir metin parÃ§asÄ±nda yapabileceÄŸiniz en basit ÅŸeylerden biriyle baÅŸlÄ±yoruz, yani kelimeleri saymak:

```python
def word_count(xstring):
    return xstring.split().str.len()
```

ArdÄ±ndan, dikkatimizi yarÄ±ÅŸma metriÄŸine Ã§eviriyoruz: **Spearman korelasyonu**, iki deÄŸiÅŸken arasÄ±ndaki istatistiksel iliÅŸkinin bir Ã¶lÃ§Ã¼sÃ¼dÃ¼r. Ä°ki deÄŸiÅŸken arasÄ±ndaki iliÅŸkinin gÃ¼cÃ¼nÃ¼ ve yÃ¶nÃ¼nÃ¼ belirlemek iÃ§in kullanÄ±lan parametrik olmayan bir testtir. Ã–zellikle, iki deÄŸiÅŸkenin ne Ã¶lÃ§Ã¼de iliÅŸkili olduÄŸunu ve aralarÄ±ndaki iliÅŸkinin pozitif (yani, bir deÄŸiÅŸken arttÄ±kÃ§a diÄŸer deÄŸiÅŸken de artar) mÄ± yoksa negatif (yani, bir deÄŸiÅŸken arttÄ±kÃ§a diÄŸer deÄŸiÅŸken azalÄ±r) mi olduÄŸunu Ã¶lÃ§er. Spearman korelasyon katsayÄ±sÄ±, **Yunanca rho (${\rho}$) harfiyle gÃ¶sterilir**, her bir veri noktasÄ± iÃ§in iki deÄŸiÅŸkenin sÄ±ralamalarÄ± arasÄ±ndaki fark alÄ±narak, farklarÄ±n karesi alÄ±narak ve toplanarak hesaplanÄ±r. KatsayÄ± -1 ile 1 arasÄ±nda deÄŸiÅŸebilir; 0 deÄŸiÅŸkenler arasÄ±nda iliÅŸki olmadÄ±ÄŸÄ±nÄ±, 1 mÃ¼kemmel pozitif korelasyonu ve -1 mÃ¼kemmel negatif korelasyonu gÃ¶sterir. 1'e veya -1'e yakÄ±n bir Spearman korelasyon katsayÄ±sÄ±, deÄŸiÅŸkenler arasÄ±nda gÃ¼Ã§lÃ¼ bir iliÅŸki olduÄŸunu, 0'a daha yakÄ±n bir katsayÄ± ise daha zayÄ±f bir iliÅŸki olduÄŸunu gÃ¶sterir. Spearman korelasyon katsayÄ±sÄ± formÃ¼lÃ¼ aÅŸaÄŸÄ±daki gibidir:

$$\rho(X, Y) = \frac{cov(R(X), R(Y))}{\sigma(R(X))\sigma(R(Y))}$$

burada:

* **$R(X)$ ve $R(Y)$** ilgili $X$ ve $Y$ deÄŸiÅŸkenlerinin sÄ±ralamalara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ halleridir: [https://en.wikipedia.org/wiki/Ranking](https://en.wikipedia.org/wiki/Ranking)
* **$cov$** sÄ±ralama deÄŸiÅŸkenlerinin **kovaryansÄ±dÄ±r**.
* **$\sigma(R(X))$ ve $\sigma(R(Y))$** sÄ±ralama deÄŸiÅŸkenlerinin **standart sapmalarÄ±dÄ±r**.

Spearman korelasyonu, veriler normal daÄŸÄ±lmadÄ±ÄŸÄ±nda veya deÄŸiÅŸkenler arasÄ±ndaki iliÅŸki **doÄŸrusal olmadÄ±ÄŸÄ±nda** sÄ±klÄ±kla kullanÄ±lÄ±r â€“ ki bu durum, birim aralÄ±ÄŸÄ±na normalize edilmiÅŸ ve neredeyse kesin olarak metnin Ã¶zellikleriyle doÄŸrusal olmayan bir ÅŸekilde iliÅŸkili olan hedeflerimiz iÃ§in geÃ§erlidir. AyrÄ±ca, **sÄ±ralÄ± verilerle** (ordinal data) veya sÃ¼rekli bir Ã¶lÃ§ekte Ã¶lÃ§Ã¼lmek yerine **sÄ±ralanmÄ±ÅŸ** veya dÃ¼zenlenmiÅŸ verilerle Ã§alÄ±ÅŸÄ±rken de faydalÄ±dÄ±r:

```python
def spearman_corr(y_true, y_pred):
    if np.ndim(y_pred) == 2:
        corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] 
for i in range(y_true.shape[1])])
    else:
        corr = stats.spearmanr(y_true, y_pred)[0]
    return corr
```

```python
custom_scorer = make_scorer(spearman_corr, greater_is_better=True)
```

Son kÄ±sÄ±m, yani fonksiyonumuzu bir **puanlayÄ±cÄ±ya (scorer)** dÃ¶nÃ¼ÅŸtÃ¼rme, Spearman korelasyonunu bir **scikit-learn** hattÄ± (pipeline) iÃ§inde kullanmak istersek gereklidir; `make_scorer` bir puanlama fonksiyonu alÄ±r ve tahmin edicinin (estimator) bir Ã§Ä±ktÄ±sÄ±nÄ± deÄŸerlendiren Ã§aÄŸrÄ±labilir bir nesne (callable) dÃ¶ndÃ¼rÃ¼r.

> AlÄ±ÅŸtÄ±rma 1
> 
> 
> 
> YarÄ±ÅŸma metriÄŸi, bu yarÄ±ÅŸmadaki birÃ§ok katÄ±lÄ±mcÄ± iÃ§in yeniydi; bu nedenle, diÄŸer metriklerden farkÄ±na dair Ã§ok sayÄ±da tartÄ±ÅŸma ve araÅŸtÄ±rma yarattÄ±. YarÄ±ÅŸma forumunda **Spearman korelasyonuna** odaklanan tartÄ±ÅŸmalarÄ± keÅŸfedin.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ± (size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± yazÄ±n):**

Daha sonra, $l$'den $n$ boyutunda ardÄ±ÅŸÄ±k parÃ§alar Ã§Ä±karmak iÃ§in kÃ¼Ã§Ã¼k bir yardÄ±mcÄ± fonksiyon oluÅŸturuyoruz. Bu, daha sonra **bellek sorunlarÄ±na yol aÃ§madan** metin gÃ¶vdemiz iÃ§in gÃ¶mÃ¼lÃ¼ vektÃ¶rler (embeddings) oluÅŸturmamÄ±za yardÄ±mcÄ± olacaktÄ±r:

```python
def chunks(l, n):
    for i in range(0, len(l), n):
        yield l[i:i + n]
```

KullanacaÄŸÄ±mÄ±z Ã¶znitelik setinin bir kÄ±smÄ±, Ã¶nceden eÄŸitilmiÅŸ modellerden elde edilen gÃ¶mÃ¼lÃ¼ vektÃ¶rlerdir (embeddings). AmacÄ±mÄ±zÄ±n bir baÅŸlangÄ±Ã§ Ã§Ã¶zÃ¼mÃ¼ (**baseline**) oluÅŸturmak olduÄŸunu hatÄ±rlayÄ±n â€“ ancak son yÄ±llarda bÃ¼yÃ¼k dil modellerinin kullanÄ±labilirliÄŸi, NLP'de bir baÅŸlangÄ±Ã§ Ã§Ã¶zÃ¼mÃ¼nÃ¼n anlamÄ±nÄ± yeniden tanÄ±mladÄ±. Ä°lk Ã§Ã¶zÃ¼mÃ¼mÃ¼zÃ¼ **BERT benzeri bir model** Ã¼zerine kurabilir miyiz?

GÃ¶mÃ¼lÃ¼ vektÃ¶rler oluÅŸturmak iÃ§in bir fonksiyon tanÄ±mlayarak, jetonlaÅŸtÄ±rÄ±cÄ±yÄ± (tokenizer) ve modeli iÃ§e aktararak baÅŸlÄ±yor ve ardÄ±ndan kÃ¼lliyatÄ± (corpus) parÃ§alar halinde iÅŸliyor, her bir soru/cevabÄ± sabit boyutlu bir gÃ¶mÃ¼lÃ¼ vektÃ¶re kodluyoruz.

Bir NLP modelinde, **jetonlaÅŸtÄ±rÄ±cÄ± (tokenizer)**, girdi metnini modele beslenebilecek bireysel jetonlara (**kelimeler veya alt kelimeler**) ayÄ±rmaktan sorumludur. JetonlaÅŸtÄ±rÄ±cÄ±, Ã¶nce girdi metnini ayÄ±rÄ±cÄ± olarak boÅŸluk ve noktalama iÅŸaretlerini kullanarak kelimelere ayÄ±rÄ±r ve ardÄ±ndan **BERT modelinin kelime daÄŸarcÄ±ÄŸÄ±nÄ±** kullanarak her kelimeyi karÅŸÄ±lÄ±k gelen jetonuna dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Bu sÃ¼rece **jetonlaÅŸtÄ±rma (tokenization)** denir ve metin verilerini bir BERT modeline girdi olarak hazÄ±rlamada kritik bir adÄ±mdÄ±r. JetonlaÅŸtÄ±rÄ±cÄ± ayrÄ±ca her cÃ¼mlenin sonuna Ã¶zel jetonlar ekleyerek girdi metninin yapÄ±sÄ±nÄ± belirtir. JetonlaÅŸtÄ±rÄ±cÄ±nÄ±n Ã§Ä±ktÄ±sÄ±, cÃ¼mleyi doÄŸrudan BERT modeline beslenebilecek bir jeton dizisi olarak temsil eder:

```python
def fetch_vectors(string_list, batch_size=64):
    DEVICE = torch.device("cuda")
    tokenizer = transformers.DistilBertTokenizer.from_pretrained("../input/distilbertbaseuncased/")
    model = transformers.DistilBertModel.from_pretrained("../input/distilbertbaseuncased/")
    model.to(DEVICE)
    fin_features = []
    for data in chunks(string_list, batch_size):
        tokenized = []
        for x in data:
            x = " ".join(x.strip().split()[:300])
            tok = tokenizer.encode(x, add_special_tokens=True)
            tokenized.append(tok[:512])
        max_len = 512
        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])
        attention_mask = np.where(padded != 0, 1, 0)
        input_ids = torch.tensor(padded).to(DEVICE)
        attention_mask = torch.tensor(attention_mask).to(DEVICE)
        with torch.no_grad():
            last_hidden_states = model(input_ids, attention_mask=attention_mask)
        features = last_hidden_states[0][:, 0, :].cpu().numpy()
        fin_features.append(features)
    fin_features = np.vstack(fin_features)
    return fin_features
```

> AlÄ±ÅŸtÄ±rma 2
> 
> 
> 
> Transformers kÃ¼tÃ¼phanesinde kullanÄ±lan farklÄ± jetonlaÅŸtÄ±rÄ±cÄ±lar (Tokenizers) arasÄ±ndaki farklarÄ± inceleyin: **Byte-Pair (BPE)**, **WordPiece** ve **SentencePiece**. Burada baÅŸka bir seÃ§im daha iyi olabilir miydi? Ä°pucu: Hugging Face dokÃ¼mantasyonu ile baÅŸlayÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ± (size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± yazÄ±n):**

Fonksiyonun anlaÅŸÄ±lmasÄ± zor olabilir, bu yÃ¼zden **akÄ±ÅŸtaki ana adÄ±mlarÄ±n** bir aÃ§Ä±klamasÄ± aÅŸaÄŸÄ±dadÄ±r:

1.  BelirteÃ§leyiciyi (**tokenizer**) ve modeli bir **kontrol noktasÄ±ndan (checkpoint)** yÃ¼kleyin.
2.  Girdi dizileri (**input strings**) arasÄ±nda, kÃ¼Ã§Ã¼k **yÄ±ÄŸÄ±nlar (batch)** halinde dÃ¶ngÃ¼ yapÄ±n (bu, `batch_size` argÃ¼manÄ± tarafÄ±ndan belirlenir).
3.  Girdi verilerini kelimelere ayÄ±rÄ±n ve elde edilen koleksiyonu **300 Ã¶ÄŸede kesin (truncate)**.
4.  Girdi verilerini **belirteÃ§lere ayÄ±rÄ±n (tokenize)**.
5.  BelirteÃ§lere ayrÄ±lmÄ±ÅŸ girdi verilerini **doldurun (pad)**, bÃ¶ylece tÃ¼m diziler, dizilerin maksimum uzunluÄŸuyla aynÄ± uzunluÄŸa sahip olur.
6.  DoldurulmuÅŸ veri iÃ§in **maskeyi oluÅŸturun**.
7.  Girdi kimliklerini (**input IDs**) ve **dikkat maskesini (attention mask)** tensÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n.

Fonksiyon hakkÄ±nda daha fazla bilgi iÃ§in, yarÄ±ÅŸma sÄ±rasÄ±nda bu fonksiyonun oluÅŸturulmasÄ±na ilham veren gÃ¶nderi en iyi tavsiyemdir: [https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/).

-----

**Veri Ä°ÅŸleme ve Ã–zellik OluÅŸturma**

DosyalarÄ± okuduktan sonra, ÅŸimdi verileri iÅŸlemeye ve **Ã¶zellikleri (features)** oluÅŸturmaya geÃ§ebiliriz:

```python
xtrain = pd.read_csv(data_dir + 'train.csv')
xtest = pd.read_csv(data_dir + 'test.csv')
```

ÃœÃ§ girdi bileÅŸenindeki (**baÅŸlÄ±k, gÃ¶vde ve cevap**) kelimeleri sayarak baÅŸlÄ±yoruz â€“ bu, basit ama Ã§ok kullanÄ±ÅŸlÄ± ve sÄ±kÃ§a kullanÄ±lan bir Ã¶zelliktir:

```python
for colname in ['question_title', 'question_body', 'answer']:
    newname = colname + '_word_len'
    xtrain[newname] = xtrain[colname].str.split().str.len()
    xtest[newname] = xtest[colname].str.split().str.len()
del newname, colname
```

> **AlÄ±ÅŸtÄ±rma 3**
> 
> 
> 
> Ã–nceki kodu hÄ±zlandÄ±rmanÄ±n bir yolunu dÃ¼ÅŸÃ¼nebilir misiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±n):

Bir diÄŸer faydalÄ± Ã¶zellik de **sÃ¶zcÃ¼ksel Ã§eÅŸitliliktir (lexical diversity)**: bir metinde kullanÄ±lan kelime daÄŸarcÄ±ÄŸÄ±nÄ±n zenginliÄŸini Ã¶lÃ§en bir Ã¶lÃ§Ã¼ttÃ¼r. Tipik olarak, bir metindeki **benzersiz kelime sayÄ±sÄ±nÄ±n** o metindeki **toplam kelime sayÄ±sÄ±na** bÃ¶lÃ¼nmesiyle hesaplanÄ±r. DoÄŸrudan NLP'de metin sÄ±nÄ±flandÄ±rmasÄ±yla ilgili olmasa da, belirli durumlarda Ã§ok Ã¶nemli bir faktÃ¶r olabilir. Ã–rneÄŸin, bir metin Ã§ok sÄ±nÄ±rlÄ± bir kelime daÄŸarcÄ±ÄŸÄ± iÃ§eriyorsa, duygu sÄ±nÄ±flandÄ±rmasÄ± yapmak veya insan deÄŸerlendiriciler tarafÄ±ndan atanan Ã¶zellikleri tahmin etmek zor olabilir. Ä°yi bir sÄ±nÄ±flandÄ±rma modelinin, geniÅŸ bir sÃ¶zcÃ¼ksel Ã§eÅŸitliliÄŸe sahip girdiyi iÅŸleyebilmesi gerekir, ancak bazÄ± durumlarda yÃ¼ksek sÃ¶zcÃ¼ksel Ã§eÅŸitlilik faydalÄ± olabilir:

```python
colname = 'answer'
xtrain[colname+'_div'] = xtrain[colname].apply(lambda s: len(set(s.split())) / len(s.split()))
xtest[colname+'_div'] = xtest[colname].apply(lambda s: len(set(s.split())) / len(s.split()))
```

Tamamen **tanÄ±mlayÄ±cÄ± istatistiklerden** baÅŸka birÃ§ok Ã¶zellik biÃ§imi Ã§Ä±karmak mÃ¼mkÃ¼ndÃ¼r ve bunlarÄ±n Ã§oÄŸu oldukÃ§a aÃ§Ä±klayÄ±cÄ± olduÄŸundan, bunlarÄ± en az yorumla sunuyoruz:

**Alan BileÅŸenleri (Domain Components)**

```python
for df in [xtrain, xtest]:
    
    df['domcom'] = df['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))
    # bileÅŸenleri say
    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))
    # bazÄ± alan adlarÄ±nÄ±n isimlerinde daha az bileÅŸen olmasÄ± durumunda uzunluÄŸu doldur
    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])
    # bileÅŸenler
    for ii in range(0,4):
        df['dom_'+str(ii)] = df['domcom'].apply(lambda s: s[ii])
    
# temizlik
xtrain.drop('domcom', axis = 1, inplace = True)
xtest.drop('domcom', axis = 1, inplace = True)
```

**PaylaÅŸÄ±lan Ã–ÄŸeler: Soru ve Cevap KaÃ§ Kelime PaylaÅŸÄ±yor?**

```python
for df in [xtrain, xtest]:
    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )
    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )
    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)
    df['qa_word_overlap_norm1'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)
    df['qa_word_overlap_norm2'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)
    df.drop(['q_words', 'a_words'], axis = 1, inplace = True)
```

**Metindeki Karakter, Duraklama Kelimesi, Noktalama ve BÃ¼yÃ¼k Harf SayÄ±larÄ±**

```python
for df in [xtrain, xtest]:
 ## Metindeki Karakter SayÄ±sÄ± ##
    df["question_title_num_chars"] = df["question_title"].apply(lambda x: len(str(x)))
    df["question_body_num_chars"] = df["question_body"].apply(lambda x: len(str(x)))
    df["answer_num_chars"] = df["answer"].apply(lambda x: len(str(x)))
 ## Metindeki Duraklama Kelimesi (Stopword) SayÄ±sÄ± ##
    df["question_title_num_stopwords"] = df["question_title"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))
    df["question_body_num_stopwords"] = df["question_body"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))
    df["answer_num_stopwords"] = df["answer"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))
 ## Metindeki Noktalama Ä°ÅŸareti SayÄ±sÄ± ##
    df["question_title_num_punctuations"] =df['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )
    df["question_body_num_punctuations"] =df['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )
    df["answer_num_punctuations"] =df['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )
 ## Metindeki TamamÄ± BÃ¼yÃ¼k Harfli Kelime SayÄ±sÄ± ##
    df["question_title_num_words_upper"] = df["question_title"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))
    df["question_body_num_words_upper"] = df["question_body"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))
    df["answer_num_words_upper"] = df["answer"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))
```

Bu ÅŸekilde Ã§Ä±karÄ±lan Ã¶zelliklerin tam listesi iÃ§in, kitabÄ±n deposunda bulunan ilgili not defterine bakÄ±nÄ±z: [https://github.com/PacktPublishing/The-Kaggle-Workbook](https://github.com/PacktPublishing/The-Kaggle-Workbook).

-----

**GÃ¶mÃ¼lÃ¼ Uzayda (Embedding Space) Mesafeler**

Åimdi, girdi bileÅŸenleri arasÄ±ndaki **gÃ¶mÃ¼lÃ¼ uzaydaki mesafelere** dayanan baÅŸka bir Ã¶zellik grubu oluÅŸturmaya geÃ§iyoruz. Metin parÃ§alarÄ± iÃ§in hÄ±zlÄ± ve kaliteli gÃ¶mÃ¼lmeler (embeddings) oluÅŸturmanÄ±n bir yolu, bir **cÃ¼mle kodlayÄ±cÄ± (sentence encoder)** kullanmaktÄ±r.

**Evrensel CÃ¼mle KodlayÄ±cÄ± (Universal Sentence Encoder - USE)**, doÄŸal dil cÃ¼mlelerini yÃ¼ksek boyutlu bir uzaya eÅŸleyen, **semantik olarak benzer cÃ¼mlelerin birbirine yakÄ±n** olduÄŸu bir Google tarafÄ±ndan geliÅŸtirilmiÅŸ makine Ã¶ÄŸrenimi modelidir. Ã‡eÅŸitli doÄŸal dil metinlerinden oluÅŸan bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde eÄŸitilmiÅŸtir ve belge sÄ±nÄ±flandÄ±rmasÄ±, dil Ã§evirisi ve bilgi eriÅŸimi gibi Ã§eÅŸitli NLP gÃ¶revleri iÃ§in kullanÄ±labilir.

USE, deÄŸiÅŸken uzunluktaki bir girdi dizisini sabit uzunlukta bir vektÃ¶r temsiline kodlamak iÃ§in dikkat mekanizmalarÄ±nÄ± kullanan bir **kodlayÄ±cÄ±-kod Ã§Ã¶zÃ¼cÃ¼ (encoder-decoder)** model tÃ¼rÃ¼dÃ¼r ve bu vektÃ¶r daha sonra deÄŸiÅŸken uzunlukta bir Ã§Ä±ktÄ± dizisine geri Ã§Ã¶zÃ¼lÃ¼r. KodlayÄ±cÄ±, girdi cÃ¼mlesini iÅŸler ve anlamÄ±nÄ±n bir temsilini oluÅŸturur, bu da daha sonra kod Ã§Ã¶zÃ¼cÃ¼ tarafÄ±ndan Ã§Ä±ktÄ± dizisini oluÅŸturmak iÃ§in kullanÄ±lÄ±r.

USE, geniÅŸ bir doÄŸal dil cÃ¼mlesi yelpazesini kompakt ve anlamlÄ± bir vektÃ¶r temsiline eÅŸlemek Ã¼zere eÄŸitilmiÅŸtir ve Ã§eÅŸitli dilleri ve cÃ¼mle yapÄ±larÄ±nÄ± ele alabilecek ÅŸekilde tasarlanmÄ±ÅŸtÄ±r. Dil Ã§evirisi, metin sÄ±nÄ±flandÄ±rmasÄ± ve bilgi eriÅŸimi dahil olmak Ã¼zere Ã§eÅŸitli uygulamalarda kullanÄ±labilir. USE hakkÄ±nda daha fazla bilgi iÃ§in resmi proje sayfasÄ±na gÃ¶z atÄ±n: [https://tfhub.dev/google/universal-sentence-encoder-large/5](https://tfhub.dev/google/universal-sentence-encoder-large/5):

```python
module_url = "../input/universalsentenceencoderlarge4/"
embed = hub.load(module_url)
embeddings_train = {}
embeddings_test = {}
for text in ['question_title', 'question_body', 'answer']:
    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()
    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()
    curr_train_emb = []
    curr_test_emb = []
    batch_size = 4
    ind = 0
    while ind*batch_size < len(train_text):
        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])["outputs"].numpy())
        ind += 1
    ind = 0
    while ind*batch_size < len(test_text):
        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])["outputs"].numpy())
        ind += 1    
    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)
    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)
    print(text)
del embed
```

BaÅŸlÄ±k, soru ve cevap bileÅŸenleri iÃ§in gÃ¶mÃ¼lmeleri oluÅŸturduktan sonra, gÃ¶mÃ¼lÃ¼ uzaydaki aralarÄ±ndaki mesafeleri hesaplayabiliriz:

```python
l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)
cos_dist = lambda x, y: (x*y).sum(axis=1)

dist_features_train = np.array([
    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),
    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),
    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),
    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),
    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),
    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])
 ]).T

dist_features_test = np.array([
    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),
    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),
    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),
    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),
    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),
    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])
 ]).T

del embeddings_train, embeddings_test

for ii in range(0,6):
    xtrain['dist'+str(ii)] = dist_features_train[:,ii]
    xtest['dist'+str(ii)] = dist_features_test[:,ii]
```

> **AlÄ±ÅŸtÄ±rma 4**
> 
> 
> 
> Ã–nceki kodu hÄ±zlandÄ±rma yollarÄ±nÄ± inceleyin. Ä°pucu: **liste iÃ§erme (list comprehension)**.
> 
> 
> 
> AyrÄ±ca, Ã¶nceki koddaki vektÃ¶r Ã§iftleri arasÄ±nda hesaplanabilecek **baÅŸka hangi uzaklÄ±k metrikleri (distance metrics)** vardÄ±r?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±n):

Ã–zellik hazÄ±rlama kÄ±smÄ±nÄ± tamamladÄ±ktan sonra, **modelleme ardÄ±ÅŸÄ±k dÃ¼zenimizi (modeling pipeline)** oluÅŸturmaya geÃ§ebiliriz. **Scikit-learn**'de, ardÄ±ÅŸÄ±k dÃ¼zenler (`pipelines`), `fit` ve `transform` Ã¼ye iÅŸlevlerini uygulayan nesnelere bir dizi iÅŸlemi sÄ±ralÄ± olarak uygulamak iÃ§in kullanÄ±lÄ±r. Bu, birden Ã§ok adÄ±mÄ± tek bir scikit-learn tahmincisine (`estimator`) dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in Ã§ok kullanÄ±ÅŸlÄ±dÄ±r, bÃ¶ylece her adÄ±mÄ± ayrÄ± ayrÄ± eÄŸitmek yerine, birden Ã§ok adÄ±mÄ± iÃ§eren bir modeli **tek bir Ã§aÄŸrÄ±da** eÄŸitebilirsiniz. Biz de bu yaklaÅŸÄ±mÄ±, metin verilerini Ã¶n iÅŸlemek, Ã¶zellik Ã§Ä±karmak ve tahmine dayalÄ± bir model tahmin etmek iÃ§in kullanacaÄŸÄ±z.

-----

**TfidfVectorizer ile Metin VektÃ¶rleÅŸtirme**

Ã–zellik Ã§Ä±karma ardÄ±ÅŸÄ±k dÃ¼zenimizin Ã¶nemli bir bileÅŸeni **TfidfVectorizer**'dÄ±r: **Terim SÄ±klÄ±ÄŸÄ±-Ters Belge SÄ±klÄ±ÄŸÄ± (Term Frequency-Inverse Document Frequency - TF-IDF)**, NLP'de yaygÄ±n olarak kullanÄ±lan bir metin verilerini vektÃ¶rleÅŸtirme yÃ¶ntemidir.

**TF-IDF vektÃ¶rleyici**, girdi metnini tek tek kelimelere (veya alt kelimelere) ayÄ±rarak (**tokenize** ederek) Ã§alÄ±ÅŸÄ±r ve ardÄ±ndan puanÄ± hesaplar â€“ bu puan, bir kelimenin belgede kaÃ§ kez gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ Ã¶lÃ§en **Terim SÄ±klÄ±ÄŸÄ± (Term Frequency - TF)** ile kelimenin kÃ¼lliyattaki (**corpus**) nadirliÄŸini Ã¶lÃ§en **Ters Belge SÄ±klÄ±ÄŸÄ±'nÄ±n (Inverse Document Frequency - IDF)** bir Ã§arpÄ±mÄ±dÄ±r. Metindeki her kelime iÃ§in bunlarÄ± birleÅŸtirerek, daha sonra bir makine Ã¶ÄŸrenimi modeline girdi olarak kullanÄ±labilecek metnin bir **vektÃ¶r temsilini** oluÅŸturabiliriz.

AÅŸaÄŸÄ±da, (baÅŸlÄ±k, soru ve cevap) kÃ¼mesinden farklÄ± alanlar iÃ§in sayÄ±sÄ±z ardÄ±ÅŸÄ±k dÃ¼zeni tanÄ±mlÄ±yoruz â€“ yalnÄ±zca girdinin **metinsel temsilinin** ele alÄ±nmasÄ±na odaklanarak.

-----

**ArdÄ±ÅŸÄ±k DÃ¼zenlerin TanÄ±mlanmasÄ±**

BaÅŸlÄ±k alanÄ± iÃ§in iki dÃ¶nÃ¼ÅŸÃ¼m oluÅŸturarak baÅŸlÄ±yoruz: biri **kelime dÃ¼zeyinde** ve diÄŸeri **karakter dÃ¼zeyinde**. Ortaya Ã§Ä±kan vektÃ¶rlerin boyutunu kontrol etmek iÃ§in, kelime dÃ¼zeyindeki dÃ¶nÃ¼ÅŸÃ¼mÃ¼ 25.000, karakter dÃ¼zeyindeki dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ise 5.000 Ã¶zellikle sÄ±nÄ±rlandÄ±rÄ±yoruz. `max_features` parametresi, **TfidfVectorizer**'Ä±n daha Ã¶nce aÃ§Ä±klanan TF-IDF matrisini oluÅŸtururken yalnÄ±zca en sÄ±k kullanÄ±lan ilk `max_features` kelimeyi/karakteri dikkate alacaÄŸÄ± anlamÄ±na gelir:

```python
limit_word = 25000
limit_char = 5000

# BaÅŸlÄ±k AlanÄ± (Question Title)
title_col = 'question_title'
title_transformer = Pipeline([
    ('tfidf', TfidfVectorizer(lowercase = False, 
max_df = 0.3, min_df = 1,  
binary = False, use_idf = True, 
smooth_idf = False,
           ngram_range = (1,2), stop_words = 'english', 
           token_pattern = '(?u)\\b\\w+\\b' , 
max_features = limit_word ))
])
title_transformer2 = Pipeline([
    ('tfidf2',  TfidfVectorizer( sublinear_tf=True,
    strip_accents='unicode', analyzer='char',
    stop_words='english', ngram_range=(1, 4), 
max_features= limit_char))   
])

# Soru GÃ¶vdesi AlanÄ± (Question Body)
body_col = 'question_body'
body_transformer = Pipeline([
    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,
                             binary = False, use_idf = True, smooth_idf = False,
                             ngram_range = (1,2), stop_words = 'english', 
                             token_pattern = '(?u)\\b\\w+\\b' , max_features = limit_word ))
])
body_transformer2 = Pipeline([
    ('tfidf2',  TfidfVectorizer( sublinear_tf=True,
    strip_accents='unicode', analyzer='char',
    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   
])

# Cevap AlanÄ± (Answer)
answer_col = 'answer'
answer_transformer = Pipeline([
    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,
                             binary = False, use_idf = True, smooth_idf = False,
                             ngram_range = (1,2), stop_words = 'english', 
                             token_pattern = '(?u)\\b\\w+\\b' , max_features = limit_word ))
])
answer_transformer2 = Pipeline([
    ('tfidf2',  TfidfVectorizer( sublinear_tf=True,
    strip_accents='unicode', analyzer='char',
    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   
])
```

> **AlÄ±ÅŸtÄ±rma 5**
> 
> 
> 
> **TfidfVectorizer** iÃ§in farklÄ± argÃ¼manlarÄ±n model performansÄ± Ã¼zerindeki etkisini araÅŸtÄ±rÄ±n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±n):

OluÅŸturduÄŸumuz Ã¶zellik uzayÄ±mÄ±z, daha Ã¶nce yarattÄ±ÄŸÄ±mÄ±z **tanÄ±mlayÄ±cÄ± istatistikler** ve **gÃ¶mÃ¼lmeler (embeddings) arasÄ±ndaki mesafeler** gibi birden fazla sayÄ±sal Ã¶zellik iÃ§erir ve bunlarÄ±n, aÅŸikÃ¢r nedenlerle, metin alanlarÄ±ndan biraz daha farklÄ± bir ÅŸekilde iÅŸlenmesi gerekir:

```python
num_cols = [
    'question_title_word_len', 'question_body_word_len', 'answer_word_len',
    'answer_div', 'question_title_num_chars', 'question_body_num_chars',
    'answer_num_chars', 'question_title_num_stopwords', 'question_body_num_stopwords',
    'answer_num_stopwords', 'question_title_num_punctuations', 'question_body_num_punctuations',
    'answer_num_punctuations', 'question_title_num_words_upper', 'question_body_num_words_upper',
    'answer_num_words_upper', 'dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5'
]
```

SayÄ±sal sÃ¼tunlara hafif bir iÅŸleme uyguluyoruz:

* **SimpleImputer**: Bir veri setindeki eksik deÄŸerleri atamak iÃ§in kullanÄ±lan bir scikit-learn sÄ±nÄ±fÄ±dÄ±r. Eksik deÄŸerleri sabit bir deÄŸer, ortalama veya medyan deÄŸerini kullanarak atama gibi temel stratejiler sunar.
* **PowerTransformer**: Veri Ã¼zerinde bir kuvvet dÃ¶nÃ¼ÅŸÃ¼mÃ¼ gerÃ§ekleÅŸtirir, bu da **varyansÄ± stabilize etme** ve veri daÄŸÄ±lÄ±mÄ±nÄ± **Gauss'a (normale) yakÄ±nlaÅŸtÄ±rma** amacÄ±na hizmet eder (bu Ã¶zelliklere doÄŸrusal bir model uygulamak istersek faydalÄ±dÄ±r).

```python
num_transformer = Pipeline([
    ('impute', SimpleImputer(strategy='constant', fill_value=0)),
    ('scale', PowerTransformer(method='yeo-johnson'))
])
```

-----

Son olarak, daha Ã¶nce oluÅŸturulan **kategorik sÃ¼tunlarÄ±mÄ±z** var:

```python
cat_cols = [
    'dom_0', 'dom_1', 'dom_2', 
    'dom_3', 'category', 
    'is_question_no_name_user',
    'is_answer_no_name_user',
    'dom_cnt'
]
cat_transformer = Pipeline([
    ('impute', SimpleImputer(strategy='constant', fill_value='')),
    ('encode', OneHotEncoder(handle_unknown='ignore'))
])
```

-----

**TÃ¼m Ã–zellikleri BirleÅŸtirme: ColumnTransformer**

YukarÄ±daki tÃ¼m Ã¶ÄŸeleri, Ã¶zellik uzayÄ±mÄ±zÄ±n tÃ¼m iÃ§eriÄŸini iÅŸleyen tek bir dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼de birleÅŸtirebiliriz:

```python
preprocessor = ColumnTransformer(
    transformers = [
        ('title', title_transformer, title_col),
        ('title2', title_transformer2, title_col),
        ('body', body_transformer, body_col),
        ('body2', body_transformer2, body_col),
        ('answer', answer_transformer, answer_col),
        ('answer2', answer_transformer2, answer_col),
        ('num', num_transformer, num_cols),
        ('cat', cat_transformer, cat_cols)
    ]
)
```

-----

**Modelleme ArdÄ±ÅŸÄ±k DÃ¼zeni**

Son olarak, tÃ¼m bu Ã¶zellikleri kullanmak iÃ§in gerÃ§ek bir regresyon modeline ihtiyacÄ±mÄ±z var. Bu basit bir temel Ã§izgi (**baseline**) olduÄŸu iÃ§in, **ridge regresyonunu** seÃ§iyoruz: hatÄ±rlatmak gerekirse, **L2 dÃ¼zenlileÅŸtirmesi (L2 regularization)** kullanarak modelin genellemesini iyileÅŸtiren ve **aÅŸÄ±rÄ± uydurmayÄ± (overfitting) Ã¶nleyen** bir doÄŸrusal regresyon modelidir. L2 dÃ¼zenlileÅŸtirmesi, modelin kayÄ±p fonksiyonuna, **karelenmiÅŸ aÄŸÄ±rlÄ±klarÄ±n toplamÄ±yla orantÄ±lÄ±** bir ceza terimi ekler. SonuÃ§ olarak, model "daha kÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±klar" kullanmaya teÅŸvik edilir, bu da yorumlanabilirliÄŸi de artÄ±rabilir.

$$\text{KayÄ±p} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p} w_j^2$$

```python
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('estimator',Ridge(random_state=RANDOM_STATE))
])
```

> **AlÄ±ÅŸtÄ±rma 6**
> 
> 
> 
> DoÄŸrusal bir modeli dÃ¼zenlileÅŸtirmek (**regularize**) iÃ§in **ridge** regresyonu dÄ±ÅŸÄ±nda hangi olasÄ±lÄ±klara sahibiz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±n):

Modeli uydurmak iÃ§in, kimlik sÃ¼tununu veya hedefleri ayÄ±rmak gibi temel hazÄ±rlÄ±klarÄ± yapÄ±yoruz:

```python
# hazÄ±rlÄ±k
id_train = xtrain['qa_id']
ytrain = xtrain[target_cols]
xtrain.drop(target_cols + ['qa_id'], axis = 1, inplace = True)
id_test = xtest['qa_id'] 
xtest.drop('qa_id', axis = 1, inplace = True)

dropcols = ['question_user_name', 'question_user_page',
 'answer_user_name', 'answer_user_page','url','host']
xtrain.drop(dropcols, axis = 1, inplace = True)
xtest.drop(dropcols, axis = 1, inplace = True)
```

**Model PerformansÄ±nÄ±n DeÄŸerlendirilmesi**

Temel modelimizin performansÄ±nÄ± **KÄ±vrÄ±m DÄ±ÅŸÄ± (Out-of-Fold - OOF) Ã§apraz doÄŸrulama** kullanarak deÄŸerlendireceÄŸiz. OOF, bir makine Ã¶ÄŸrenimi modelinin performansÄ±nÄ±, mevcut verinin bir kÄ±smÄ±nda modeli eÄŸiterek ve kalan kÄ±smÄ±nda deÄŸerlendirerek tahmin etmek iÃ§in kullanÄ±lan bir yÃ¶ntemdir. Bu, modelin **genelleme performansÄ±nÄ±n**, yani modelin yeni, gÃ¶rÃ¼lmeyen verilerde ne kadar iyi performans gÃ¶stereceÄŸinin bir tahminini elde etmenin bir yoludur.

OOF Ã§apraz doÄŸrulama, mevcut veriyi, tipik olarak 5 veya 10 gibi bir dizi kÄ±vrÄ±ma (**fold**) bÃ¶lmeyi ve her seferinde verinin farklÄ± bir alt kÃ¼mesinde modeli eÄŸitmeyi, bir kÄ±vrÄ±mÄ± doÄŸrulama kÃ¼mesi olarak bÄ±rakmayÄ± ve diÄŸer kÄ±vrÄ±mlarÄ± eÄŸitim iÃ§in kullanmayÄ± iÃ§erir. Bu sÃ¼reÃ§, her kÄ±vrÄ±m bir kez doÄŸrulama kÃ¼mesi olarak kullanÄ±lana kadar tekrarlanÄ±r ve tÃ¼m kÄ±vrÄ±mlardaki ortalama performans, **OOF performansÄ±** olarak alÄ±nÄ±r.

Veriyi kÄ±vrÄ±mlara ayÄ±rmak birden fazla ÅŸekilde yapÄ±labilir â€“ bu Ã¶rnekte, sorularÄ±n benzersiz olmadÄ±ÄŸÄ± gerÃ§eÄŸini hesaba katmamÄ±z gerekir, yani birden Ã§ok kayÄ±t farklÄ± cevaplarla aynÄ± soruya karÅŸÄ±lÄ±k gelebilir (ve dolayÄ±sÄ±yla insan deÄŸerlendiriciler tarafÄ±ndan farklÄ± ÅŸekilde deÄŸerlendirilebilir). Bu, her sorunun yalnÄ±zca bir kÄ±vrÄ±mda gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nden emin olmamÄ±z gerektiÄŸi anlamÄ±na gelir ve bu amaÃ§la **GroupKFold** kullanÄ±yoruz. Bu doÄŸrulama kÄ±vrÄ±mlarÄ±nÄ± hesaplama ÅŸekli, veri gruplara ayrÄ±ldÄ±ÄŸÄ±nda kullanÄ±lan bir k-katlÄ± Ã§apraz doÄŸrulama varyasyonudur (Ã¶rneÄŸin, her Ã¶rnek belirli bir sÄ±nÄ±fa ait olduÄŸunda):

```python
nfolds = 5
mvalid = np.zeros((xtrain.shape[0], len(target_cols)))
mfull = np.zeros((xtest.shape[0], len(target_cols)))
kf = GroupKFold(n_splits= nfolds).split(X=xtrain.question_body, 
groups=xtrain.question_body)
```

Bu kod ilk olarak veriyi scikit-learn'den `KFold` sÄ±nÄ±fÄ±nÄ± kullanarak beÅŸ kÄ±vrÄ±ma ayÄ±rÄ±r, ardÄ±ndan her adÄ±mda (1'den `nfolds`'a kadar) dÃ¶ngÃ¼ yapar, veriyi eÄŸitim ve doÄŸrulama kÃ¼melerine ayÄ±rÄ±r ve modeli eÄŸitim verisine uydurur. Model daha sonra doÄŸrulama verisi Ã¼zerinde deÄŸerlendirilir ve skor, skor listesine eklenir. Son olarak, tÃ¼m kÄ±vrÄ±mlarÄ±n ortalama skoru hesaplanÄ±r ve `mean_score` deÄŸiÅŸkeninde saklanÄ±r. Bu ortalama skor, modelin OOF performansÄ±nÄ±n bir tahmini olarak kullanÄ±labilir.

**OOF Model PerformansÄ±nÄ± Hesaplama ProsedÃ¼rÃ¼**

OOF model performansÄ±nÄ± hesaplamak iÃ§in standart bir prosedÃ¼r izliyoruz:

1.  Her kÄ±vrÄ±m iÃ§in, eÄŸitim/doÄŸrulama kÃ¼melerini oluÅŸturun.
2.  Bu ayÄ±rÄ±mda, hedef sÃ¼tunlarÄ±n her biri iÃ§in bir ridge regresyon modeli tahmin edin.
3.  Tahminleri depolayÄ±n.
4.  Daha sonra yeniden kullanmak iÃ§in modeli bir **pickle** dosyasÄ±na kaydedin.

Bu son nokta iÃ§in birkaÃ§ sÃ¶z sÃ¶ylemek gerekir: **pickle formatÄ±**, Python'da nesneleri seri hale getirmenin yaygÄ±n bir yoludur ve tekrarlayan iÅŸ akÄ±ÅŸÄ±nÄ±zÄ± hÄ±zlandÄ±rmanÄ±n mÃ¼kemmel derecede makul bir yoludur (bÃ¶ylece ardÄ±ÅŸÄ±k dÃ¼zeni her seferinde yeniden uydurmak zorunda kalmazsÄ±nÄ±z). Pickle'Ä± iÅŸbirlikÃ§i bir ortamda kullanmaya baÅŸladÄ±ÄŸÄ±nÄ±z anda, dikkate alÄ±nmasÄ± gereken iki husus vardÄ±r:

  * Pickle insan tarafÄ±ndan okunabilir deÄŸildir ve farklÄ± Python sÃ¼rÃ¼mleri veya pickle nesnelerini oluÅŸturmak iÃ§in kullanÄ±lan kÃ¼tÃ¼phaneler arasÄ±nda geriye dÃ¶nÃ¼k uyumluluÄŸu garanti edilmez.
  * Pickle formatÄ±, kÃ¶tÃ¼ niyetli saldÄ±rÄ±lara karÅŸÄ± savunmasÄ±zdÄ±r, Ã§Ã¼nkÃ¼ kÃ¶tÃ¼ amaÃ§lÄ± olarak hazÄ±rlanmÄ±ÅŸ bir pickle dosyasÄ±, dosya aÃ§Ä±ldÄ±ÄŸÄ±nda yÃ¼rÃ¼tÃ¼len rastgele kod iÃ§erebilir.

AÅŸaÄŸÄ±daki kod parÃ§asÄ±, model performansÄ±nÄ±n kÄ±vrÄ±m dÄ±ÅŸÄ± tahminler Ã¼zerinde nasÄ±l hesaplandÄ±ÄŸÄ±nÄ± gÃ¶sterir:

  * Her kÄ±vrÄ±m iÃ§in, veriyi eÄŸitim ve test veri kÃ¼melerine ayÄ±rÄ±rÄ±z: ilki modeli uydurmak, ikincisi ise performans deÄŸerlendirmesi iÃ§in kullanÄ±lÄ±r.
  * Her sÃ¼tun iÃ§in ayrÄ± bir ardÄ±ÅŸÄ±k dÃ¼zen (Ã¶n iÅŸleme + ridge regresyon modeli) uydururuz.
  * Bu kÄ±vrÄ±m iÃ§in eÄŸitilmiÅŸ model uygun bir pickle dosyasÄ±na kaydedilir.
  * Tahminler, sÄ±rasÄ±yla skoru hesaplamak ve bir gÃ¶nderi hazÄ±rlamak iÃ§in daha sonra kullanÄ±lmak Ã¼zere `mvalid`/`mfull` dizilerinde depolanÄ±r.

```python
for ind, (train_index, test_index) in enumerate(kf):
 # ayÄ±r
    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]
    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]
    for ii in range(0, ytrain.shape[1]):
        # modeli uydur
        be = clone(pipeline)
        be.fit(x0, np.array(y0)[:,ii])
        filename = 'ridge_f' + str(ind) + '_c' + str(ii) + '.pkl'
        pickle.dump(be, open(filename, 'wb'))
        # tahmini park et
        mvalid[test_index, ii] = be.predict(x1)
        mfull[:,ii] += be.predict(xtest)/nfolds
    print('---')
```

**PerformansÄ±n RaporlanmasÄ±**

Modelimizin performansÄ±nÄ±, OOF tahminlerinin her sÃ¼tunu iÃ§in **Spearman korelasyonunu** (yarÄ±ÅŸma metriÄŸimiz) hesaplayarak ve ardÄ±ndan tÃ¼m sÃ¼tunlarda ortalama alarak deÄŸerlendirebiliriz:

```python
corvec = np.zeros((ytrain.shape[1],1))
for ii in range(0, ytrain.shape[1]):
    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]
    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]
    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]
print(corvec.mean())
# 0.3041
```

YarÄ±ÅŸma metriÄŸi iÃ§in elde edilen sonuÃ§ **0.3041**'dir.

> **AlÄ±ÅŸtÄ±rma 7**
> 
> 
> 
> FarklÄ± kat sayÄ±sÄ± iÃ§in model performansÄ± nasÄ±l deÄŸiÅŸir? DÃ¼ÅŸÃ¼k bir kat sayÄ±sÄ± (Ã¶rneÄŸin, **3**) ve yÃ¼ksek bir kat sayÄ±sÄ± (Ã¶rneÄŸin, **10**) kullanarak inceleyin.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ± (size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± yazÄ±n):**

Bu bÃ¶lÃ¼mde, nispeten basit yÃ¶ntemler kullanarak bir **temel Ã§Ã¶zÃ¼m (baseline solution)** oluÅŸturmayÄ± gÃ¶sterdik: metin alanlarÄ± Ã¼zerindeki **tanÄ±mlayÄ±cÄ± istatistikler** ve ilgili gÃ¶reve ayarlanmamÄ±ÅŸ, **Ã¶nceden eÄŸitilmiÅŸ bir modelden** elde edilen gÃ¶mÃ¼lÃ¼ gÃ¶sterimlerin (embedding) birleÅŸimi. Bir madalya bÃ¶lgesine ulaÅŸmak iÃ§in yeterli olmasa da, bu yaklaÅŸÄ±m, bir sonraki bÃ¶lÃ¼m iÃ§in yararlÄ± bir temel teÅŸkil etmektedir: **en iyi Ã§Ã¶zÃ¼mleri** ve bu Ã§Ã¶zÃ¼mlerin problemimiz hakkÄ±nda verdiÄŸi **iÃ§gÃ¶rÃ¼leri** analiz etmek.

## En iyi Ã§Ã¶zÃ¼mlerden ders alma *(Learning from top solutions)*

YukarÄ±da aÃ§Ä±klanan temel yaklaÅŸÄ±m iyi bir baÅŸlangÄ±Ã§ noktasÄ±dÄ±r, peki yarÄ±ÅŸmanÄ±n kendisinde nasÄ±l bir performans sergilemiÅŸtir? GÃ¶rÃ¼nÃ¼ÅŸe gÃ¶re oldukÃ§a iyi: Chris Deotte ([https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)) **meta Ã¶zelliklerle** baÅŸladÄ± ve bir ara **LightGBM** modeli kurarak, model oluÅŸturma sÃ¼recindeki sonraki adÄ±mlarÄ±na Ä±ÅŸÄ±k tutan bazÄ± ilginÃ§ iliÅŸkileri keÅŸfedebildi: [https://www.kaggle.com/competitions/google-quest-challenge/discussion/130041](https://www.kaggle.com/competitions/google-quest-challenge/discussion/130041).

-----

Chris tarafÄ±ndan Ã¶nerilen Ã§Ã¶zÃ¼mden (Ã¶zel liderlik tablosunda gÃ¼mÃ¼ÅŸ madalya bÃ¶lgesinin bir sÄ±ra altÄ±nda yer aldÄ±) elde edilen ikinci ilginÃ§ iÃ§gÃ¶rÃ¼, **iÅŸlem sonrasÄ± (post-processing)** idi: KapsamlÄ± Ã§apraz doÄŸrulama (cross-validation) ile, farklÄ± sÃ¼tunlar iÃ§in tahminleri **kÄ±rpma (clipping)** iÃ§in uygun aralÄ±klarÄ± belirledi. Bu strateji, yarÄ±ÅŸma metriÄŸini Ã¶nemli Ã¶lÃ§Ã¼de iyileÅŸtirdi:

```
clippings = {
 'question_has_commonly_accepted_answer':[0,0.6],
 'question_conversational':[0.15,1],
 'question_multi_intent':[0.1,1],
 'question_type_choice':[0.1,1],
 'question_type_compare':[0.1,1],
 'question_type_consequence':[0.08,1],
 'question_type_definition':[0.1,1],
 'question_type_entity':[0.13,1]
}
```

-----

Bu yarÄ±ÅŸmadaki tÃ¼m yÃ¼ksek performanslÄ± Ã§Ã¶zÃ¼mler, bir **BERT-tipi model** eÄŸitmeyi iÃ§eriyordu; bu yaklaÅŸÄ±mÄ± keÅŸfedenler iÃ§in mÃ¼kemmel bir not defteri, **akensert** ([https://www.kaggle.com/akensert](https://www.kaggle.com/akensert)) tarafÄ±ndan yayÄ±mlanan Ã§Ã¶zÃ¼mdÃ¼r. Kendisi, **TensorFlow'da** sÄ±fÄ±rdan eksiksiz bir **BERT ardÄ±ÅŸÄ±k dÃ¼zeni (pipeline)** oluÅŸturmayÄ± gÃ¶stermiÅŸtir: [https://www.kaggle.com/code/akensert/quest-bert-base-tf20?scriptVersionId=39570432](https://www.google.com/search?q=https://www.kaggle.com/code/akensert/quest-bert-base-tf20%3FscriptVersionId%3D39570432).

-----

Son olarak, bu bÃ¶lÃ¼mÃ¼ mutlaka okunmasÄ± gereken bir yazÄ±yla kapatÄ±yoruz: **Dmytro Danevskyi** ([https://www.kaggle.com/ddanevskyi](https://www.kaggle.com/ddanevskyi)), **Yury Kashnitsky** ([https://www.kaggle.com/kashnitsky](https://www.kaggle.com/kashnitsky)), **Oleg Yaroshevskiy** ([https://www.kaggle.com/yaroshevskiy](https://www.kaggle.com/yaroshevskiy)) ve **Dmitry Abulkhanov** ([https://www.kaggle.com/dmitriyab)'dan](https://www.kaggle.com/dmitriyab)'dan) oluÅŸan kazanan takÄ±m **"bibimorph"** ile yapÄ±lan bir rÃ¶portaj. RÃ¶portaj, Medium'da yayÄ±mlanmÄ±ÅŸtÄ±r: [https://medium.com/kaggle-blog/the-3-ingredients-to-our-success-winners-dish-on-their-solution-to-googles-quest-q-a-labeling-c1a63014b88](https://medium.com/kaggle-blog/the-3-ingredients-to-our-success-winners-dish-on-their-solution-to-googles-quest-q-a-labeling-c1a63014b88) ve detaylÄ± incelenmeye kesinlikle deÄŸerdir.

-----

**ğŸ¥‡ KazananlarÄ±n En Ã–nemli Tespitleri**

Kazananlar tarafÄ±ndan paylaÅŸÄ±lan en Ã¶nemli noktalarÄ± ve bulgularÄ± Ã¶zetleyelim.

**1\. Dil Modeli Ã–n EÄŸitimi**

  * **Transfer Ã¶ÄŸrenimi**, takÄ±m performansÄ± iÃ§in kilit noktaydÄ±; yarÄ±ÅŸmada mevcut olan **kÃ¼Ã§Ã¼k veri seti**, etiketsiz verilerden yararlanmanÄ±n Ã§ok Ã¶nemli olduÄŸu anlamÄ±na geliyordu.
  * Ã–zellikle, ekip, bir BERT modelini **Maskeli Dil Modeli (MLM)** gÃ¶revi aracÄ±lÄ±ÄŸÄ±yla ince ayar yapmak iÃ§in bir **Stack Overflow kÃ¼lliyatÄ± (corpus)** kullandÄ±. MLM hakkÄ±nda daha fazla bilgiyi BERT makalesinde bulabilirsiniz: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).
  * Harici kÃ¼lliyat Ã¼zerinde Ã¶zel olarak tasarladÄ±klarÄ± **yardÄ±mcÄ± hedefler (auxiliary targets)** eklediler.
  * **Transfer Ã¶ÄŸrenimi** ve **alan uyarlamasÄ±nÄ±n (domain adaptation)** birleÅŸimi, baÅŸarÄ±larÄ±nda etkili oldu.

-----

**2\. SÃ¶zde Etiketleme (Pseudo-labeling)**

Bir sonraki Ã¶nemli bileÅŸen, ÅŸu adÄ±mlarÄ± takip eden **sÃ¶zde etiketleme** idi:

1.  Modeli etiketli verilerle eÄŸitin.
2.  Etiketsiz veriler iÃ§in etiketleri tahmin edin; bu tahminlere **sÃ¶zde etiketler (pseudo-labels)** denir.
3.  Modeli mevcut tÃ¼m verileri kullanarak yeniden eÄŸitin ve orijinal verileri sÃ¶zde etiketlerle birleÅŸtirin.

> **AlÄ±ÅŸtÄ±rma 8**
> 
> 
> 
> **SÃ¶zde etiketlemeyi (pseudo-labeling)** araÅŸtÄ±rÄ±n ve **iÅŸe yaramayabileceÄŸi**, yani sÃ¶zde etiketleme uygulamasÄ±nÄ±n bir sonucu olarak performansÄ±n **kÃ¶tÃ¼leÅŸebileceÄŸi** bir durumu dÃ¼ÅŸÃ¼nÃ¼n.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ± (size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± yazÄ±n):**

Son olarak, tahminleri **iÅŸlem sonrasÄ± aÅŸamaya (post-process)** tabi tutun. YarÄ±ÅŸma metriÄŸi (**Spearman korelasyonu**) sÄ±ralamalar cinsinden tanÄ±mlandÄ±ÄŸÄ± iÃ§in, Ã¶rneklemdeki gÃ¶zlemlerin eÅŸit olmasÄ±na (yani **beraberliklerin** bulunmasÄ±na) karÅŸÄ± duyarlÄ±dÄ±r. TakÄ±m, bu sorunun Ã¼stesinden, modelden gelen tahminleri verinin daÄŸÄ±lÄ±mÄ±nÄ± taklit edecek ÅŸekilde **gruplara ayÄ±rarak (bucketing)** gelmiÅŸtir:

```python
# Nihai karÄ±ÅŸÄ±ma iÅŸlem sonrasÄ± uygulama
def postprocess_single(target, ref):
    """
    Buradaki fikir, belirli bir tahmin edilen sÃ¼tunun daÄŸÄ±lÄ±mÄ±nÄ±, 
    eÄŸitim veri setindeki karÅŸÄ±lÄ±k gelen sÃ¼tunun (burada ref olarak adlandÄ±rÄ±lÄ±r) 
    daÄŸÄ±lÄ±mÄ±yla eÅŸleÅŸtirmektir.
    """
    ids = np.argsort(target)
    counts = sorted(Counter(ref).items(), key=lambda s: s[0])
    scores = np.zeros_like(target)
    last_pos = 0
    v = 0
    for value, count in counts:
        next_pos = last_pos + int(round(count / len(ref) * len(target)))
        if next_pos == last_pos:
            next_pos += 1
        cond = ids[last_pos:next_pos]
        scores[cond] = v
        last_pos = next_pos
        v += 1
    return scores / scores.max()
```

-----

YukarÄ±daki iÅŸlev, katÄ±lÄ±mcÄ±lardan birinin GitHub deposundan alÄ±nmÄ±ÅŸtÄ±r: [https://github.com/oleg-yaroshevskiy/quest\_qa\_labeling/blob/yorko/step11\_final/blending\_n\_postprocessing.py\#L48](https://www.google.com/search?q=https://github.com/oleg-yaroshevskiy/quest_qa_labeling/blob/yorko/step11_final/blending_n_postprocessing.py%23L48):

1.  Hedef sÃ¼tunu dizinine gÃ¶re sÄ±ralayarak baÅŸlar ve benzersiz deÄŸerlerin sayÄ±larÄ±nÄ±n, deÄŸerlerin kendilerine gÃ¶re sÄ±ralanmÄ±ÅŸ bir listesini oluÅŸturur.
2.  Ä°ÅŸlev, sayÄ±mlar listesi boyunca yineleme yapar ve her sayÄ±m iÃ§in, deÄŸiÅŸtirilmesi gereken hedefin iÃ§indeki bir sonraki konumu hesaplar.
3.  Daha sonra, mevcut deÄŸeri hedef sÃ¼tunun dilimine atar ve deÄŸeri, son deÄŸiÅŸtirilen konumla birlikte artÄ±rÄ±r.
4.  Ortaya Ã§Ä±kan dizi, maksimum deÄŸeri **1 olacak ÅŸekilde normalize edilir**.

-----

RÃ¶portajÄ± okumak ve verilen baÄŸlantÄ±larÄ± takip etmek, gerÃ§ek Ã§Ã¶zÃ¼mÃ¼ kodla incelemek iÃ§in gerekli bilgiyle sizi donatÄ±r; kazanan Ã§Ã¶zÃ¼mÃ¼ yarÄ±ÅŸma forumunda Ã¶zetleyen gÃ¶nderi buradadÄ±r: [https://www.kaggle.com/competitions/google-quest-challenge/discussion/129840](https://www.google.com/search?q=https://www.kaggle.com/competitions/google-quest-challenge/discussion/129840).

Temel ardÄ±ÅŸÄ±k dÃ¼zen (pipeline) ise [https://www.kaggle.com/code/phoenix9032/pytorch-bert-plain/notebook](https://www.google.com/search?q=https://www.kaggle.com/code/phoenix9032/pytorch-bert-plain/notebook) adresinde bulunabilir.

-----

**ğŸ§ª Deney YapÄ±landÄ±rmasÄ±**

Dikkate deÄŸer bir unsur, deneyime yÃ¶nelik yapÄ±landÄ±rÄ±lmÄ±ÅŸ yaklaÅŸÄ±mdÄ±r: ardÄ±ÅŸÄ±k dÃ¼zen (pipeline) yapÄ±landÄ±rmasÄ± iÃ§in bir sÄ±nÄ±f tanÄ±mlanmasÄ±:

```python
class PipeLineConfig:
    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_
                 tail,freeze,question_weight,answer_weight,fold,train):
        self.lr = lr
        self.warmup = warmup
        self.accum_steps = accum_steps
        self.epochs = epochs
        self.seed = seed
        self.expname = expname
        self.head_tail = head_tail
        self.freeze = freeze
        self.question_weight = question_weight
        self.answer_weight =answer_weight
        self.fold = fold
        self.train = train
```

Bu tanÄ±mlandÄ±ktan sonra, hem deneyin **tekrarlanabilirliÄŸini** hem de farklÄ± modeller arasÄ±nda hÄ±zlÄ± **yinelemeleri** saÄŸlayabiliriz.

> **AlÄ±ÅŸtÄ±rma 9**
> 
> 
> 
> YukarÄ±da baÄŸlantÄ±sÄ± verilen not defterini kendi Ã§Ã¶zÃ¼mÃ¼nÃ¼z iÃ§in bir temel olarak kullanÄ±n. Temel mekanikleri anladÄ±ÄŸÄ±nÄ±zdan emin olarak, **blok blok Ã§alÄ±ÅŸtÄ±rÄ±n**.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ± (size yardÄ±mcÄ± olacak notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± yazÄ±n):**

## Ã–zet *(Summary)*

Bu bÃ¶lÃ¼mde, NLP yarÄ±ÅŸmalarÄ±na, Ã¶zellikle **Google Quest Soru-Cevap Etiketleme** yarÄ±ÅŸmasÄ±na yÃ¶nelik bir yaklaÅŸÄ±mÄ± inceledik. Ä°ÅŸe, eski yÃ¶ntemleri (metin alanlarÄ±nÄ±n Ã¶zet/tanÄ±mlayÄ±cÄ± Ã¶zellikleri) Ã¶nceden eÄŸitilmiÅŸ bir modelden gelen gÃ¶mÃ¼lÃ¼ gÃ¶sterimlerle (embedding) birleÅŸtiren bir **temel Ã§Ã¶zÃ¼m (baseline)** ile baÅŸladÄ±k. Bu, ilgili zorluklara dair temel bir anlayÄ±ÅŸ saÄŸladÄ± ve ardÄ±ndan yarÄ±ÅŸmada iyi performans gÃ¶steren daha **geliÅŸmiÅŸ Ã§Ã¶zÃ¼mleri** tartÄ±ÅŸtÄ±k. Bu bÃ¶lÃ¼m size NLP sÄ±nÄ±flandÄ±rma yarÄ±ÅŸmalarÄ±na nasÄ±l yaklaÅŸÄ±lacaÄŸÄ±na dair bir anlayÄ±ÅŸ sunmalÄ±dÄ±r; alana yeni baÅŸlayanlar temel Ã§Ã¶zÃ¼mlerden faydalanÄ±rken, daha deneyimli Kaggle kullanÄ±cÄ±larÄ± yayÄ±mlanan madalyalÄ± yaklaÅŸÄ±mlarÄ±n saÄŸladÄ±ÄŸÄ± rehberlikten yararlanabilirler.