# BÃ¶lÃ¼m 1: En ÃœnlÃ¼ Tablosal YarÄ±ÅŸma â€“ Porto Seguro'nun GÃ¼venli SÃ¼rÃ¼cÃ¼ Tahmini *(Chapter 1: The Most Renowned Tabular Competition â€“ Porto Seguroâ€™s Safe Driver Prediction)*

Herhangi bir Kaggle yarÄ±ÅŸmasÄ±nda liderlik tablosunun zirvesine nasÄ±l ulaÅŸÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenmek sabÄ±r, **azim** ve en iyi sonuÃ§larÄ± elde etmek iÃ§in rekabet etmenin en iyi yolunu Ã¶ÄŸrenmek iÃ§in **birÃ§ok deneme** gerektirir. Bu nedenle, geÃ§miÅŸteki bazÄ± Kaggle yarÄ±ÅŸmalarÄ±nÄ± deneyerek ve tartÄ±ÅŸmalarÄ± okuyarak, not defterlerini (notebooks) yeniden kullanarak, **Ã¶zellik mÃ¼hendisliÄŸi** (feature engineering) yaparak ve Ã§eÅŸitli modelleri eÄŸiterek liderlik tablosunun zirvesine nasÄ±l ulaÅŸÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenerek bu becerileri daha hÄ±zlÄ± geliÅŸtirmenize yardÄ±mcÄ± olabilecek bir Ã§alÄ±ÅŸma kitabÄ± dÃ¼ÅŸÃ¼ndÃ¼k.

En Ã¼nlÃ¼ tablo (tabular) yarÄ±ÅŸmalarÄ±ndan biri olan **Porto Seguroâ€™s Safe Driver Prediction** ile baÅŸlÄ±yoruz. Bu yarÄ±ÅŸmada, sigortacÄ±lÄ±kta yaygÄ±n bir sorunu Ã§Ã¶zmeniz ve Ã¶nÃ¼mÃ¼zdeki yÄ±l kimin araÃ§ sigortasÄ± talebinde bulunacaÄŸÄ±nÄ± bulmanÄ±z isteniyor. BÃ¶yle bir bilgi, talepte bulunma olasÄ±lÄ±ÄŸÄ± daha yÃ¼ksek olan sÃ¼rÃ¼cÃ¼ler iÃ§in sigorta Ã¼cretini artÄ±rmak ve olasÄ±lÄ±ÄŸÄ± daha dÃ¼ÅŸÃ¼k olanlar iÃ§in dÃ¼ÅŸÃ¼rmek aÃ§Ä±sÄ±ndan faydalÄ±dÄ±r.

Bu yarÄ±ÅŸmayÄ± Ã§Ã¶zmek iÃ§in gerekli olan temel iÃ§gÃ¶rÃ¼leri ve teknik detaylarÄ± aÃ§Ä±klarken, size gerekli kodu gÃ¶sterecek ve **The Kaggle Book**â€™ta bulunan konularÄ± incelemenizi ve sorularÄ± yanÄ±tlamanÄ±zÄ± isteyeceÄŸiz. Ã–yleyse, daha fazla gecikmeden, bu yeni Ã¶ÄŸrenme yolunuza baÅŸlayalÄ±m.

Bu bÃ¶lÃ¼mde ÅŸunlarÄ± Ã¶ÄŸreneceksiniz:

  * Bir **LightGBM** modelini nasÄ±l ayarlayacaÄŸÄ±nÄ±z (tune) ve eÄŸiteceÄŸiniz
  * Bir **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±yÄ± (denoising autoencoder)** nasÄ±l oluÅŸturacaÄŸÄ±nÄ±z ve bunu bir sinir aÄŸÄ±nÄ± beslemek iÃ§in nasÄ±l kullanacaÄŸÄ±nÄ±z
  * Birbirinden oldukÃ§a farklÄ± olan modelleri **etkili bir ÅŸekilde nasÄ±l harmanlayacaÄŸÄ±nÄ±z (blend)**

> Bu bÃ¶lÃ¼mdeki tÃ¼m kod dosyalarÄ± **[https://packt.link/kwbchp1](https://www.google.com/search?q=https://packt.link/kwbchp1)** adresinde bulunabilir.

## YarÄ±ÅŸmayÄ± ve veriyi anlama *(Understanding the competition and the data)*

Porto Seguro, Brezilya'nÄ±n (Brezilya ve Uruguay'da faaliyet gÃ¶steren) Ã¼Ã§Ã¼ncÃ¼ bÃ¼yÃ¼k sigorta ÅŸirketidir; otomobil sigortasÄ± teminatÄ±nÄ±n yanÄ± sÄ±ra pek Ã§ok baÅŸka sigorta Ã¼rÃ¼nÃ¼ de sunmaktadÄ±r ve son 20 yÄ±ldÄ±r fiyatlarÄ±nÄ± belirlemek ve otomatik sigorta teminatÄ±nÄ± daha fazla sÃ¼rÃ¼cÃ¼ iÃ§in daha eriÅŸilebilir kÄ±lmak amacÄ±yla analitik yÃ¶ntemler ve makine Ã¶ÄŸrenimi kullanmaktadÄ±r. GÃ¶revlerini baÅŸarmanÄ±n yeni yollarÄ±nÄ± keÅŸfetmek amacÄ±yla, Kaggle kullanÄ±cÄ±larÄ±nÄ±n (Kagglers) bazÄ± temel analitik problemlerine yeni ve daha iyi Ã§Ã¶zÃ¼mler bulmalarÄ±nÄ± bekleyerek bir yarÄ±ÅŸmaya ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction)) sponsor olmuÅŸlardÄ±r.

YarÄ±ÅŸma, Kaggle kullanÄ±cÄ±larÄ±nÄ±n, bir sÃ¼rÃ¼cÃ¼nÃ¼n Ã¶nÃ¼mÃ¼zdeki yÄ±l bir otomobil sigortasÄ± talebi baÅŸlatma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eden bir model oluÅŸturmasÄ±nÄ± amaÃ§lamaktadÄ±r ki bu oldukÃ§a yaygÄ±n bir gÃ¶rev tÃ¼rÃ¼dÃ¼r (sponsor bunu "sigortacÄ±lÄ±k iÃ§in klasik bir meydan okuma" olarak bahsetmektedir). Talepte bulunma olasÄ±lÄ±ÄŸÄ± hakkÄ±ndaki bu tÃ¼r bilgiler, bir sigorta ÅŸirketi iÃ§in oldukÃ§a deÄŸerli olabilir. BÃ¶yle bir model olmadan, sigorta ÅŸirketleri mÃ¼ÅŸterilerine risklerine bakÄ±lmaksÄ±zÄ±n yalnÄ±zca sabit bir prim uygulayabilir veya kÃ¶tÃ¼ performans gÃ¶steren bir modele sahiplerse, onlara **uygunsuz bir prim** uygulayabilirler. MÃ¼ÅŸterilerin riskini profilendirmedeki yanlÄ±ÅŸlÄ±klar, iyi sÃ¼rÃ¼cÃ¼lere daha yÃ¼ksek sigorta maliyeti yÃ¼klenmesine ve kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼ler iÃ§in fiyatÄ±n dÃ¼ÅŸÃ¼rÃ¼lmesine yol aÃ§abilir. Bunun ÅŸirket Ã¼zerindeki etkisi iki yÃ¶nlÃ¼ olacaktÄ±r: iyi sÃ¼rÃ¼cÃ¼ler sigortalarÄ±nÄ± baÅŸka yerlerde arayacak ve ÅŸirketin portfÃ¶yÃ¼ kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼lerle aÅŸÄ±rÄ± yÃ¼klenecektir (teknik olarak, ÅŸirketin **kÃ¶tÃ¼ bir hasar oranÄ±** olacaktÄ±r: [https://www.investopedia.com/terms/l/loss-ratio.asp](https://www.investopedia.com/terms/l/loss-ratio.asp)). Bunun yerine, ÅŸirket talep olasÄ±lÄ±ÄŸÄ±nÄ± doÄŸru bir ÅŸekilde tahmin edebilirse, mÃ¼ÅŸterilerinden **adil bir fiyat** isteyebilir, bÃ¶ylece pazar paylarÄ±nÄ± artÄ±rabilir, daha memnun mÃ¼ÅŸterilere ve daha dengeli bir mÃ¼ÅŸteri portfÃ¶yÃ¼ne (daha iyi hasar oranÄ±) sahip olabilir ve rezervlerini (ÅŸirketin talepleri Ã¶demek iÃ§in ayÄ±rdÄ±ÄŸÄ± para) daha iyi yÃ¶netebilir.

Bunu yapmak iÃ§in, sponsor eÄŸitim ve test veri kÃ¼meleri saÄŸlamÄ±ÅŸtÄ±r ve veri kÃ¼mesi Ã§ok bÃ¼yÃ¼k olmadÄ±ÄŸÄ± ve Ã§ok iyi hazÄ±rlanmÄ±ÅŸ gÃ¶rÃ¼ndÃ¼ÄŸÃ¼ iÃ§in yarÄ±ÅŸma herkes iÃ§in idealdi.

Verilerin sunulmasÄ±na ayrÄ±lmÄ±ÅŸ olan yarÄ±ÅŸma sayfasÄ±nda ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data)) belirtildiÄŸi gibi:

> Benzer gruplara ait Ã¶zellikler, Ã¶zellik adlarÄ±nda bu ÅŸekilde etiketlenmiÅŸtir (Ã¶rneÄŸin, ind, reg, car, calc).
>
> Ek olarak, Ã¶zellik adlarÄ± ikili (binary) Ã¶zellikleri belirtmek iÃ§in **bin** ve kategorik Ã¶zellikleri belirtmek iÃ§in **cat** sonekini iÃ§erir. Bu tanÄ±mlamalara sahip olmayan Ã¶zellikler ya sÃ¼rekli (continuous) ya da sÄ±ralÄ±dÄ±r (ordinal). **-1** deÄŸerleri, Ã¶zelliÄŸin gÃ¶zlemde **eksik** olduÄŸunu gÃ¶sterir. **Hedef (target)** sÃ¼tunu, poliÃ§e sahibi iÃ§in bir talep aÃ§Ä±lÄ±p aÃ§Ä±lmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.

YarÄ±ÅŸma iÃ§in veri hazÄ±rlÄ±ÄŸÄ±, herhangi bir bilgi sÄ±zÄ±ntÄ±sÄ±nÄ± (leak) Ã¶nlemek iÃ§in dikkatlice yapÄ±lmÄ±ÅŸtÄ±r ve Ã¶zelliklerin anlamÄ± hakkÄ±nda gizlilik korunmuÅŸ olsa da, kullanÄ±lan farklÄ± etiketlerin motorlu taÅŸÄ±t sigortasÄ± modellemesinde yaygÄ±n olarak kullanÄ±lan belirli Ã¶zellik tÃ¼rlerine atÄ±fta bulunduÄŸu oldukÃ§a aÃ§Ä±ktÄ±r:

  * **ind**: "Bireysel Ã¶zellikler" (individual characteristics)
  * **car**: "Araba Ã¶zellikleri" (car characteristics)
  * **calc**: "HesaplanmÄ±ÅŸ Ã¶zellikler" (calculated features)
  * **reg**: "BÃ¶lgesel/coÄŸrafi Ã¶zellikler" (regional/geographic features)

Bireysel Ã¶zelliklere gelince, yarÄ±ÅŸma sÄ±rasÄ±nda anlamlarÄ± hakkÄ±nda Ã§ok fazla spekÃ¼lasyon yapÄ±lmÄ±ÅŸtÄ±r. Ã–rneÄŸin ÅŸuraya bakÄ±nÄ±z:

  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489), burada Raddar, **ps\_car\_13** Ã¶zelliÄŸinin iki yÄ±lda bir zorunlu araÃ§ muayeneleri arasÄ±nda kat edilen mesafeyi temsil edebileceÄŸini Ã¶ne sÃ¼rmektedir.
  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488), burada Raddar, **ps\_car\_12** Ã¶zelliÄŸinin bunun yerine motor silindir hacmini temsil edebileceÄŸini Ã¶ne sÃ¼rmektedir.
  * [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057), burada bazÄ± Ã¶zelliklerin Porto Seguro'nun Ã§evrimiÃ§i teklif formundan tÃ¼retildiÄŸi yÃ¶nÃ¼ndeki Ã¶neriyi okuyabilirsiniz.

TÃ¼m bu ve daha fazla Ã§abaya raÄŸmen, sonunda Ã¶zelliklerin Ã§oÄŸunun anlamÄ± ÅŸimdiye kadar bir **gizem olarak kalmÄ±ÅŸtÄ±r**.

Bu yarÄ±ÅŸmanÄ±n ilginÃ§ gerÃ§ekleri ÅŸunlardÄ±r:

1.  Veriler, Ã¶zellikler anonim olsa da, **gerÃ§ek dÃ¼nyadan** alÄ±nmÄ±ÅŸtÄ±r.
2.  Veriler, herhangi bir tÃ¼r sÄ±zÄ±ntÄ± olmaksÄ±zÄ±n **Ã§ok iyi hazÄ±rlanmÄ±ÅŸtÄ±r** (burada sihirli Ã¶zellikler yoktur â€“ sihirli Ã¶zellik, becerikli bir iÅŸlemle Kaggle yarÄ±ÅŸmasÄ±nda modellerinize yÃ¼ksek tahmin gÃ¼cÃ¼ saÄŸlayabilen bir Ã¶zelliktir).
3.  Test veri seti sadece eÄŸitim veri setiyle aynÄ± kategorik seviyeleri tutmakla kalmaz; aynÄ± zamanda aynÄ± daÄŸÄ±lÄ±mdan geliyormuÅŸ gibi gÃ¶rÃ¼nmektedir, ancak Yuya Yamamoto, verilerin t-SNE ile Ã¶n iÅŸlenmesinin dÃ¼ÅŸmanca doÄŸrulama (adversarial validation) testinin baÅŸarÄ±sÄ±z olmasÄ±na yol aÃ§tÄ±ÄŸÄ±nÄ± iddia etmektedir ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784)).

> ğŸ“ AlÄ±ÅŸtÄ±rma 1
> 
> 
> 
> Ä°lk alÄ±ÅŸtÄ±rma olarak, **The Kaggle Book**'taki **dÃ¼ÅŸmanca doÄŸrulama (adversarial validation)** ile ilgili iÃ§eriklere ve koda (sayfa 179'dan baÅŸlayarak) atÄ±fta bulunarak, eÄŸitim ve test verilerinin **bÃ¼yÃ¼k olasÄ±lÄ±kla aynÄ± veri daÄŸÄ±lÄ±mÄ±ndan** kaynaklandÄ±ÄŸÄ±nÄ± kanÄ±tlayÄ±nÄ±z.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

Tilii (Mensur Dlakic, Montana Eyalet Ãœniversitesi'nde DoÃ§ent: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197)) tarafÄ±ndan yapÄ±lan ilginÃ§ bir paylaÅŸÄ±m, t-SNE kullanarak ÅŸunu gÃ¶stermektedir: "**Sigorta parametreleri aÃ§Ä±sÄ±ndan Ã§ok benzer olan birÃ§ok insan vardÄ±r, ancak bunlardan bazÄ±larÄ± talepte bulunacak, bazÄ±larÄ± ise bulunmayacaktÄ±r**." Tilii'nin bahsettiÄŸi ÅŸey, sigortacÄ±lÄ±kta olanlara oldukÃ§a tipiktir; belirli Ã¶ncÃ¼ller (sigorta parametreleri) iÃ§in bir ÅŸeyin olma olasÄ±lÄ±ÄŸÄ± aynÄ±dÄ±r, ancak o olay, olaylar dizisini ne kadar sÃ¼re gÃ¶zlemlediÄŸimize baÄŸlÄ± olarak gerÃ§ekleÅŸir veya gerÃ§ekleÅŸmez.

Ã–rneÄŸin, sigortacÄ±lÄ±kta IoT ve telematik verilerini ele alalÄ±m. Bir sÃ¼rÃ¼cÃ¼nÃ¼n gelecekte talepte bulunup bulunmayacaÄŸÄ±nÄ± tahmin etmek iÃ§in sÃ¼rÃ¼ÅŸ davranÄ±ÅŸÄ±nÄ± analiz etmek oldukÃ§a yaygÄ±ndÄ±r. GÃ¶zlem sÃ¼reniz **Ã§ok kÄ±saysa** (Ã¶rneÄŸin, bu yarÄ±ÅŸmada olduÄŸu gibi bir yÄ±l), Ã§ok kÃ¶tÃ¼ sÃ¼rÃ¼cÃ¼lerin bile bir talepte bulunmamasÄ± sÃ¶z konusu olabilir, Ã§Ã¼nkÃ¼ kÃ¶tÃ¼ bir sÃ¼rÃ¼cÃ¼ iÃ§in bile bÃ¶yle bir olayÄ±n kÄ±sa bir zaman diliminde meydana gelme olasÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼ktÃ¼r.

Benzer fikirler, Andy Harless ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735)) tarafÄ±ndan tartÄ±ÅŸÄ±lmaktadÄ±r; Harless, bunun yerine yarÄ±ÅŸmanÄ±n gerÃ§ek gÃ¶revinin "kazaya daha yatkÄ±n olan sÃ¼rÃ¼cÃ¼leri belirleyen **gizli bir sÃ¼rekli deÄŸiÅŸkenin deÄŸerini tahmin etmek**" olduÄŸunu savunur, Ã§Ã¼nkÃ¼ aslÄ±nda "**talepte bulunmak bir sÃ¼rÃ¼cÃ¼nÃ¼n Ã¶zelliÄŸi deÄŸil; ÅŸansÄ±n bir sonucudur**."

## DeÄŸerlendirme metriÄŸini anlama
*(Understanding the evaluation metric)*

Elbette, metninizi TÃ¼rkÃ§eye Ã§evirdim:

-----

YarÄ±ÅŸmada kullanÄ±lan metrik, **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±dÄ±r** (ekonomide kullanÄ±lan benzer Gini katsayÄ±sÄ±/endeksinden almÄ±ÅŸtÄ±r) ve daha Ã¶nce baÅŸka bir yarÄ±ÅŸmada, Allstate Claim Prediction Challenge'da ([https://www.kaggle.com/competitions/ClaimPredictionChallenge](https://www.google.com/search?q=https://www.kaggle.com/competitions/ClaimPredictionChallenge)) kullanÄ±lmÄ±ÅŸtÄ±r. Bu yarÄ±ÅŸmadan, metriÄŸin ne hakkÄ±nda olduÄŸuna dair Ã§ok net bir aÃ§Ä±klama alabiliriz:

> Bir giriÅŸ gÃ¶nderdiÄŸinizde, gÃ¶zlemler "en bÃ¼yÃ¼k tahminden" "en kÃ¼Ã§Ã¼k tahmine" doÄŸru sÄ±ralanÄ±r. Tahminlerinizin devreye girdiÄŸi tek adÄ±m budur, bu nedenle yalnÄ±zca tahminlerinizin belirlediÄŸi sÄ±ra Ã¶nemlidir. GÃ¶zlemleri soldan saÄŸa, en bÃ¼yÃ¼k tahminler solda olacak ÅŸekilde dÃ¼zenlenmiÅŸ olarak gÃ¶rselleÅŸtirin. ArdÄ±ndan soldan saÄŸa hareket ederek ÅŸunu sorarÄ±z: "**Verinin en soldaki %x'lik kÄ±smÄ±nda, gerÃ§ekten gÃ¶zlemlenen kaybÄ±n ne kadarÄ±nÄ± biriktirdiniz?**" Bir model olmadan, tahminlerin %10'unda kaybÄ±n %10'unu biriktirmeyi beklersiniz, bu nedenle model olmamasÄ± (veya bir "sÄ±fÄ±r" modeli) dÃ¼z bir Ã§izgiye ulaÅŸÄ±r. **Sizin eÄŸriniz ile bu dÃ¼z Ã§izgi arasÄ±ndaki alana Gini katsayÄ±sÄ± diyoruz.**
>
> "MÃ¼kemmel" bir model iÃ§in ulaÅŸÄ±labilecek maksimum bir alan vardÄ±r. Biz, modelinizin Gini katsayÄ±sÄ±nÄ± mÃ¼kemmel modelin Gini katsayÄ±sÄ±na bÃ¶lerek **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ±** kullanacaÄŸÄ±z.

Daha iyi bir aÃ§Ä±klama da Kilian Batzner'Ä±n not defterinde (notebook) saÄŸlanmÄ±ÅŸtÄ±r: [https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation](https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation). Kilian, net Ã§izimler ve bazÄ± basit Ã¶rnekler kullanarak, sigorta ÅŸirketlerinin aktÃ¼erya departmanlarÄ± tarafÄ±ndan rutin olarak kullanÄ±lan, ancak Ã§ok yaygÄ±n olmayan bu metriÄŸi anlamlandÄ±rmaya Ã§alÄ±ÅŸmaktadÄ±r.

Bu metrik, yaklaÅŸÄ±k olarak $2 \cdot \text{ROC-AUC} - 1$ formÃ¼lÃ¼ne karÅŸÄ±lÄ±k geldiÄŸi iÃ§in **ROC-AUC skoru** veya **Mannâ€“Whitney U non-parametrik istatistiksel testi** (U istatistiÄŸi, alÄ±cÄ± iÅŸletim karakteristiÄŸi eÄŸrisi altÄ±ndaki alana â€“ AUC'ye eÅŸdeÄŸer olduÄŸundan) ile yaklaÅŸtÄ±rÄ±labilir. DolayÄ±sÄ±yla, **ROC-AUC'yi maksimize etmek, normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ± maksimize etmekle aynÄ±dÄ±r** (bir referans iÃ§in Wikipedia giriÅŸindeki *DiÄŸer istatistiksel Ã¶lÃ§Ã¼mlerle iliÅŸki* bÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z: [https://en.wikipedia.org/wiki/Gini\_coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)).

Metrik, ayrÄ±ca Ã¶lÃ§eklenmiÅŸ tahmin sÄ±rasÄ± (rank) ile Ã¶lÃ§eklenmiÅŸ hedef deÄŸerinin kovaryansÄ± olarak da yaklaÅŸÄ±k olarak ifade edilebilir, bu da daha anlaÅŸÄ±lÄ±r bir sÄ±ra iliÅŸkisi Ã¶lÃ§Ã¼tÃ¼ saÄŸlar (bkz. Dmitriy Guller: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576)).

**AmaÃ§ fonksiyonu** (objective function) aÃ§Ä±sÄ±ndan bakÄ±ldÄ±ÄŸÄ±nda, bir sÄ±nÄ±flandÄ±rma probleminde yapacaÄŸÄ±nÄ±z gibi **ikili log-kaybÄ±** (binary log-loss) iÃ§in optimizasyon yapabilirsiniz. Ne ROC-AUC ne de normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ± tÃ¼revlenebilir deÄŸildir ve bunlar yalnÄ±zca doÄŸrulama kÃ¼mesi Ã¼zerindeki metrik deÄŸerlendirmesi iÃ§in kullanÄ±labilir (Ã¶rneÄŸin, erken durdurma veya bir sinir aÄŸÄ±nda Ã¶ÄŸrenme hÄ±zÄ±nÄ± azaltma iÃ§in). Ancak, log-kaybÄ± iÃ§in optimizasyon yapmak her zaman ROC-AUC'yi ve normalleÅŸtirilmiÅŸ Gini katsayÄ±larÄ±nÄ± iyileÅŸtirmez ve ikisi de doÄŸrudan tÃ¼revlenebilir deÄŸildir.

> AslÄ±nda tÃ¼revlenebilir bir ROC-AUC yaklaÅŸtÄ±rmasÄ± mevcuttur. Bunun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Toon Calders ve Szymon Jaroszewicz'in *Efficient AUC Optimization for Classification* adlÄ± Ã§alÄ±ÅŸmasÄ±nda okuyabilirsiniz. European Conference on Principles of Data Mining and Knowledge Discovery. Springer, Berlin, Heidelberg, 2007: [https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9\_8.pdf](https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf).

Ancak, yarÄ±ÅŸmada bir amaÃ§ fonksiyonu olarak log-kaybÄ±ndan farklÄ± bir ÅŸey kullanmaya ve deÄŸerlendirme metriÄŸi olarak ROC-AUC veya normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±ndan baÅŸka bir ÅŸey kullanmaya gerek olmadÄ±ÄŸÄ± anlaÅŸÄ±lmaktadÄ±r.

Kaggle Not Defterleri arasÄ±nda normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ± hesaplamak iÃ§in aslÄ±nda birkaÃ§ Python uygulamasÄ± bulunmaktadÄ±r. Burada, CPMP'nin ([https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook](https://www.google.com/search?q=https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook)) **Numba** kullanarak hesaplamalarÄ± hÄ±zlandÄ±ran Ã§alÄ±ÅŸmasÄ±nÄ± kullandÄ±k ve Ã¶neriyoruz: bu hem kesin hem de hÄ±zlÄ±dÄ±r.

> ğŸ“ AlÄ±ÅŸtÄ±rma 2
> 
> 
> 
> **The Kaggle Book**'un 5. bÃ¶lÃ¼mÃ¼nde (sayfa 95 ve sonrasÄ±), Ã¶zellikle yeni ve genellikle bilinmeyen yarÄ±ÅŸma metrikleriyle nasÄ±l baÅŸa Ã§Ä±kÄ±lacaÄŸÄ±nÄ± aÃ§Ä±klamÄ±ÅŸtÄ±k.
> 
> 
> 
> Bir alÄ±ÅŸtÄ±rma olarak, Kaggle'da **normalleÅŸtirilmiÅŸ Gini katsayÄ±sÄ±nÄ±** bir deÄŸerlendirme metriÄŸi olarak kullanan **kaÃ§ yarÄ±ÅŸma** olduÄŸunu bulabilir misiniz?
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

## Michael Jahrer'Ä±n en iyi Ã§Ã¶zÃ¼m fikirlerini inceleme *(Examining the top solution ideas from Michael Jahrer)*

Michael Jahrer ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer), yarÄ±ÅŸma BÃ¼yÃ¼k UstasÄ± ve Netflix Ã–dÃ¼lÃ¼'nÃ¼n "BellKor's Pragmatic Chaos" ekibindeki kazananlarÄ±ndan biri), yarÄ±ÅŸma boyunca halka aÃ§Ä±k liderlik tablosunu uzun sÃ¼re ve belirgin bir farkla Ã¶nde gÃ¶tÃ¼rdÃ¼ ve Ã¶zel Ã§Ã¶zÃ¼mler nihayet aÃ§Ä±klandÄ±ÄŸÄ±nda kazanan ilan edildi.

KÄ±sa sÃ¼re sonra, tartÄ±ÅŸma forumunda, **gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ±larÄ± (denoising autoencoders)** ve sinir aÄŸlarÄ±nÄ± akÄ±llÄ±ca kullanmasÄ± nedeniyle birÃ§ok Kaggler iÃ§in bir referans haline gelen Ã§Ã¶zÃ¼mÃ¼nÃ¼n kÄ±sa bir Ã¶zetini yayÄ±nladÄ± ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)). Michael, Ã§Ã¶zÃ¼mÃ¼ne iliÅŸkin herhangi bir Python kodu eklememiÅŸ olsa da (kodlama Ã§alÄ±ÅŸmasÄ±nÄ± doÄŸrudan C++/CUDA ile Python kullanmadan yazÄ±lmÄ±ÅŸ "eski usul" ve "dÃ¼ÅŸÃ¼k seviyeli" olarak tanÄ±mlamÄ±ÅŸtÄ±r), yazÄ±larÄ± kullandÄ±ÄŸÄ± modellere, hiperparametrelerine ve mimarilerine dair referanslar aÃ§Ä±sÄ±ndan oldukÃ§a zengindir.

Ã–ncelikle Michael, Ã§Ã¶zÃ¼mÃ¼nÃ¼n altÄ± modelin bir karÄ±ÅŸÄ±mÄ±ndan (bir LightGBM modeli ve beÅŸ sinir aÄŸÄ±) oluÅŸtuÄŸunu aÃ§Ä±klamaktadÄ±r. AyrÄ±ca, muhtemelen **aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting)** nedeniyle, karÄ±ÅŸÄ±ma her bir modelin katkÄ±sÄ±nÄ± aÄŸÄ±rlÄ±klandÄ±rmanÄ±n (ayrÄ±ca doÄŸrusal ve doÄŸrusal olmayan yÄ±ÄŸÄ±nlama (stacking) yapmanÄ±n) bir avantaj saÄŸlamadÄ±ÄŸÄ± iÃ§in, yalnÄ±zca farklÄ± tohumlardan (seed) oluÅŸturulmuÅŸ, **eÅŸit aÄŸÄ±rlÄ±ÄŸa sahip** modellerin bir karÄ±ÅŸÄ±mÄ±na baÅŸvurduÄŸunu belirtmektedir.

Bu iÃ§gÃ¶rÃ¼, Michael'Ä±n yaklaÅŸÄ±mÄ±nÄ± Ã§oÄŸaltma gÃ¶revimizi Ã§ok daha kolay hale getiriyor, Ã§Ã¼nkÃ¼ kendisi ayrÄ±ca, **yalnÄ±zca LightGBM sonuÃ§larÄ±nÄ± oluÅŸturduÄŸu sinir aÄŸlarÄ±ndan biriyle karÄ±ÅŸtÄ±rmanÄ±n (blending)** bile yarÄ±ÅŸmada birinciliÄŸi garantilemek iÃ§in yeterli olacaÄŸÄ±nÄ± da belirtmiÅŸtir.

Bu iÃ§gÃ¶rÃ¼, alÄ±ÅŸtÄ±rma Ã§alÄ±ÅŸmamÄ±zÄ± bir sÃ¼rÃ¼ model yerine, iki iyi tekil modelle sÄ±nÄ±rlayacaktÄ±r. Buna ek olarak, Michael, bazÄ± sÃ¼tunlarÄ± Ã§Ä±karmak ve kategorik Ã¶zellikleri **tek-sÄ±cak kodlama (one-hot encoding)** dÄ±ÅŸÄ±nda Ã§ok az veri iÅŸleme yaptÄ±ÄŸÄ±nÄ± da belirtmiÅŸtir.

## Bir LightGBM gÃ¶nderimi oluÅŸturma *(Building a LightGBM submission)*

AlÄ±ÅŸtÄ±rmamÄ±z LightGBM tabanlÄ± bir Ã§Ã¶zÃ¼m Ã¼zerinde Ã§alÄ±ÅŸmakla baÅŸlÄ±yor. Kodu, Kaggle Notebooks'u kullanarak hemen yÃ¼rÃ¼tÃ¼lmeye hazÄ±r ÅŸekilde ÅŸu adreste bulabilirsiniz: [https://www.kaggle.com/code/lucamassaron/workbook-lgb](https://www.kaggle.com/code/lucamassaron/workbook-lgb). Kodu kolayca eriÅŸilebilir hale getirmiÅŸ olsak da, bunun yerine kodu doÄŸrudan kitaptan yazmanÄ±zÄ± veya kopyalamanÄ±zÄ± ve hÃ¼cre hÃ¼cre Ã§alÄ±ÅŸtÄ±rmanÄ±zÄ± Ã¶neriyoruz; her bir kod satÄ±rÄ±nÄ±n ne yaptÄ±ÄŸÄ±nÄ± anlamak ve Ã§Ã¶zÃ¼mÃ¼ kiÅŸiselleÅŸtirmek, performansÄ±nÄ± daha da artÄ±rabilir.

> LightGBM kullanÄ±rken, **GPU veya TPU hÄ±zlandÄ±rÄ±cÄ±larÄ±ndan hiÃ§birini aÃ§manÄ±za gerek yoktur ve aÃ§mamalÄ±sÄ±nÄ±z**. GPU hÄ±zlandÄ±rmasÄ± yalnÄ±zca LightGBM'nin GPU sÃ¼rÃ¼mÃ¼nÃ¼ yÃ¼klediyseniz yardÄ±mcÄ± olabilir. Bu tÃ¼r GPU hÄ±zlandÄ±rmalÄ± bir sÃ¼rÃ¼mÃ¼ Kaggle Notebooks'a nasÄ±l kuracaÄŸÄ±nÄ±za dair Ã§alÄ±ÅŸma ipuÃ§larÄ±nÄ± ÅŸu Ã¶rnekte bulabilirsiniz: [https://www.kaggle.com/code/lucamassaron/gpu-accelerated-lightgbm](https://www.kaggle.com/code/lucamassaron/gpu-accelerated-lightgbm).

Anahtar paketleri (hiperparametre optimizasyonu iÃ§in NumPy, pandas ve Optuna, LightGBM ve bazÄ± yardÄ±mcÄ± fonksiyonlar) iÃ§e aktararak baÅŸlÄ±yoruz. AyrÄ±ca bir konfigÃ¼rasyon sÄ±nÄ±fÄ± tanÄ±mlÄ±yor ve onu Ã¶rneklendiriyoruz. KonfigÃ¼rasyon sÄ±nÄ±fÄ±nda tanÄ±mlanan parametreleri, kodun ilerleyiÅŸi sÄ±rasÄ±nda keÅŸfederken tartÄ±ÅŸacaÄŸÄ±z. Burada Ã¶nemli olan nokta, tÃ¼m parametrelerinizi iÃ§eren bir sÄ±nÄ±f kullanarak, onlarÄ± kod boyunca tutarlÄ± bir ÅŸekilde deÄŸiÅŸtirmenizin daha kolay olacaÄŸÄ±dÄ±r. YarÄ±ÅŸmanÄ±n hararetinde, kodun birden fazla yerinde atÄ±fta bulunulan bir parametreyi gÃ¼ncellemeyi unutmak kolaydÄ±r ve parametreler hÃ¼crelere ve fonksiyonlara daÄŸÄ±lmÄ±ÅŸsa bunlarÄ± ayarlamak her zaman zordur. Bir konfigÃ¼rasyon sÄ±nÄ±fÄ±, size Ã§ok Ã§aba kazandÄ±rabilir ve yol boyunca hatalardan koruyabilir:

```python
import numpy as np
import pandas as pd
import optuna
import lightgbm as lgb
from path import Path
from sklearn.model_selection import StratifiedKFold

class Config:
    input_path = Path('../input/porto-seguro-safe-driver-prediction')
    optuna_lgb = False
    n_estimators = 1500
    early_stopping_round = 150
    cv_folds = 5
    random_state = 0
    params = {'objective': 'binary',
              'boosting_type': 'gbdt',
              'learning_rate': 0.01,
              'max_bin': 25,
              'num_leaves': 31,
              'min_child_samples': 1500,
              'colsample_bytree': 0.7,
              'subsample_freq': 1,
              'subsample': 0.7,
              'reg_alpha': 1.0,
              'reg_lambda': 1.0,
              'verbosity': 0,
              'random_state': 0}

config = Config()
```

Bir sonraki adÄ±m, eÄŸitim, test ve Ã¶rnek gÃ¶nderim veri setlerini iÃ§e aktarmayÄ± gerektirir. Bunu pandas'Ä±n `read_csv` fonksiyonunu kullanarak yapÄ±yoruz. AyrÄ±ca, yÃ¼klenen `DataFrame`'lerin indeksini her bir veri Ã¶rneÄŸinin tanÄ±mlayÄ±cÄ±sÄ±na (`id` sÃ¼tununa) ayarlÄ±yoruz.

Benzer gruplara ait Ã¶zellikler etiketlendiÄŸi (`ind`, `reg`, `car` ve `calc` etiketlerini kullanarak) ve ayrÄ±ca ikili (binary) ve kategorik Ã¶zellikler kolayca bulunabildiÄŸi (etiketlerinde sÄ±rasÄ±yla `bin` ve `cat` etiketlerini kullanÄ±rlar), bunlarÄ± numaralandÄ±rabilir ve listelere kaydedebiliriz:

```python
train = pd.read_csv(config.input_path / 'train.csv', index_col='id')
test = pd.read_csv(config.input_path / 'test.csv', index_col='id')
submission = pd.read_csv(config.input_path / 'sample_submission.csv', 
index_col='id')
calc_features = [feat for feat in train.columns if "_calc" in feat]
cat_features = [feat for feat in train.columns if "_cat" in feat]
```

Daha sonra, sadece hedefi (0'lar ve 1'lerden oluÅŸan ikili bir hedef) Ã§Ä±karÄ±r ve eÄŸitim veri setinden kaldÄ±rÄ±rÄ±z:

```python
target = train["target"]
train = train.drop("target", axis="columns")
```

Bu noktada, Michael Jahrer'in de iÅŸaret ettiÄŸi gibi, `calc` Ã¶zelliklerini silebiliriz. Bu fikir, Ã¶zellikle not defterlerinde yarÄ±ÅŸma sÄ±rasÄ±nda Ã§okÃ§a yinelenmiÅŸtir ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970](https://www.google.com/search?q=https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970)), Ã§Ã¼nkÃ¼ bunlarÄ± silmenin hem yerel Ã§apraz doÄŸrulama skorunu hem de halka aÃ§Ä±k liderlik tablosu skorunu iyileÅŸtirdiÄŸi deneysel olarak doÄŸrulanabilmiÅŸtir (genel bir kural olarak, Ã¶zellik seÃ§imi sÄ±rasÄ±nda her ikisini de takip etmek Ã¶nemlidir). AyrÄ±ca, bu Ã¶zellikler gradyan artÄ±rma (gradient boosting) modellerinde de kÃ¶tÃ¼ performans gÃ¶stermiÅŸtir (Ã¶nemleri her zaman ortalamanÄ±n altÄ±ndadÄ±r).

TartÄ±ÅŸabiliriz ki, bu Ã¶zellikler mÃ¼hendislik Ã¼rÃ¼nÃ¼ Ã¶zellikler olduklarÄ± iÃ§in, orijinal Ã¶zelliklerine gÃ¶re yeni bir bilgi iÃ§ermezler, ancak onlarÄ± iÃ§eren eÄŸitilmiÅŸ herhangi bir modele sadece gÃ¼rÃ¼ltÃ¼ katarlar:

```python
train = train.drop(calc_features, axis="columns")
test = test.drop(calc_features, axis="columns")
```

> ğŸ“ AlÄ±ÅŸtÄ±rma 3
> 
> 
> 
> **The Kaggle Book**'un 220. sayfasÄ±nda (Ã‡alÄ±ÅŸmanÄ±zÄ± deÄŸerlendirmek iÃ§in Ã¶zellik Ã¶nemini kullanma baÅŸlÄ±ÄŸÄ± altÄ±nda) saÄŸlanan Ã¶nerilere dayanarak, bir alÄ±ÅŸtÄ±rma olarak:
> 
> 
> 
> 1.  Bu yarÄ±ÅŸma iÃ§in **kendi Ã¶zellik seÃ§imi** not defterinizi (notebook) kodlayÄ±n.
> 
> 2.  **Hangi Ã¶zelliklerin tutulmasÄ±** ve **hangilerinin atÄ±lmasÄ±** gerektiÄŸini kontrol edin.
> 
> 
> 
> **AlÄ±ÅŸtÄ±rma NotlarÄ±** (Size yardÄ±mcÄ± olacak tÃ¼m notlarÄ± veya Ã§alÄ±ÅŸmalarÄ± buraya yazÄ±nÄ±z):

## Bir gÃ¼rÃ¼ltÃ¼ giderici otomatik kodlayÄ±cÄ± (denoising autoencoder) ve bir DNN kurma *(Setting up a denoising autoencoder and a DNN)*

## SonuÃ§larÄ± birleÅŸtirme *(Ensembling the results)*

## Ã–zet *(Summary)*

---

# BÃ¶lÃ¼m 2: Makridakis YarÄ±ÅŸmalarÄ± â€“ DoÄŸruluk ve Belirsizlik Ä°Ã§in Kaggle'daki M5 *(Chapter 2: The Makridakis Competitions â€“ M5 on Kaggle for Accuracy and Uncertainty)*

## YarÄ±ÅŸmayÄ± ve veriyi anlama *(Understanding the competition and the data)*

## DeÄŸerlendirme MetriÄŸini Anlama *(Understanding the Evaluation Metric)*

## Monsaraida'nÄ±n 4. sÄ±radaki Ã§Ã¶zÃ¼m fikirlerini inceleme *(Examining the 4th place solutionâ€™s ideas from Monsaraida)*

## Belirli tarihler ve zaman ufuklarÄ± (time horizons) iÃ§in tahminleri hesaplama *(Computing predictions for specific dates and time horizons)*

## Herkese aÃ§Ä±k (public) ve Ã¶zel (private) tahminleri bir araya getirme *(Assembling public and private predictions)*

## Ã–zet *(Summary)*

---

# BÃ¶lÃ¼m 3: GÃ¶rsel YarÄ±ÅŸma: Manyok YapraÄŸÄ± HastalÄ±ÄŸÄ± YarÄ±ÅŸmasÄ± *(Chapter 3: Vision Competition: Cassava Leaf Disease Competition)*

## Veriyi ve metrikleri anlama *(Understanding the data and metrics)*

## Bir temel model (baseline model) oluÅŸturma *(Building a baseline model)*

## En iyi Ã§Ã¶zÃ¼mlerden ders alma *(Learning from top solutions)*

### Ã–n eÄŸitim *(Pretraining)*

### Test zamanÄ± artÄ±rÄ±mÄ± *(Test time augmentation)*

### DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ler *(Transformers)*

### BirleÅŸtirme *(Ensembling)*

## KapsamlÄ± bir Ã§Ã¶zÃ¼m *(A complete solution)*

## Ã–zet *(Summary)*

---

# BÃ¶lÃ¼m 4: NLP YarÄ±ÅŸmasÄ± â€“ Google Quest Soru-Cevap Etiketleme *(Chapter 4: NLP Competition â€“ Google Quest Q&A Labeling)*

## Temel Ã§Ã¶zÃ¼m *(The baseline solution)*

## En iyi Ã§Ã¶zÃ¼mlerden ders alma *(Learning from top solutions)*

## Ã–zet *(Summary)*